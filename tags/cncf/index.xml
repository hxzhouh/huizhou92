<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>CNCF on huizhou92&#39;s Blog</title>
        <link>https://huizhou92.com/tags/cncf/</link>
        <description>Recent content in CNCF on huizhou92&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <copyright>Example Person</copyright>
        <lastBuildDate>Wed, 17 Apr 2024 10:14:00 +0000</lastBuildDate><atom:link href="https://huizhou92.com/tags/cncf/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>A Deep Dive into CNCF’s Cloud-Native AI Whitepaper</title>
        <link>https://huizhou92.com/p/a-deep-dive-into-cncfs-cloud-native-ai-whitepaper/</link>
        <pubDate>Wed, 17 Apr 2024 10:14:00 +0000</pubDate>
        
        <guid>https://huizhou92.com/p/a-deep-dive-into-cncfs-cloud-native-ai-whitepaper/</guid>
        <description>&lt;p&gt;&lt;img src=&#34;https://images.hxzhouh.com/blog-images/2024/04/3c8f677dae51c4d491a982224b6a3e0d.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;cncf&#34;
	
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;During KubeCon EU 2024, CNCF launched its first Cloud-Native AI Whitepaper. This article provides an in-depth analysis of the content of this whitepaper.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In March 2024, during KubeCon EU, the Cloud-Native Computing Foundation (CNCF) released its first detailed whitepaper on Cloud-Native Artificial Intelligence (CNAI) &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. This report extensively explores the current state, challenges, and future development directions of integrating cloud-native technologies with artificial intelligence. This article will delve into the core content of this whitepaper.&lt;/p&gt;
&lt;h2 id=&#34;what-is-cloud-native-ai&#34;&gt;What is Cloud-Native AI?
&lt;/h2&gt;&lt;p&gt;Cloud-Native AI refers to building and deploying artificial intelligence applications and workloads using cloud-native technology principles. This includes leveraging microservices, containerization, declarative APIs, and continuous integration/continuous deployment (CI/CD) among other cloud-native technologies to enhance AI applications&amp;rsquo; scalability, reusability, and operability.&lt;/p&gt;
&lt;p&gt;The following diagram illustrates the architecture of Cloud-Native AI, redrawn based on the whitepaper.&lt;br&gt;
&lt;img src=&#34;https://images.hxzhouh.com/blog-images/2024/04/40eb5be3bd0139d72f816cef9d25a51f.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Pasted image 20240418101533&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;relationship-between-cloud-native-ai-and-cloud-native-technologies&#34;&gt;Relationship between Cloud-Native AI and Cloud-Native Technologies
&lt;/h2&gt;&lt;p&gt;Cloud-native technologies provide a flexible, scalable platform that makes the development and operation of AI applications more efficient. Through containerization and microservices architecture, developers can iterate and deploy AI models quickly while ensuring high availability and scalability of the system. Kuuch as resource scheduling, automatic scaling, and service discovery.&lt;/p&gt;
&lt;p&gt;The whitepaper provides two examples to illustrate the relationship between Cloud-Native AI and cloud-native technologies, namely running AI on cloud-native infrastructure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;OpenAI Scaling Kubernetes to 7,500 nodes&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;challenges-of-cloud-native-ai&#34;&gt;Challenges of Cloud-Native AI
&lt;/h2&gt;&lt;p&gt;Despite providing a solid foundation for AI applications, there are still challenges when integrating AI workloads with cloud-native platforms. These challenges include data preparation complexity, model training resource requirements, and maintaining model security and isolation in multi-tenant environments. Additionally, resource management and scheduling in cloud-native environments are crucial for large-scale AI applications and need further optimization to support efficient model training and inference.&lt;/p&gt;
&lt;h2 id=&#34;development-path-of-cloud-native-ai&#34;&gt;Development Path of Cloud-Native AI
&lt;/h2&gt;&lt;p&gt;The whitepaper proposes several development paths for Cloud-Native AI, including improving resource scheduling algorithms to better support AI workloads, developing new service mesh technologies to enhance the performance and security of AI applications, and promoting innovation and standardization of Cloud-Native AI technology through open-source projects and community collaboration.&lt;/p&gt;
&lt;h2 id=&#34;cloud-native-ai-technology-landscape&#34;&gt;Cloud-Native AI Technology Landscape
&lt;/h2&gt;&lt;p&gt;Cloud-Native AI involves various technologies, ranging from containers and microservices to service mesh and serverless computing. Kubernetes plays a central role in deploying and managing AI applications, while service mesh technologies such as Istio and Envoy provide robust traffic management and security features. Additionally, monitoring tools like Prometheus and Grafana are crucial for maintaining the performance and reliability of AI applications.&lt;/p&gt;
&lt;p&gt;Below is the Cloud-Native AI landscape diagram provided in the whitepaper.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes&lt;/li&gt;
&lt;li&gt;Volcano&lt;/li&gt;
&lt;li&gt;Armada&lt;/li&gt;
&lt;li&gt;Kuberay&lt;/li&gt;
&lt;li&gt;Nvidia NeMo&lt;/li&gt;
&lt;li&gt;Yunikorn&lt;/li&gt;
&lt;li&gt;Kueue&lt;/li&gt;
&lt;li&gt;Flame&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;distributed-training&#34;&gt;Distributed Training
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Kubeflow Training Operator&lt;/li&gt;
&lt;li&gt;Pytorch DDP&lt;/li&gt;
&lt;li&gt;TensorFlow Distributed&lt;/li&gt;
&lt;li&gt;Open MPI&lt;/li&gt;
&lt;li&gt;DeepSpeed&lt;/li&gt;
&lt;li&gt;Megatron&lt;/li&gt;
&lt;li&gt;Horovod&lt;/li&gt;
&lt;li&gt;Apla&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ml-serving&#34;&gt;ML Serving
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Kserve&lt;/li&gt;
&lt;li&gt;Seldon&lt;/li&gt;
&lt;li&gt;VLLM&lt;/li&gt;
&lt;li&gt;TGT&lt;/li&gt;
&lt;li&gt;Skypilot&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;cicd--delivery&#34;&gt;CI/CD — Delivery
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Kubeflow Pipelines&lt;/li&gt;
&lt;li&gt;Mlflow&lt;/li&gt;
&lt;li&gt;TFX&lt;/li&gt;
&lt;li&gt;BentoML&lt;/li&gt;
&lt;li&gt;MLRun&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-science&#34;&gt;Data Science
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Jupyter&lt;/li&gt;
&lt;li&gt;Kubeflow Notebooks&lt;/li&gt;
&lt;li&gt;PyTorch&lt;/li&gt;
&lt;li&gt;TensorFlow&lt;/li&gt;
&lt;li&gt;Apache Zeppelin&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;workload-observability&#34;&gt;Workload Observability
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Prometheus&lt;/li&gt;
&lt;li&gt;Influxdb&lt;/li&gt;
&lt;li&gt;Grafana&lt;/li&gt;
&lt;li&gt;Weights and Biases (wandb)&lt;/li&gt;
&lt;li&gt;OpenTelemetry&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;automl&#34;&gt;AutoML
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Hyperopt&lt;/li&gt;
&lt;li&gt;Optuna&lt;/li&gt;
&lt;li&gt;Kubeflow Katib&lt;/li&gt;
&lt;li&gt;NNI&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;governance--policy&#34;&gt;Governance &amp;amp; Policy
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Kyverno&lt;/li&gt;
&lt;li&gt;Kyverno-JSON&lt;/li&gt;
&lt;li&gt;OPA/Gatekeeper&lt;/li&gt;
&lt;li&gt;StackRox Minder&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-architecture&#34;&gt;Data Architecture
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;ClickHouse&lt;/li&gt;
&lt;li&gt;Apache Pinot&lt;/li&gt;
&lt;li&gt;Apache Druid&lt;/li&gt;
&lt;li&gt;Cassandra&lt;/li&gt;
&lt;li&gt;ScyllaDB&lt;/li&gt;
&lt;li&gt;Hadoop HDFS&lt;/li&gt;
&lt;li&gt;Apache HBase&lt;/li&gt;
&lt;li&gt;Presto&lt;/li&gt;
&lt;li&gt;Trino&lt;/li&gt;
&lt;li&gt;Apache Spark&lt;/li&gt;
&lt;li&gt;Apache Flink&lt;/li&gt;
&lt;li&gt;Kafka&lt;/li&gt;
&lt;li&gt;Pulsar&lt;/li&gt;
&lt;li&gt;Fluid&lt;/li&gt;
&lt;li&gt;Memcached&lt;/li&gt;
&lt;li&gt;Redis&lt;/li&gt;
&lt;li&gt;Alluxio&lt;/li&gt;
&lt;li&gt;Apache Superset&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;vector-databases&#34;&gt;Vector Databases
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Chroma&lt;/li&gt;
&lt;li&gt;Weaviate&lt;/li&gt;
&lt;li&gt;Quadrant&lt;/li&gt;
&lt;li&gt;Pinecone&lt;/li&gt;
&lt;li&gt;Extensions&lt;/li&gt;
&lt;li&gt;Redis&lt;/li&gt;
&lt;li&gt;Postgres SQL&lt;/li&gt;
&lt;li&gt;ElasticSearch&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;modelllm-observability&#34;&gt;Model/LLM Observability
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;• Trulens&lt;/li&gt;
&lt;li&gt;Langfuse&lt;/li&gt;
&lt;li&gt;Deepchecks&lt;/li&gt;
&lt;li&gt;OpenLLMetry&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;Finally, the following key points are summarized:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Role of Open Source Community&lt;/strong&gt;: The whitepaper indicates the role of the open-source community in advancing Cloud-Native AI, including accelerating innovation and reducing costs through open-source projects and extensive collaboration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Importance of Cloud-Native Technologies&lt;/strong&gt;: Cloud-Native AI, built according to cloud-native principles, emphasizes the importance of repeatability and scalability. Cloud-native technologies provide an efficient development and operation environment for AI applications, especially in resource scheduling and service scalability.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Existing Challenges&lt;/strong&gt;: Despite bringing many advantages, Cloud-Native AI still faces challenges in data preparation, model training resource requirements, and model security and isolation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Future Development Directions&lt;/strong&gt;: The whitepaper proposes development paths including optimizing resource scheduling algorithms to support AI workloads, developing new service mesh technologies to enhance performance and security, and promoting technology innovation and standardization through open-source projects and community collaboration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key Technological Components&lt;/strong&gt;: Key technologies involved in Cloud-Native AI include containers, microservices, service mesh, and serverless computing, among others. Kubernetes plays a central role in deploying and managing AI applications, while service mesh technologies like Istio and Envoy provide necessary traffic management and security.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more details, please download the Cloud-Native AI whitepaper &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;reference-links&#34;&gt;Reference Links
&lt;/h2&gt;&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.cncf.io/reports/cloud-native-artificial-intelligence-whitepaper/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Whitepaper:&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://huggingface.co/blog/hugging-face-endpoints-on-azure&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://openai.com/research/scaling-kubernetes-to-7500-nodes&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt; OpenAI Scaling Kubernetes to 7,500 nodes:&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.cncf.io/reports/cloud-native-artificial-intelligence-whitepaper/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Cloud-Native AI Whitepaper: &lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
        </item>
        
    </channel>
</rss>
