<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Ai on huizhou92&#39;s Blog</title>
        <link>https://huizhou92.com/tags/ai/</link>
        <description>Recent content in Ai on huizhou92&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <copyright>Copyright © 2023 huizhou92</copyright>
        <lastBuildDate>Mon, 23 Jun 2025 10:06:40 +0800</lastBuildDate><atom:link href="https://huizhou92.com/tags/ai/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>MIT: Heavy Use of ChatGPT Can Lead to Brain Damage</title>
        <link>https://huizhou92.com/p/mit-heavy-use-of-chatgpt-can-lead-to-brain-damage/</link>
        <pubDate>Mon, 23 Jun 2025 10:06:40 +0800</pubDate>
        
        <guid>https://huizhou92.com/p/mit-heavy-use-of-chatgpt-can-lead-to-brain-damage/</guid>
        <description>&lt;p&gt;Generative AI has already left an indelible mark on humanity’s digital landscape. Suppose we accept that as the “necessary cost” of a new industrial revolution, we must also confront a far more alarming—yet rarely discussed—price: the potential damage to our brains, especially those of teenagers. A new “whistle-blower” study from MIT offers an unsettling risk perspective on how we develop and deploy AI.&lt;/p&gt;
&lt;!-- more--&gt;
&lt;h2 id=&#34;productivity-booster-or-thought-killer&#34;&gt;Productivity Booster or Thought Killer?
&lt;/h2&gt;&lt;p&gt;The core question is back on the table: Does ChatGPT &lt;strong&gt;liberate&lt;/strong&gt; human productivity, or does it &lt;strong&gt;erode&lt;/strong&gt; our capacity to think?&lt;/p&gt;
&lt;p&gt;Researchers at MIT’s Media Lab recently released a controversial paper—the first to utilize high-density EEG brain scans to examine the impact of large language models (LLMs) on human cognition. The findings are anything but encouraging.&lt;/p&gt;
&lt;h2 id=&#34;how-the-experiment-worked&#34;&gt;How the Experiment Worked
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Participants:&lt;/strong&gt; 54 Boston residents, ages 18–39&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tasks:&lt;/strong&gt; Multiple 20-minute SAT-style English essays&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Groups:&lt;/strong&gt;
&lt;ol&gt;
&lt;li&gt;ChatGPT&lt;/li&gt;
&lt;li&gt;Google Search&lt;/li&gt;
&lt;li&gt;“Raw Brainpower” (no external tools)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Throughout every writing session, 32-channel EEG sensors tracked alpha, theta, delta, and other brainwave bands associated with creativity, semantic processing, and working memory.&lt;/p&gt;
&lt;h2 id=&#34;the-cognitive-cliff-in-the-chatgpt-group&#34;&gt;The “Cognitive Cliff” in the ChatGPT Group
&lt;/h2&gt;&lt;p&gt;Compared with the other two cohorts, participants who leaned on ChatGPT showed unmistakable signs of cognitive decline:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;EEG drop-off:&lt;/strong&gt; Neural activity fell sharply, especially in executive-control and attention networks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Behavioral slump:&lt;/strong&gt; By the third essay, many subjects stopped thinking aloud entirely, effectively outsourcing the task to ChatGPT.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Language sameness:&lt;/strong&gt; Essays became formulaic, recycling the same sentence patterns and structures.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory wipe:&lt;/strong&gt; 83 percent of users could not recall what they had written.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Two high-school English teachers reviewing the results summed it up: “Technically correct, grammatically smooth—but soulless.”&lt;/p&gt;
&lt;h2 id=&#34;raw-brains-and-search-engines-fared-better&#34;&gt;Raw Brains and Search Engines Fared Better
&lt;/h2&gt;&lt;p&gt;Participants who drafted essays with &lt;strong&gt;zero tools&lt;/strong&gt; displayed the densest neural connectivity, especially in bands linked to deep semantic work and creativity.&lt;/p&gt;
&lt;p&gt;Surprisingly, the &lt;strong&gt;Google Search&lt;/strong&gt; group also kept their brains firing. Actively searching, filtering, and synthesizing information stimulates cognition in ways that simply “letting the AI spit out prose” does not.&lt;/p&gt;
&lt;h2 id=&#34;its-_how_-you-use-ai-that-matters&#34;&gt;It’s &lt;em&gt;How&lt;/em&gt; You Use AI That Matters
&lt;/h2&gt;&lt;p&gt;MIT ran a crossover test:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ChatGPT users rewrote one essay &lt;em&gt;without&lt;/em&gt; the AI.&lt;/li&gt;
&lt;li&gt;The raw-brain group used ChatGPT to refine their existing drafts.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The result:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The original ChatGPT group’s brainwaves dropped even further, and memory of the text virtually vanished.&lt;/li&gt;
&lt;li&gt;The raw-brain cohort exhibited increased neural activity when they utilized ChatGPT as a polishing tool.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Takeaway:&lt;/strong&gt; AI is not inherently damaging. When used to &lt;em&gt;enhance&lt;/em&gt; your thinking—after you’ve already done the heavy lifting—it can be a genuine cognition multiplier. But when AI &lt;em&gt;replaces&lt;/em&gt; thinking, mental muscles atrophy.&lt;/p&gt;
&lt;h2 id=&#34;teens-are-at-the-greatest-risk&#34;&gt;Teens Are at the Greatest Risk
&lt;/h2&gt;&lt;p&gt;Lead author &lt;strong&gt;Nataliya Kosmyna&lt;/strong&gt; rushed the data online before peer review because “ChatGPT is already barreling into classrooms.”&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“I’m terrified we’ll hear about a ‘GPT kindergarten’ in six months—that would be a cognitive catastrophe,” she warns.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Children’s and teens’ brains are highly plastic. Long-term dependence on AI-generated content may prune the neural pathways that support critical thinking and independent learning.&lt;/p&gt;
&lt;p&gt;Child psychiatrist &lt;strong&gt;Zishan Khan&lt;/strong&gt; echoes the concern:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Over-reliance on LLMs is starving the neural circuits kids need for memory, association, and stress resilience. Unused circuits, either, leaving them less able to tackle complex problems later in life.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;what-about-programmers&#34;&gt;What About Programmers?
&lt;/h2&gt;&lt;p&gt;Writing isn’t the only worry. MIT is already running a companion study on &lt;strong&gt;coders&lt;/strong&gt;. Preliminary data suggest that heavy AI-assisted programming boosts short-term speed but appears to compromise long-term problem-solving and architectural reasoning.&lt;/p&gt;
&lt;p&gt;Tech leaders eyeing AI as a drop-in replacement for junior developers may want to rethink that calculus.&lt;/p&gt;
&lt;h2 id=&#34;the-ironic-aftermath&#34;&gt;The Ironic Aftermath
&lt;/h2&gt;&lt;p&gt;No sooner had the study been posted than hordes of users fed it straight into ChatGPT for a summary. Kosmyna anticipated the move and embedded a “language trap” in the paper, asking any summarizer to focus on a single table. Predictably, many AI-generated synopses misreported the findings and even hallucinated references to “GPT-4o,” which the paper never mentioned.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Original study:&lt;/em&gt; &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2506.08872&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/pdf/2506.08872&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Bottom line:&lt;/strong&gt; Use AI to &lt;em&gt;amplify&lt;/em&gt; your ideas, not to &lt;em&gt;replace&lt;/em&gt; them—especially if your brain is still under construction.&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://huizhou92.com/p/mit-heavy-use-of-chatgpt-can-lead-to-brain-damage/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Long Time Link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you find my blog helpful, please subscribe to me via &lt;a class=&#34;link&#34; href=&#34;https://huizhou92.com/index.xml&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RSS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Or follow me on &lt;a class=&#34;link&#34; href=&#34;https://x.com/@piaopiaopig&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;X&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you have a &lt;a class=&#34;link&#34; href=&#34;https://medium.huizhou92.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Medium&lt;/a&gt; account, follow me there. My articles will be published there as soon as possible.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>DeepSeek: Disrupting the AI Landscape with Cost-Effective Innovation</title>
        <link>https://huizhou92.com/p/deepseek-disrupting-the-ai-landscape-with-cost-effective-innovation/</link>
        <pubDate>Tue, 04 Feb 2025 18:46:54 +0800</pubDate>
        
        <guid>https://huizhou92.com/p/deepseek-disrupting-the-ai-landscape-with-cost-effective-innovation/</guid>
        <description>&lt;img src="https://images.hxzhouh.com/blog-images/2025/02/2a985170a85b4e9ac863fdbcfc54deb2.png" alt="Featured image of post DeepSeek: Disrupting the AI Landscape with Cost-Effective Innovation" /&gt;&lt;p&gt;DeepSeek&amp;rsquo;s recent breakthroughs have sent shockwaves through the AI industry, with some observers calling it the &amp;ldquo;Temu of AI&amp;rdquo; for its cost-slashing innovations. By challenging traditional narratives dominated by NVIDIA and OpenAI, this Chinese AI model has forced a global reassessment of open-source capabilities and hardware-software co-design strategies. Let&amp;rsquo;s dissect the technical wizardry behind this disruption and its implications for AI&amp;rsquo;s future.&lt;/p&gt;
&lt;!-- more--&gt;
&lt;h2 id=&#34;1-the-inference-efficiency-revolution&#34;&gt;1. The Inference Efficiency Revolution
&lt;/h2&gt;&lt;p&gt;DeepSeek&amp;rsquo;s breakthroughs in &lt;strong&gt;KV cache compression&lt;/strong&gt; and &lt;strong&gt;FP8 low-precision computing&lt;/strong&gt; have reduced inference costs to &amp;lt;10% of conventional methods. This isn&amp;rsquo;t brute-force optimization but surgical algorithmic improvements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dynamic state pruning&lt;/strong&gt; eliminates redundant intermediate computations during chain-of-thought reasoning.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Verifiable reward mechanisms&lt;/strong&gt; validate reasoning steps in real-time, reducing wasteful token generation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hardware-aware kernels&lt;/strong&gt; leverage AMD MI300X&amp;rsquo;s matrix math units for 6-7x speedups.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The implications are profound:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Smartphones could soon handle complex 10-step reasoning tasks locally.&lt;/li&gt;
&lt;li&gt;Cloud API providers face margin compression as open-source alternatives achieve 95% performance at 1/10th cost.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-the-distillation-dilemma&#34;&gt;2. The Distillation Dilemma
&lt;/h2&gt;&lt;p&gt;While model distillation helps smaller models mimic GPT-4&amp;rsquo;s outputs, it creates hidden traps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Diversity Collapse&lt;/strong&gt;: Models may memorize common solution paths instead of learning accurate problem-solving heuristics (e.g., pattern-matching math proofs rather than understanding principles).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ceiling Effect&lt;/strong&gt;: Distilled models can&amp;rsquo;t surpass their teachers&amp;rsquo; capabilities, creating innovation bottlenecks.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The winning formula emerging from China&amp;rsquo;s labs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Phase 1&lt;/strong&gt;: Use distillation for rapid capability bootstrapping.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Phase 2&lt;/strong&gt;: Switch to reinforcement learning with process-oriented rewards to rebuild genuine reasoning muscles.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3-open-sources-asymmetric-warfare&#34;&gt;3. Open Source&amp;rsquo;s Asymmetric Warfare
&lt;/h2&gt;&lt;p&gt;DeepSeek-R1&amp;rsquo;s success reveals a new open-source playbook:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Vertical Domination&lt;/strong&gt;: Fine-tuned 7B models now match GPT-4 in niche domains like legal contract analysis.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hardware Agnosticism&lt;/strong&gt;: Optimized CUDA alternatives for AMD/Homegrown chips break NVIDIA&amp;rsquo;s moat.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compliance Advantage&lt;/strong&gt;: Full data control addresses growing regulatory concerns about closed APIs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Yet closed-source players aren&amp;rsquo;t standing still. OpenAI&amp;rsquo;s rumored 500B-parameter &amp;ldquo;StarGate&amp;rdquo; project hints at next-gen architectures that could reset the competition.&lt;/p&gt;
&lt;h2 id=&#34;4-the-compute-paradox&#34;&gt;4. The Compute Paradox
&lt;/h2&gt;&lt;p&gt;Efficiency gains haven&amp;rsquo;t reduced overall compute demand—they&amp;rsquo;ve redirected it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Architecture Explorers&lt;/strong&gt;: Spending $10M+/experiment on radical designs (e.g., non-Transformer models).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimization Arms Race&lt;/strong&gt;: New techniques like MoE dynamic routing cut training costs by 80% but require continuous R&amp;amp;D.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inference Tsunami&lt;/strong&gt;: Real-time AI agents could increase global inference compute demand 100x by 2026.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Meta&amp;rsquo;s planned 60% YoY capex growth in 2025 confirms this trend—the battle has shifted from brute FLOPs to compute ROI.&lt;/p&gt;
&lt;h2 id=&#34;5-chinas-constraint-driven-innovation&#34;&gt;5. China&amp;rsquo;s Constraint-Driven Innovation
&lt;/h2&gt;&lt;p&gt;Operating under U.S. chip restrictions, Chinese teams have perfected constraint-driven engineering:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Automated Reward Verification&lt;/strong&gt;: Reduced RLHF data needs by 90% through algorithmic self-checking.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pipeline Innovation&lt;/strong&gt;: Achieved 70% utilization on 2000-GPU clusters vs. typical 30% at Western hyperscalers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Chip Customization&lt;/strong&gt;: Co-designing models with domestic ASICs for 4-bit inference without accuracy loss.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While not pursuing AGI moonshots, these &amp;ldquo;good enough&amp;rdquo; solutions rapidly commercialize AI in manufacturing, logistics, and fintech.&lt;/p&gt;
&lt;h2 id=&#34;6-the-next-frontier&#34;&gt;6. The Next Frontier
&lt;/h2&gt;&lt;p&gt;Three developments could redefine AI&amp;rsquo;s trajectory:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;MCTS for Language&lt;/strong&gt;: Integrating Monte Carlo tree search to enable AlphaGo-style &amp;ldquo;thinking about thinking&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stepwise Reward Modeling&lt;/strong&gt;: Scoring each reasoning step like chess moves quality assessments.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vision-Language Synergy&lt;/strong&gt;: Using spatial reasoning from multimodal training to boost STEM problem-solving.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;the-bottom-line&#34;&gt;The Bottom Line
&lt;/h2&gt;&lt;p&gt;DeepSeek&amp;rsquo;s rise symbolizes a pivotal shift from &amp;ldquo;bigger is better&amp;rdquo; to &amp;ldquo;smarter is cheaper.&amp;rdquo; While not eliminating the need for foundational breakthroughs, it proves that disciplined engineering can dramatically lower AI&amp;rsquo;s accessibility threshold. As the industry bifurcates into explorers (chasing AGI) and exploiters (democratizing today&amp;rsquo;s AI), the real winners may be those who master both games simultaneously.&lt;/p&gt;
&lt;p&gt;The AI revolution isn&amp;rsquo;t being televised—it&amp;rsquo;s being distilled, quantized, and deployed on a smartphone near you.&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://huizhou92.com/p/deepseek-disrupting-the-ai-landscape-with-cost-effective-innovation/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Long Time Link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you find my blog helpful, please subscribe to me via &lt;a class=&#34;link&#34; href=&#34;https://huizhou92.com/index.xml&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RSS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Or follow me on &lt;a class=&#34;link&#34; href=&#34;https://x.com/@piaopiaopig&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;X&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you have a &lt;a class=&#34;link&#34; href=&#34;https://medium.huizhou92.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Medium&lt;/a&gt; account, follow me there. My articles will be published there as soon as possible.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Big Tech Is Killing Their Customers</title>
        <link>https://huizhou92.com/p/big-tech-is-killing-their-customers/</link>
        <pubDate>Thu, 09 Jan 2025 16:02:38 +0800</pubDate>
        
        <guid>https://huizhou92.com/p/big-tech-is-killing-their-customers/</guid>
        <description>&lt;img src="https://images.hxzhouh.com/blog-images/2025/01/589319fe1c17cafde315ff3dab8b3f24.webp" alt="Featured image of post Big Tech Is Killing Their Customers" /&gt;&lt;p&gt;Recently, &lt;a class=&#34;link&#34; href=&#34;https://pod.geraspora.de/people/4ce5b01a9e3a1b18d700000a&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Dennis Schubert&lt;/a&gt; posted a fiery update about a crisis in the “diaspora*” project. The platform&amp;rsquo;s network infrastructure was collapsing due to heavy traffic. But what caused this overload? Shockingly, &lt;strong&gt;70% of the requests came from LLM (Large Language Model) bots operated by major tech companies&lt;/strong&gt;. These bots ignored &lt;code&gt;robots.txt&lt;/code&gt; directives, relentlessly scraping every data they could access.&lt;/p&gt;
&lt;!-- more--&gt;
&lt;p&gt;&lt;img src=&#34;https://images.hxzhouh.com/blog-images/2025/01/3cbeeac63070c2d3d19c34cc21828f43.webp&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Dennis discovered that &lt;code&gt;ChatGPT&lt;/code&gt; and &lt;code&gt;Amazon&lt;/code&gt; bots even went as far as to scrape &lt;strong&gt;the entire edit history of Wiki pages&lt;/strong&gt;—every single revision. He couldn’t help but ask:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“What are they trying to achieve? Are they analyzing how text evolves?”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This data hoarding significantly strained diaspora*’s servers, slowing the platform for legitimate users. Dennis tried several countermeasures:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Updating robots.txt&lt;/strong&gt;: Useless, as the bots ignored it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rate-limiting&lt;/strong&gt;: Failed because the bots rotated their IP addresses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Blocking User Agents&lt;/strong&gt;: Ineffective, as the bots disguised themselves as regular browsers.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Frustrated, Dennis likened the situation to a &lt;strong&gt;DDoS attack on the entire internet&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;why-does-big-tech-need-our-data&#34;&gt;Why Does Big Tech Need Our Data?
&lt;/h2&gt;&lt;p&gt;The answer lies in &lt;strong&gt;AI’s insatiable hunger for training data&lt;/strong&gt;.&lt;br&gt;
High-quality datasets are the backbone of AI models, and the industry is running out of fresh material to train on. As OpenAI engineer James Betker once wrote:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As I’ve spent these hours observing the effects of tweaking various model configurations and hyperparameters, one thing that has struck me is the similarities between all the training runs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It’s becoming clear to me that these models are truly approximating their datasets to an incredible degree&lt;/p&gt;
&lt;p&gt;To stay ahead in the AI arms race, tech giants are aggressively scraping data from every corner of the web—personal blogs, independent wikis, and small projects. They don’t just scrape; they &lt;strong&gt;strip the internet bare&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;can-we-fight-back&#34;&gt;Can We Fight Back?
&lt;/h2&gt;&lt;p&gt;Big Tech has teams of experts balancing web scraping and user experience, but small websites and independent projects lack these resources. For individuals, it’s an uphill battle.&lt;/p&gt;
&lt;p&gt;Dennis suggested two unconventional methods to fend off bots:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Tarpit Strategy&lt;/strong&gt;: Generate meaningless random text to trick bots into wasting resources on irrelevant data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;JavaScript Traps&lt;/strong&gt;: Serve bot-detected requests with JavaScript-heavy content, embedding scripts that only bots would execute, such as crypto mining code.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;While these approaches might work, they’re expensive and technically demanding.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-zero-click-internet&#34;&gt;The Zero-Click Internet
&lt;/h2&gt;&lt;p&gt;What’s Big Tech’s ultimate goal?&lt;br&gt;
To &lt;strong&gt;trap users within their ecosystems&lt;/strong&gt;. By leveraging AI to generate “the best content,” they eliminate the need for users to visit other websites. No more outbound links, no more exploring—their AI serves everything directly, with ads seamlessly integrated.&lt;/p&gt;
&lt;p&gt;For individual creators, this means:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SEO doesn’t matter anymore.&lt;/li&gt;
&lt;li&gt;High-quality content won’t reach users.&lt;/li&gt;
&lt;li&gt;Revenue dries up.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Your work is more than fuel for Big Tech’s data engines in this new reality.&lt;/p&gt;
&lt;h2 id=&#34;the-inevitable-decline-of-the-open-web&#34;&gt;The Inevitable Decline of the Open Web
&lt;/h2&gt;&lt;p&gt;Big Tech is reshaping the internet, exploiting data while squeezing value out of independent creators. Fighting back is nearly impossible for small websites. The shift is already happening, and it’s irreversible. Ironically, the open web is dying, and the companies that built it are killing it.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://pod.geraspora.de/posts/17342163&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Dennis Schubert’s Post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;The IT in AI Models is the Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.techspot.com/article/2908-the-zero-click-internet/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;TechSpot: The Zero-Click Internet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;This version maintains the original tone while aligning with Medium’s concise, conversational style. It’s structured for readability and flows logically to keep readers engaged.&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://huizhou92.com/p/big-tech-is-killing-their-customers/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Long Time Link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you find my blog helpful, please subscribe to me via &lt;a class=&#34;link&#34; href=&#34;https://huizhou92.com/index.xml&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RSS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Or follow me on &lt;a class=&#34;link&#34; href=&#34;https://x.com/@piaopiaopig&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;X&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you have a &lt;a class=&#34;link&#34; href=&#34;https://medium.huizhou92.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Medium&lt;/a&gt; account, follow me there. My articles will be published there as soon as possible.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
