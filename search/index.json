[{"content":"Recently, Tao Shu\u0026rsquo;s Blog published an article about reciprocal links, and I took the initiative to request a link exchange. Tao Shu promptly responded and kindly reminded me that adding a favicon (icon) to my blog would help RSS subscribers quickly identify it. At that moment, I wondered, \u0026ldquo;What is a favicon?\u0026rdquo; (The struggles of a backend developer!)\nLater, I consulted DeepSeek:\nIn web design, icons are small yet significant elements. They assist users in quickly identifying a website and enhancing the overall user experience.\nCommon Usage Scenarios: Browser Tab: Displayed next to the webpage title. Bookmark Bar: Shown when users save a webpage. Home Screen Icon: Displayed when a webpage is added to the home screen on mobile devices. PWA (Progressive Web App): Used as the application icon.\nFor example:\nIcon displayed in the browser tab.\nThe icon displayed in the bookmark bar. The icon can be displayed even on Android devices, as the Chrome browser\u0026rsquo;s feature to add a webpage to the home screen allows this.\nHow to Set Up The simplest way to set an icon is to add a line in the \u0026lt;head\u0026gt; section of the webpage.\n1 \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/png\u0026#34; href=\u0026#34;/favicon.png\u0026#34;\u0026gt; If you are using Hugo or other tools, there may be specific settings for the favicon.\nSome large websites, such as google.com and apple.com, may have more considerations, and their setups may differ.\nImportant Considerations The icon size is also essential to optimize the user experience and achieve the best display effect across various scenarios.\nCommon Sizes: 16x16: Browser tab icon. 32x32: Bookmark bar icon. 64x64: High-resolution screen icon. 180x180: iOS device home screen icon. 192x192 and 512x512: PWA icons.\nThis is why we often see multiple icons set on some websites, such as the Hugo website.\nModern browsers support selecting different icon sizes based on various scenarios and screen PPI, striving to achieve the best display effect in all situations.\nChoosing Icon Formats Icons can be created in different image formats, specified by the type attribute. Standard icon formats include:\nICO: Traditional format, good compatibility, supports multiple sizes. PNG: Modern format, supports transparent backgrounds, and is suitable for high-resolution screens. SVG: Vector format, infinitely scalable without losing quality, suitable for responsive design. Multi-Resolution Icons If maintaining multiple icon files seems cumbersome, you can use a multi-resolution icon (Multi-Resolution ICO or Multi-Size ICO), a single file containing various sizes and color depths. This allows multiple bitmaps (BMP or PNG format) to be stored in one file.\nAn ICO file includes:\nFile Header: Defines the ICO file type and the number of images it contains. Image Directory: Describes the size, color depth, offset, and other information for each image. Image Data: Contains the actual pixel data for the images.\nWhen a browser needs an icon, it selects the most appropriate size from the ICO file. The downside is that an ICO file containing multiple sizes may be larger than a file with a single size. This article does not delve deeper into topics such as the manifest.json unique to Android Chrome or the apple-mobile-web-app-capable for Apple devices. Interested readers are encouraged to explore further.\nLong Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2025-02-18T11:30:38+08:00","image":"https://images.hxzhouh.com/blog-images/2025/02/65e64661a046efca8dcbc39b98ae2b91.png","permalink":"https://huizhou92.com/p/the-importance-of-website-icons-favicon/","title":"The Importance of Website Icons (Favicon)"},{"content":"In Go language development, memory leak issues are often challenging to pinpoint. While traditional pprof tools can provide some assistance, their capabilities are limited in complex scenarios. To analyze and resolve these issues more effectively, the CloudWeGo team has developed a new tool called goref.\nBased on Delve, goref can deeply analyze heap object references in Go programs, displaying the distribution of memory references to help developers quickly locate memory leaks or optimize garbage collection (GC) overhead. This tool supports the analysis of runtime processes and core dump files, providing Go developers with a powerful memory analysis tool.\nLimitations Of pprof When encountering memory leaks in Go development, most people first attempt to generate a heap profile to investigate the issue. However, the heap profile flame graph often does not provide much help in troubleshooting because it only records where objects are created. In complex business scenarios, where objects are passed through multiple layers of dependencies or reused from memory pools, it becomes nearly impossible to identify the root cause based solely on the stack information of the creation.\nFor example, in the following heap profile, the FastRead function stack is a deserialization function from the Kitex framework. If a business coroutine leaks a request object, it does not reflect the corresponding leaking code location but only shows that the FastRead function stack occupies memory.\nAs we know, Go is a garbage-collected language, and an object cannot be released primarily because the GC marks it as alive through reference analysis. Similarly, as a GC language, Java has more sophisticated analysis tools, such as JProfiler, which can effectively display object reference relationships. Therefore, we also wanted to implement an efficient reference analysis tool in Go that can accurately and directly inform us of memory reference distribution and relationships, liberating us from the arduous task of static analysis. The good news is that we have nearly completed the development of this tool, which is open-sourced in the goref repository, with usage instructions available in the README document.\nThe following sections will share the design ideas and detailed implementation of this tool.\nImplementation Ideas of Goref GC Marking Process Before discussing the specific implementation, let’s review how the GC marks objects as alive.\nGo employs a tiered allocation scheme similar to tcmalloc, where each heap object is assigned to an mspan during allocation, with a fixed size. During GC, a heap address calls runtime.spanOf to find this mspan from a multi-level index, thus obtaining the base address and size of the original object.\n1 2 3 4 5 6 // simplified code func spanOf(p uintptr) *mspan { ri := arenaIndex(p) ha := mheap_.arenas[ri.l1()][ri.l2()] return ha.spans[(p/pageSize)%pagesPerArena] } By using the runtime.heapBitsForAddr function, we can obtain a GC bitmap for an object address range. The GC bitmap indicates whether each 8-byte aligned address in the memory of an object is a pointer type, thus determining whether to further mark downstream objects.\nFor example, consider the following Go code snippet:\n1 2 3 4 5 6 7 8 9 10 11 12 type Object struct { A string B int64 C *[]byte } // global variables var a = echo() var b *int64 = \u0026amp;echo().B func echo() *Object { bytes := make([]byte, 1024) return \u0026amp;Object{A: string(bytes), C: \u0026amp;bytes} } When the GC scans the variable b, it does not simply scan the memory of the B int64 field; instead, it looks up the base and elem size through the mspan index and then scans, marking the memory of fields A and C, as well as their downstream objects, as alive.\nWhen scanning the variable a, the corresponding GC bit is 1001. How should we interpret this? We can consider that the addresses base+0 and base+24 are pointers, indicating that downstream objects should be scanned further. Here, both A string and C *[]byte contain pointers to downstream objects.\nBased on the above brief analysis, we can see that to find all live objects, the simple principle is to start from the GC Root and scan the GC bits of each object. If an address is marked as 1, we continue scanning downstream. Each downstream address must determine its mspan to obtain the complete object base address, size, and GC bit.\nDWARF Type Information However, merely knowing the reference relationships of objects is almost useless for troubleshooting, as it does not output any effective variable names or type information for developers to locate issues. Therefore, another crucial step is to obtain the variable names and type information of these objects.\nGo is a statically typed language, and objects generally do not directly contain their type information. For instance, when we create an object using obj=new(Object), the actual memory only stores the values of fields A/B/C, occupying only 32 bytes in memory. Given this, how can we obtain type information?\nImplementation Of goref Introduction To Delve Tool Those who have experience in Go development should be familiar with Delve. If you think you haven’t used it, don’t doubt yourself; the debugging functionality you use in the Goland IDE is fundamentally based on Delve. At this point, you may recall the debugging window during your debugging sessions. Indeed, the variable names, values, and types displayed in the debugging window are precisely the type information we need!\n1 2 3 4 5 $ ./dlv attach 270 (dlv) ... (dlv) locals tccCli = (\\\u0026#34;*code.byted.org/gopkg/tccclient.ClientV2\\\u0026#34;)(0xc000782240) ticker = (*time.Ticker)(0xc001086be0) So, how does Delve obtain this variable information? Delve reads the executable file path from the soft link in /proc/\u0026lt;pid\u0026gt;/exe when we attach it to a process. Go generates debugging information during compilation, stored in sections with the .debug_* prefix in the executable file, following the DWARF standard format. The type of information required for reference analysis for global and local variables can be parsed from this DWARF information.\nFor global variables, Delve iterates through all DWARF Entries, parsing those with the Variable tag. These Entries contain attributes such as Location, Type, and Name.\nThe Type attribute records its type information, which can be recursively traversed in DWARF format to determine the type of each sub-object further. The Location attribute is a relatively complex property that records either an executable expression or a simple variable address, serving to determine a variable\u0026rsquo;s memory address or return a register\u0026rsquo;s value. During global variable parsing, Delve uses this to obtain the variable\u0026rsquo;s memory address. The principle for parsing local variables in goroutines is similar to that of global variables, but it is somewhat more complex. For example, it requires determining the DWARF offset based on the PC, and the location expressions are more complicated, involving register access. We will not elaborate on this here.\nBuilding GC Analysis Metadata We can also obtain memory access permissions by utilizing Delve\u0026rsquo;s process attachment and core file analysis capabilities. We mimic the GC marking process for objects, constructing the necessary metadata for the process to be analyzed in the tool\u0026rsquo;s runtime memory. This includes:\nThe address space range of each Goroutine stack in the process to be analyzed, including the stackmap that stores the gcmask for each Goroutine stack, used to mark whether it may point to a live heap object. The address space range of each data/bss segment in the process to be analyzed, including the gcmask for each segment, is also used to mark whether it may point to a live heap object. The above two steps are necessary to obtain GC Roots. The final step is to read the mspan index of the process to be analyzed, along with the base, elem size, gcmask, and other information for each mspan, restoring this index in the tool\u0026rsquo;s memory. These steps outline the general process, which also involves handling some detail issues, such as dealing with GC finalizer objects and special handling of allocation header features in Go version 1.22, which we will not delve into here.\nDWARF Type Scanning With everything in place, we are ready for the most critical step: object reference relationship analysis.\nWe call each GC Root variable the findRef function, accessing the object\u0026rsquo;s memory according to different DWARF types. If it is a pointer that may point to downstream objects, we read the pointer\u0026rsquo;s value and find this downstream object in the GC metadata. We have obtained the object\u0026rsquo;s base address, elem size, gcmask, and other information.\nIf the object is accessed, we record a mark bit to avoid re-accessing the object. By constructing a new variable with the DWARF sub-object type, we recursively call findRef until all known types of objects are confirmed.\nHowever, this reference scanning method is entirely contrary to the GC approach. The main reason is that Go has many unsafe type conversions. For instance, an object created with pointer fields may look like this:\n1 2 3 4 5 func echo() *byte { bytes := make([]byte, 1024) obj := \u0026amp;Object{A: string(bytes), C: \u0026amp;bytes} return (*byte)(unsafe.Pointer(obj)) } From the GC\u0026rsquo;s perspective, although the type has been converted to *byte using unsafe, it does not affect its gcmask marking. Therefore, when scanning downstream objects, we can still scan the complete Object object and identify the bytes downstream object, marking it as alive.\nHowever, DWARF type scanning cannot achieve this. When it encounters the byte type, it will consider it a non-pointer object and skip further scanning. Thus, the only solution is to prioritize DWARF type scanning, and for objects that cannot be scanned, we will use the GC method to mark them.\nTo implement this, whenever we access a pointer to an object using DWARF types, we will mark its corresponding gcmask from 1 to 0. After scanning an object, if there are still non-zero marked pointers within the object\u0026rsquo;s address space, we will record them for final marking tasks. Once all objects have been scanned using DWARF types, we will extract these final marking tasks and perform a secondary scan using the GC method.\nFor example, when accessing the aforementioned Object, its gcmask is 1010. After reading field A, the gcmask changes to 1000. If field C is not accessed due to type conversion, it will be included in the final scan during GC marking.\nIn addition to type conversion, memory out-of-bounds references are also common issues. For instance, in the example code var b *int64 = \u0026amp;echo().B, fields A and C belong to memory that cannot be scanned using DWARF types and will also be counted during the final scan.\nFinal Scan The fields that were type-converted or could not be accessed due to exceeding the DWARF-defined address range, as well as variables like unsafe.Pointer whose types cannot be determined, will all be marked during the final scan. Since these objects cannot be assigned specific types, we only need to record their size and count in the known reference chain.\nIn Go\u0026rsquo;s native implementation, many commonly used libraries utilize unsafe.Pointer, leading to issues with sub-object identification. These types require special handling.\nOutput File Format After scanning all objects, the reference chains, object counts, and object memory spaces are output to a file, aligned with the pprof binary file format and encoded using protobuf.\nRoot Object Format:\nStack Variable Format: Package name + Function name + Stack variable name github.com/cloudwego/kitex/client.invokeHandleEndpoint.func1.sendMsg Global Variable Format: Package name + Global variable name github.com/cloudwego/kitex/pkg/loadbalance/lbcache.balancerFactories Sub-object Format:\nOutputs the type name of the sub-object, such as: net.Conn; If it is a map key or value field, it is output in the form of $mapkey. (type_name) or $mapval. (type_name); If it is an element of an array, it is output in the format [0]. (type_name); for elements greater than or equal to 10, it is output as [10+]. (type_name); Effect Demonstration Below is a flame graph of object references sampled from a real business using the tool:\nThe graph displays the names of each root variable, along with the names and types of the fields they reference. Note: Due to the lack of support for closure type field offsets in DWARF Info before Go 1.23, the closure variable wpool.(*Pool).GoCtx.func1.task cannot currently display downstream objects.\nBy selecting the inuse_objects tag, you can also view the flame graph of object count distribution:\nLong Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2025-02-13T15:05:56+08:00","image":"https://images.hxzhouh.com/blog-images/2025/02/f1e888e98523470202649a4a1fe3d769.png","permalink":"https://huizhou92.com/p/unmasking-go-memory-leaks-cloudwego-open-sources-goref-for-deep-heap-analysis/","title":"Unmasking Go Memory Leaks: CloudWeGo Open Sources goref for Deep Heap Analysis"},{"content":"As Project Scales Expand, Maintaining and Updating Codebases Becomes Increasingly Cumbersome. Whenever a Function, Constant, or Package Path Needs to Be Replaced, Manual Searching and Modification Can Be Time-consuming and Error-prone. Fortunately, the Go Language is Continuously Evolving, and the Recently Accepted Proposal for the go:fix Tool Provides Developers with an Automated Migration Solution. This Article Will Guide You through the Principles, Application Scenarios, and Specific Usage Examples of go:fix.\n1. Introduction to go:fix During daily development, the deprecation and replacement of APIs are inevitable. For instance, when a function is marked as deprecated, we may want all calls to that function to be replaced with a new implementation. Similarly, when a constant is renamed or migrated to another package, we would like the tool to update all references automatically. The proposal outlined in #32816 aims to achieve this goal by adding specific directives in the code to facilitate the automatic migration of simple deprecations.\nThe go:fix tool primarily accomplishes automated migration through two mechanisms:\nInline Functions Forward Constants\nNext, we will detail these two mechanisms and provide usage examples. 2. Inline Functions and Forward Constants 1. Inline Functions When a function is marked for inlining (for example, using the //go:fix inline comment), go:fix will automatically replace calls to that function with its implementation within the function body. This mechanism is commonly used in two scenarios:\nReplacement of Deprecated Functions: When a function is no longer recommended for use, its internal logic can be directly migrated to a new function. For example: 1 2 3 4 5 // Deprecated: prefer Pow(x, 2). //go:fix inline func Square(x int) int { return Pow(x, 2) } If there are calls to Square in the code, the tool will automatically replace them with Pow(x, 2), gradually phasing out the old function.\nPackage Migration: During package upgrades or refactoring, replacing calls to a function from an old package with its implementation in a new package may be necessary. For example: 1 2 3 4 5 6 7 8 package pkg import pkg2 \\\u0026#34;pkg/v2\\\u0026#34; //go:fix inline func F() { pkg2.F(nil) } This way, code calling pkg.F() will automatically update to pkg2.F(nil), simplifying the process of updating package paths.\n2. Forward Constants The forward constants mechanism suits scenarios involving constant renaming or cross-package migration. By adding the //go:fix forward comment before the constant definition, the tool can replace all references to that constant with its target constant. For example:\n1 2 3 4 package example //go:fix forward const Ptr = Pointer If there are calls elsewhere like:\n1 fmt.Println(example.Ptr) After running the go:fix tool, that call will be replaced with:\n1 fmt.Println(example.Pointer) This mechanism supports not only individual constants but can also apply to groups of constants simultaneously.\n3. Advantages and Challenges of go:fix Advantages Low-Risk Migration: Automatic replacements ensure consistent behavior between old and new code, reducing the risk of errors introduced by manual modifications. Increased Development Efficiency: By automating repetitive modification tasks, developers can focus more on core business logic. Consistent Updates: Ensures that all deprecated items in the codebase are uniformly updated, avoiding omissions. Seamless Integration: go:fix is closely integrated with tools like gopls (Go Language Server Protocol), providing real-time feedback to help developers promptly identify and correct issues. Challenges Handling Complex Scenarios: Special cases (e.g., constant groups, usage of iota) require particular attention. Cross-Package Dependencies: When replacements involve different packages, more detailed issues may arise, necessitating correct imports and references for the new package. Non-Deterministic Behavior: The tool needs to be particularly cautious about consistency when handling non-deterministic scenarios like map traversals. 4. Conclusion The introduction of go:fix opens up new possibilities for automated code migration in the Go language. Through simple directive comments (such as //go:fix inline and //go:fix forward), developers can easily achieve automatic replacements for deprecated functions, constants, and even package paths, thereby maintaining the modernity and consistency of the codebase. Whether for large-scale refactoring or gradually phasing out old APIs, go:fix can greatly facilitate project maintenance.\nAs the tool continues to improve and more community feedback is gathered, go:fix will cover more complex scenarios, further enhancing productivity in Go development. If you are also troubled by manual code modifications, consider looking forward to this new tool and experiencing the efficiency and convenience of automation.\n5. References https://github.com/golang/go/issues/32816\nhttps://github.com/golang/tools/blob/master/gopls/internal/analysis/gofix/doc.go\nLong Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2025-02-08T15:17:16+08:00","image":"https://images.hxzhouh.com/blog-images/2025/02/69886f43f924640d4703312dc480bdca.png","permalink":"https://huizhou92.com/p/gofix-a-revolutionary-tool-for-automated-code-migration/","title":"go:fix  A Revolutionary Tool for Automated Code Migration"},{"content":"DeepSeek\u0026rsquo;s recent breakthroughs have sent shockwaves through the AI industry, with some observers calling it the \u0026ldquo;Temu of AI\u0026rdquo; for its cost-slashing innovations. By challenging traditional narratives dominated by NVIDIA and OpenAI, this Chinese AI model has forced a global reassessment of open-source capabilities and hardware-software co-design strategies. Let\u0026rsquo;s dissect the technical wizardry behind this disruption and its implications for AI\u0026rsquo;s future.\n1. The Inference Efficiency Revolution DeepSeek\u0026rsquo;s breakthroughs in KV cache compression and FP8 low-precision computing have reduced inference costs to \u0026lt;10% of conventional methods. This isn\u0026rsquo;t brute-force optimization but surgical algorithmic improvements:\nDynamic state pruning eliminates redundant intermediate computations during chain-of-thought reasoning. Verifiable reward mechanisms validate reasoning steps in real-time, reducing wasteful token generation. Hardware-aware kernels leverage AMD MI300X\u0026rsquo;s matrix math units for 6-7x speedups. The implications are profound:\nSmartphones could soon handle complex 10-step reasoning tasks locally. Cloud API providers face margin compression as open-source alternatives achieve 95% performance at 1/10th cost. 2. The Distillation Dilemma While model distillation helps smaller models mimic GPT-4\u0026rsquo;s outputs, it creates hidden traps:\nDiversity Collapse: Models may memorize common solution paths instead of learning accurate problem-solving heuristics (e.g., pattern-matching math proofs rather than understanding principles). Ceiling Effect: Distilled models can\u0026rsquo;t surpass their teachers\u0026rsquo; capabilities, creating innovation bottlenecks. The winning formula emerging from China\u0026rsquo;s labs:\nPhase 1: Use distillation for rapid capability bootstrapping. Phase 2: Switch to reinforcement learning with process-oriented rewards to rebuild genuine reasoning muscles. 3. Open Source\u0026rsquo;s Asymmetric Warfare DeepSeek-R1\u0026rsquo;s success reveals a new open-source playbook:\nVertical Domination: Fine-tuned 7B models now match GPT-4 in niche domains like legal contract analysis. Hardware Agnosticism: Optimized CUDA alternatives for AMD/Homegrown chips break NVIDIA\u0026rsquo;s moat. Compliance Advantage: Full data control addresses growing regulatory concerns about closed APIs. Yet closed-source players aren\u0026rsquo;t standing still. OpenAI\u0026rsquo;s rumored 500B-parameter \u0026ldquo;StarGate\u0026rdquo; project hints at next-gen architectures that could reset the competition.\n4. The Compute Paradox Efficiency gains haven\u0026rsquo;t reduced overall compute demand—they\u0026rsquo;ve redirected it:\nArchitecture Explorers: Spending $10M+/experiment on radical designs (e.g., non-Transformer models). Optimization Arms Race: New techniques like MoE dynamic routing cut training costs by 80% but require continuous R\u0026amp;D. Inference Tsunami: Real-time AI agents could increase global inference compute demand 100x by 2026. Meta\u0026rsquo;s planned 60% YoY capex growth in 2025 confirms this trend—the battle has shifted from brute FLOPs to compute ROI.\n5. China\u0026rsquo;s Constraint-Driven Innovation Operating under U.S. chip restrictions, Chinese teams have perfected constraint-driven engineering:\nAutomated Reward Verification: Reduced RLHF data needs by 90% through algorithmic self-checking. Pipeline Innovation: Achieved 70% utilization on 2000-GPU clusters vs. typical 30% at Western hyperscalers. Chip Customization: Co-designing models with domestic ASICs for 4-bit inference without accuracy loss. While not pursuing AGI moonshots, these \u0026ldquo;good enough\u0026rdquo; solutions rapidly commercialize AI in manufacturing, logistics, and fintech.\n6. The Next Frontier Three developments could redefine AI\u0026rsquo;s trajectory:\nMCTS for Language: Integrating Monte Carlo tree search to enable AlphaGo-style \u0026ldquo;thinking about thinking\u0026rdquo; Stepwise Reward Modeling: Scoring each reasoning step like chess moves quality assessments. Vision-Language Synergy: Using spatial reasoning from multimodal training to boost STEM problem-solving. The Bottom Line DeepSeek\u0026rsquo;s rise symbolizes a pivotal shift from \u0026ldquo;bigger is better\u0026rdquo; to \u0026ldquo;smarter is cheaper.\u0026rdquo; While not eliminating the need for foundational breakthroughs, it proves that disciplined engineering can dramatically lower AI\u0026rsquo;s accessibility threshold. As the industry bifurcates into explorers (chasing AGI) and exploiters (democratizing today\u0026rsquo;s AI), the real winners may be those who master both games simultaneously.\nThe AI revolution isn\u0026rsquo;t being televised—it\u0026rsquo;s being distilled, quantized, and deployed on a smartphone near you.\nLong Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2025-02-04T18:46:54+08:00","image":"https://images.hxzhouh.com/blog-images/2025/02/2a985170a85b4e9ac863fdbcfc54deb2.png","permalink":"https://huizhou92.com/p/deepseek-disrupting-the-ai-landscape-with-cost-effective-innovation/","title":"DeepSeek: Disrupting the AI Landscape with Cost-Effective Innovation"},{"content":"Go 1.24 brings significant updates to its toolchain, streamlining dependency management, accelerating workflows, and catching bugs earlier. While language features often steal the spotlight, improvements to Go’s tools can dramatically impact developer productivity. Let’s break down two major areas: go tool and vet, with practical examples to show how these changes work in real projects.\nGo Command Improvements Explicit Tool Dependency Tracking Managing versioned executable tools (e.g., code generators like mockery) has long been awkward. Previously, projects relied on tools.go files with empty imports to pin versions—a hacky workaround.\nGo 1.24 introduces a dedicated tool directive in go.mod. Now, you can explicitly declare tool dependencies:\n1 2 # Install mockery v2.52.1 as a tool dependency gotip get -tool github.com/vektra/mockery/v2@v2.52.1 This adds a clean entry to your go.mod:\n1 2 3 4 5 6 7 8 9 10 module your-project.com go 1.24 tool github.com/vektra/mockery/v2 require ( github.com/vektra/mockery/v2 v2.52.1 // indirect ... ) Install or run the tool directly:\n1 2 gotip install tool # Install all declared tools gotip tool github.com/vektra/mockery/v2 --all --output ./mocks # Execute without global installs No more version conflicts across teams or cryptic tools.go files.\nExecutable Caching for Faster Iteration Running tools via go run or go tool now caches their executables in Go’s build cache (previously, only compiled packages were cached). This speeds up repeated runs—a win for code generation or frequent scripting.\nCaveat: The cache grows slightly, but Go automatically prunes stale entries:\nPackage caches: 5 days old #trimLimit Executable caches: 2 days old (#69290) Vet: Stronger Static Analysis New tests Analyzer Misconfigured test functions (e.g., wrong signatures, typos in names) can silently skip tests. Go 1.24’s vet now flags these issues:\n1 2 3 4 // Broken test (missing *testing.T parameter) func TestMyFunction() { fmt.Println(\u0026#34;This test won\u0026#39;t execute!\u0026#34;) } Running go vet ./... catches it:\n1 demo_test.go:5:1: wrong signature for TestMyFunction, must be: func TestMyFunction(t *testing.T) Smarter Printf Checks The printf analyzer now detects non-constant format strings without arguments—a common source of runtime panics:\n1 2 s := \u0026#34;Hello %s\u0026#34; fmt.Printf(s) // Danger: If s contains format verbs, this crashes at runtime. Go 1.24’s vet warns:\n1 demo_test.go:10:13: non-constant format string in call to fmt.Printf Fix it by using fmt.Print(s) for non-formatting cases.\nOther Notable Changes Build pseudo-versions for unreleased modules (#50603). External caching support via GOCACHEPROG (enabled by default, #64876). HTTP authentication extensions with GOAUTH (#26232). Structured go build output (-json flag, #62067). Why This Matters Go 1.24’s tooling updates focus on practical efficiency:\nDependency clarity: Explicit tool tracking replaces fragile workarounds. Faster workflows: Cached executables save time during heavy tool usage. Early error detection: Vet’s sharper diagnostics prevent subtle bugs from reaching CI. While these changes aren’t flashy, they quietly boost productivity and code reliability. Keep an eye on toolchain improvements—they’re often the unsung heroes of a smoother development experience.\nLong Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2025-02-02T12:08:50+08:00","permalink":"https://huizhou92.com/p/go124-in-addition-to-the-standard-library-you-should-probably-pay-more-attention-to-changes-in-go-tools/","title":"Go1.24: In addition to the standard library, you should probably pay more attention to changes in Go tools"},{"content":"In the previous article, I introduced swiss map and a Go implementation by Dolthub. Readers unfamiliar with swiss map should review that piece first.\nWith the upcoming release of Go 1.24, swiss map will replace the existing map implementation in the Go standard library. It maintains full API compatibility while delivering over 50% performance improvements in specific benchmark scenarios.\nCurrently, swiss map is my most anticipated feature in Go 1.24. But does it truly live up to the hype? This article analyzes its core design through three lenses: compatibility, Extensible Hashing implementation, and remaining challenges.\nCompatibility: Seamless Migration Support One of swiss map\u0026rsquo;s key design goals is backward compatibility with Go\u0026rsquo;s legacy map. Conditional compilation flags and type conversions enable zero-code migration. For example, in export_swiss_test.go, the newTestMapType function directly converts legacy map metadata into swiss map\u0026rsquo;s type structure:\n1 2 3 4 5 6 7 // https://github.com/golang/go/blob/3f4164f508b8148eb526fc096884dba2609f5835/src/internal/runtime/maps/export_swiss_test.go#L14 func newTestMapType[K comparable, V any]() *abi.SwissMapType { var m map[K]V mTyp := abi.TypeOf(m) mt := (*abi.SwissMapType)(unsafe.Pointer(mTyp)) // Direct type conversion return mt } This design allows existing code to enable swiss map via the experimental flag GOEXPERIMENT=swissmap (now enabled by default in gotip builds like go1.24-3f4164f5).\nTo revert to the legacy map implementation, use GOEXPERIMENT=noswissmap.\nSwiss Map\u0026rsquo;s Data Structure Extendible Hashing: Efficient Incremental Scaling Beyond compatibility improvements, swiss map introduces Extensible Hashing to enable efficient incremental scaling. Unlike traditional hash tables, which require complete data migration during resizing, Extensible Hashing distributes scaling costs across multiple operations using multi-level directories and table splitting.\nDirectory and Table Hierarchy The Map struct in map.go uses globalDepth and directory to manage hierarchy:\n1 2 3 4 5 6 // https://github.com/golang/go/blob/3f4164f508b8148eb526fc096884dba2609f5835/src/internal/runtime/maps/map.go#L194 type Map struct { globalDepth uint8 // Global depth of directory dirPtr unsafe.Pointer // Pointer to directory (array of tables) // ... } The directory size is 1 \u0026lt;\u0026lt; globalDepth, with each entry pointing to a table. When a table reaches its capacity (maxTableCapacity, default 1024), it triggers a split instead of global resizing.\nSplit Operation A split creates two child tables (left and right) with increased localDepth, redistributing data based on hash bits:\n1 2 3 4 5 6 7 8 // https://github.com/golang/go/blob/3f4164f508b8148eb526fc096884dba2609f5835/src/internal/runtime/maps/table.go#L1043 func (t *table) split(typ *abi.SwissMapType, m *Map) { localDepth := t.localDepth localDepth++ // Child tables have +1 local depth left := newTable(typ, maxTableCapacity, -1, localDepth) right := newTable(typ, maxTableCapacity, -1, localDepth) // ... } New tables allocate contiguous memory blocks via newarray, preserving cache locality.\nData Redistribution: Hash Masking During splits, hash values\u0026rsquo; high-order bits (determined by localDepth) dictate data placement:\n1 2 3 4 5 6 7 8 9 10 // https://github.com/golang/go/blob/3f4164f508b8148eb526fc096884dba2609f5835/src/internal/runtime/maps/table.go#L1052 mask := localDepthMask(localDepth) // e.g., 0x80000000 (32-bit) for ... { hash := typ.Hasher(key, m.seed) if hash \u0026amp; mask == 0 { left.uncheckedPutSlot(...) // Assign to left table } else { right.uncheckedPutSlot(...) // Assign to right table } } Mask Calculation: localDepthMask generates masks like: localDepth=1 → 0x80000000 (32-bit) or 0x8000000000000000 (64-bit) Directory Expansion When a split occurs at the global depth, the directory doubles in size:\n1 2 3 4 5 6 7 8 9 // map.go func (m *Map) installTableSplit(old, left, right *table) { if old.localDepth == m.globalDepth { newDir := make([]*table, m.dirLen*2) // Double directory size // Update directory entries... m.globalDepth++ } // ... } Example: Directory Expansion\nInitial state (globalDepth=1):\n1 2 directory[0] → Table A (localDepth=1) directory[1] → Table A (localDepth=1) After split (globalDepth=2):\n1 2 3 4 directory[0] → Left (hash prefix 00) directory[1] → Left (hash prefix 01) directory[2] → Right (hash prefix 10) directory[3] → Right (hash prefix 11) Key Advantages Locality: Only overloaded tables split Incremental Scaling: Directory grows on demand Cache Efficiency: Continuous memory allocation for table groups Additional Optimizations swiss map optimizes small-element scenarios (≤8 elements) using a single group, minimizing performance penalties for small datasets.\nRemaining Challenges Despite significant improvements, several issues remain:\nConcurrency Limitations The current implementation uses a simple writing flag for concurrent write detection:\n1 2 3 4 5 // https://github.com/golang/go/blob/3f4164f508b8148eb526fc096884dba2609f5835/src/internal/runtime/maps/map.go#L478 func (m *Map) PutSlot(typ *abi.SwissMapType, key unsafe.Pointer) unsafe.Pointer { m.writing ^= 1 // Non-atomic flag // ... } This may cause race conditions in high-concurrency scenarios. Future versions may introduce finer-grained locking.\nMemory Fragmentation The group structure (8 control bytes + 8 key-value slots) may waste memory for small types. For example, int32 keys with int8 values leave 3 bytes unused per slot.\nIterator Complexity The Iter implementation handles directory expansion and table splits, adding complexity:\n1 2 3 4 5 6 7 8 // https://github.com/golang/go/blob/3f4164f508b8148eb526fc096884dba2609f5835/src/internal/runtime/maps/table.go#L742 func (it *Iter) Next() { if it.globalDepth != it.m.globalDepth { // Handle directory expansion it.dirIdx \u0026lt;\u0026lt;= (it.m.globalDepth - it.globalDepth) } // ... } Frequent resizing may impact iterator performance.\nAdditionally, numerous TODOs remain in the latest codebase (see the image below), and questions about their resolution remain unresolved before Go 1.24\u0026rsquo;s release.\nCommunity discussions highlight performance variability, with some reports of regressions.\nhttps://x.com/valyala/status/1879988053076504761\nPerformance Testing Test code: github.com/hxzhouh/gomapbench\nEnvironment: go version devel go1.24-3f4164f5 Mon Jan 20 09:25:11 2025 -0800 darwin/arm64\nAverage performance improvements hover around 20%, with some scenarios showing up to 50% gains. However, these results are machine-specific, and some users report performance regressions. The final evaluation awaits further optimization.\nConclusion Go 1.24\u0026rsquo;s swiss map delivers significant performance gains through compatibility design, Extendible Hashing, and optimized probing sequences. However, challenges remain in concurrency handling and memory efficiency. Developers should consider adopting it in performance-critical contexts while monitoring its evolving implementation.\nReferences Tony Bai\u0026rsquo;s Analysis GitHub Issue #54766 Extendible Hashing Explained Original Implementation Article Long Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2025-01-23T14:34:27+08:00","permalink":"https://huizhou92.com/p/swiss-map-in-go-124-compatibility-extendible-hashing-and-legacy-issues/","title":"Swiss Map in Go 1.24: Compatibility, Extendible Hashing, and Legacy Issues"},{"content":"Gracefully stopping a Pod in Kubernetes is not just a technical process but a crucial aspect of maintaining application stability, ensuring data consistency, and minimizing service disruptions. The process involves several mechanisms and configurations to ensure that Pods can terminate without abruptly cutting off services or losing critical data. Below is a detailed breakdown of how Kubernetes achieves this, underscoring this topic\u0026rsquo;s importance in system reliability.\nWhat Does\u0026quot;Graceful Shutdown\u0026quot; Mean? A graceful shutdown refers to the controlled termination of a Pod, allowing it to complete ongoing tasks, clean up resources, and notify other system components before being removed. This contrasts with a “hard shutdown,” where the Pod is forcefully terminated without any cleanup.\nIn distributed systems, graceful shutdown minimizes disruptions by:\nInforming load balancers or service registries to stop routing traffic to the Pod. Allowing applications within the Pod to close connections, save state, or perform other cleanup tasks. Avoiding abrupt termination that could lead to errors like dropped requests or inconsistent data. Key Components of Graceful Shutdown in Kubernetes 1. PreStop Hook The PreStop hook is an essential feature in Kubernetes that allows you to define custom logic for cleanup before the container receives a termination signal (SIGTERM). It can execute either:\nA shell command (exec) inside the container. An HTTP request (httpGet) to an endpoint exposed by the application. For example:\n1 2 3 4 5 6 7 spec: containers: - name: my-container lifecycle: preStop: exec: command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;/cleanup-script.sh\u0026#34;] This script might handle tasks such as deregistering from external service registries or saving application state.\n2. Termination Signals (SIGTERM and SIGKILL) When a Pod is deleted (e.g., via kubectl delete pod), Kubernetes sends a SIGTERM signal to all containers in the Pod. Applications should listen for this signal and initiate shutdown procedures (e.g., closing database connections or stopping background jobs).\nIf the container does not terminate within the specified grace period (terminationGracePeriodSeconds), Kubernetes escalates by sending a SIGKILL signal, which forcefully stops the container.\n3. Termination Grace Period The terminationGracePeriodSeconds field defines how long Kubernetes waits for a Pod to shut down gracefully after sending SIGTERM. The default value is 30 seconds, but it can be customized based on your application’s needs.\nFor example:\n1 2 spec: terminationGracePeriodSeconds: 60 During this time:\nThe application processes its shutdown logic. If defined, the PreStop hook runs first. If the grace period expires before completion, Kubernetes sends SIGKILL. 4. Endpoint Removal from Services When a Pod is part of a Service (e.g., ClusterIP or LoadBalancer), Kubernetes removes its IP address from the associated Endpoints list as soon as it enters the “Terminating” state. This ensures that no new traffic is routed to the terminating Pod while it completes its shutdown process.\nHowever, ongoing requests may still be routed before removal from Endpoints is completed. To mitigate this issue, applications should implement readiness probes and connection-draining mechanisms.\nChallenges With Stateful Applications For stateless applications (e.g., web servers), graceful shutdown mechanisms are usually sufficient as they do not maintain a persistent state across requests. However, stateful distributed systems like databases (e.g., TiDB) require additional steps during shutdown:\nLeader Transfer: In systems using leader-follower architectures (e.g., Raft-based databases like TiKV), leaders must transfer leadership roles to other nodes before shutting down. Data Synchronization: Ensure all pending writes are flushed and synchronized with replicas. Manual Intervention: Manual intervention may be required instead of forcefully killing Pods if automated cleanup fails due to network issues or bugs. Custom controllers or admission webhooks can enforce stricter validation and control over Pod deletion processes to address these complexities.\nAdvanced Techniques for Ensuring Graceful Shutdown 1. ValidatingAdmissionWebhook A ValidatingAdmissionWebhook can intercept API server requests related to Pod deletion and validate whether all prerequisites for graceful shutdown have been met before allowing deletion.\nWorkflow example:\nUser deletes a Pod. The API server calls an external webhook server. The webhook checks if cleanup tasks (e.g., leader transfer) are complete. If not complete, deletion is rejected until conditions are satisfied. This approach ensures that Pods cannot be deleted prematurely during critical operations like rolling updates or node migrations.\n2. Custom Controllers Custom controllers provide fine-grained control over complex workflows involving stateful applications. They explicitly manage their lifecycle through reconciliation loops in the TiDB Operator.\nSteps include:\nNotify cluster components about node decommissioning. Wait for leader transfers and data synchronization. Proceed with node termination only when safe conditions are met. While effective, writing custom controllers requires significant development effort compared to straightforward solutions like admission webhooks.\nWhy Is Graceful Shutdown Important? Avoid Service Disruptions: Prevents routing traffic to terminated Pods mid-request. Ensure Data Consistency: Allows time for saving state or completing transactions. Minimize User Impact: Smoothly transitions workloads during rolling updates without noticeable downtime. Optimize Resource Usage: Releases resources cleanly without leaks or orphaned connections. Conclusion Kubernetes provides robust mechanisms, such as PreStop hooks, termination signals (SIGTERM, SIGKILL), grace periods (terminationGracePeriodSeconds), and advanced techniques like admission webhooks, to ensure that Pods shut down gracefully while maintaining system stability and user experience quality.\nFor more complex scenarios involving stateful applications or distributed systems requiring strict guarantees around data safety and availability during shutdowns, combining these native features with custom controllers or admission webhooks offers an ideal solution tailored to specific operational requirements.\nReference Pod Lifecycle\nTermination Process\nTiDB Operator GitHub Repository\nKubernetes Dynamic Admission Control\nLong Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2025-01-22T18:35:19+08:00","image":"https://images.hxzhouh.com/blog-images/2025/01/1571f60da6bf2c1cb8e5a5d97cbd8411.png","permalink":"https://huizhou92.com/p/how-kubernetes-ensures-graceful-pod-shutdown/","title":"How Kubernetes Ensures Graceful Pod Shutdown"},{"content":"The request hedging pattern is introduced in the paper The Tail At Scale as a solution by Google to address the long tail effect in microservices. It is also one of the two retry modes in gRPC.\nThe hedging client sends the same request to multiple nodes, and once it receives the first result, it cancels the remaining in-flight requests. This pattern primarily aims to achieve predictable latency.\nFor instance, if our service\u0026rsquo;s call chain consists of 20 nodes, each with a P99 latency of 1 second, there is a statistically significant 18.2% chance that the request time will exceed 1 second.\nUsing the hedging pattern, we consistently obtain results from the fastest node, thus preventing unpredictable long tail latencies (service faults are not within the scope here).\nIn Golang, we can quickly implement the hedging request pattern using context. For example, in the code snippet below, we make five requests to the same backend service, but we only take the first one that returns:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 func hedgedRequest() string { ch := make(chan string) // chan used to abort other requests ctx, cancel := context.WithCancel(context.Background()) for i := 0; i \u0026lt; 5; i++ { go func(ctx *context.Context, ch chan string, i int) { log.Println(\u0026#34;in goroutine: \u0026#34;, i) if request(ctx, \u0026#34;http://localhost:8090\u0026#34;, i) { ch \u0026lt;- fmt.Sprintf(\u0026#34;finsh [from %v]\u0026#34;, i) log.Println(\u0026#34;completed goroutine: \u0026#34;, i) } }(\u0026amp;ctx, ch, i) } select { case s := \u0026lt;-ch: cancel() log.Println(\u0026#34;cancelled all inflight requests\u0026#34;) return s case \u0026lt;-time.After(5 * time.Second): cancel() return \u0026#34;all requests timeout after 5 secs.\u0026#34; } } You can find the complete code at: https://go.dev/play/p/fY9Lj_M7ZYE\nThe benefit of this approach is that we can mitigate long-tail latency in services and maintain the delays between services within a controllable range. However, directly implementing it can lead to multiple times the load, so careful design is necessary.\nWhy Does Long Tail Latency Occur? There are several reasons for long tail latency, such as:\nHybrid deployments have become mainstream, meaning many users compete for critical resources on the same physical machine, potentially leading to long-term effects due to resource scheduling. Garbage Collection (GC), which doesn\u0026rsquo;t require much explanation; Golang\u0026rsquo;s Stop-The-World (STW) can amplify long tail latency. Queuing, including message queues, network delays, etc. Is there a way to avoid the request amplification caused by the hedging request pattern? The blog post Go High-Performance Programming EP7: Use singleflight To Merge The Same Request discusses how to use SingleFlight to merge identical requests. In this scenario, using SingleFlight can partially alleviate duplicate requests.\nAnother approach is to initially send only one request. If the P95 threshold does not respond, immediately request the second node. This method limits duplicate requests to about 5% and significantly reduces long-tail requests.\nIn the paper The Tail At Scale, several other methods are suggested to address long tail requests:\nService differentiation and priority queues: Different service classes can be prioritized to schedule user requests over non-interactive requests. Short low-level queues allow faster execution of higher-level strategies. Reducing head-of-line blocking: Transforming time-consuming requests into more minor, more manageable requests is also a standard web performance enhancement method. Micro-partitioning: Fine-grained load adjustments to minimize the latency impact caused by uneven load distribution. Implementing circuit breakers for poorly performing machines. \u0026hellip;\nDo you have other great ways to deal with long-tail requests? ","date":"2025-01-16T19:19:56+08:00","permalink":"https://huizhou92.com/p/use-request-hedging-to-reduce-long-tail-requests/","title":"Use Request Hedging To Reduce Long-Tail Requests"},{"content":"Background Imagine a scenario with 1 million Uber drivers, where the driver client reports data every 10 minutes. If a driver does not report data within 10 minutes, the server sets that driver to an offline status and does not dispatch rides to them.\nHow can we implement this functionality?\nUsually, we would use third-party components such as redis zset to accomplish this, but what if we don\u0026rsquo;t use third-party components? What about just using memory? There are probably several options:\nUsing a Timer Utilize a Map\u0026lt;uid, last_time\u0026gt; to track the last report time for each UID. Update this Map in real-time whenever a user with UID reports data. Start a timer that periodically scans this Map to check if each UID\u0026rsquo;s last report time exceeds 30 seconds. If it does, execute a timeout handling process. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 var ( userMap sync.Map timeout = 10 * time.Minute ) func checkTimeout() { now := time.Now().Unix() userMap.Range(func(key, value any) bool { uid := key.(string) lastTime := value.(int64) if now-lastTime \u0026gt; int64(timeout) { fmt.Printf(\u0026#34;User %s timed out. Last reported at %d\\n\u0026#34;, uid, lastTime) userMap.Delete(uid) } return true }) } Drawback: This method is inefficient since it requires periodically scanning the entire map, resulting in high time complexity.\nUsing Goroutine Management Another approach is to allocate a Goroutine for each driver to manage their heartbeat timeout.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 timer := time.NewTimer(timeout * time.Second) for { select { case \u0026lt;-heartbeat: // Received heartbeat signal if !timer.Stop() { \u0026lt;-timer.C } timer.Reset(timeout * time.Second) case \u0026lt;-timer.C: // Timeout fmt.Printf(\u0026#34;Driver %s timed out.\\n\u0026#34;, uid) break } } userMap.Delete(uid) Drawback: Although this eliminates polling, creating a Goroutine for each driver consumes significant memory when the number of drivers is very high. Additionally, creating and deleting timers operates at O(log n) time complexity, impacting efficiency.\nAfter this groundwork, we finally arrive at our main subject: the Time Wheel.\nTime Wheel Algorithm The Time Wheel is an interesting algorithm, first presented in a paper by George Varghese and Tony Lauck. Its core logic involves:\nUsing a fixed-size array to represent the time wheel, where each slot corresponds to a time interval. Each slot contains a slice to hold tasks expected to expire in that time period, along with a Map\u0026lt;uid, slot\u0026gt; to track which UID corresponds to which slot. A timer periodically advances the time wheel one position (interval), processing all tasks in the current slot. In this case, we can set the interval to 1 second and the time wheel to have 600 slots. When a driver reports data, the record is inserted into the slot corresponding to the current position minus one.\nHere’s a simple demo:\nhttps://gist.github.com/hxzhouh/5e2cedc633bae0a7cf27d9f5d47bef01\nAdvantages Adding tasks using the time wheel has a time complexity of O(1). The number of slots is fixed, ensuring memory usage remains manageable. It’s suitable for handling a large number of scheduled tasks. Disadvantages The size of the interval limits the time wheel\u0026rsquo;s precision. Deleting tasks can degrade to O(n) complexity if tasks are unevenly distributed. Thus, the time wheel is particularly suited for:\nRapid insertion of many tasks. Scenarios with low precision requirements. A reasonably uniform distribution of tasks. Optimization of the time Wheel Simple Time Wheel This maintains a fixed-length array corresponding to all tasks\u0026rsquo; maximum expiration values (intervals). For example, if there are 10 tasks with expiration times of 1s, 3s, and 100s, an array of length 100 is created, where each index corresponds to each task\u0026rsquo;s expiration. As the clock ticks, expired tasks are identified and processed.\nTo schedule a task, Convert it to its expiration value and place it in the relevant array index (O(1)). When the clock ticks, pull and execute tasks from that index (O(1)). Hash Ordered Time Wheel While the simple time wheel is efficient, maintaining an extensive array with very high expiration times can consume a lot of memory. An improved version uses a hash scheme to map tasks into a fixed-size array, maintaining a \u0026lsquo;bucket\u0026rsquo; for each index that contains tasks sorted by absolute expiration time.\nYet this approach can still face challenges, especially when task lists grow long, leading to performance issues.\nHierarchical Time Wheel This approach employs multiple time wheels, each managing different time intervals, and can effectively mitigate issues seen in simple or hash-ordered time wheels.\nRefer to Kafka\u0026rsquo;s Purgatory for a cascading time wheel implementation:\nThe first level of the time wheel is predetermined, while overflow levels are created dynamically as needed. Tasks initially allocated to higher-level wheels are divided into lower-level wheels as time elapses. Summary Here\u0026rsquo;s a comparison of various algorithms\u0026rsquo; performance\nAlgorithm Task Deletion Complexity Memory Overhead Applicable Scenarios Single Timer O(1) Low Suitable for few tasks with high precision requirements. Multi Timer O(log n) Medium Suitable for moderate numbers with independent tasks, but higher memory usage. Simple Timing Wheel O(n) High Works for evenly distributed tasks with lower timing precision requirements. Hash Timing Wheel O(n) Medium Suitable for large numbers of tasks with uneven distribution and higher tolerance for precision. Hierarchical Timing Wheel O(log n) Low to Medium Ideal for large-scale tasks, complex hierarchical management, and long-lifetime scheduling (e.g., Kafka and Netty). References Real-world implementations of time wheels: Netty\u0026rsquo;s HashedWheelTimer Kafka\u0026rsquo;s [Purgatory](https://www.confluent.io/blog/apache-kafka-purgatory-hierarchical-timing-wheels/ go-zero\u0026rsquo;s NewTimingWheel Hashed and Hierarchical Timing Wheels Long Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2025-01-14T21:20:36+08:00","image":"https://images.hxzhouh.com/blog-images/2025/01/12ffa89f8829f9122cee1eb9daacc043.webp","permalink":"https://huizhou92.com/p/efficient-timeout-management/","title":"Efficient Timeout Management"},{"content":"Background Error handling has always been an essential part of programming. However, the Go language is controversial because of its unique error-handling mode. Any blog about how to hate the Go language will definitely mention \u0026ldquo;tedious error handling.\u0026rdquo; This problem has sparked much discussion in the Go community about how to reduce template code while maintaining clarity and maintainability.\nDetails of Proposal ianlancetaylor made a new proposal #71203 ,to introduce the operator ?, for error handling in Go. is used to simplify Go\u0026rsquo;s error handling. In the future, Go\u0026rsquo;s error handling might look like this.\n1 2 3 4 5 6 7 // now result, err := someFunction() if err != nil { return nil, err } // proposal ? result := someFunction()? In this example, the results of the two writings are equivalent: if someFunction() returns an error, it returns.\nThat\u0026rsquo;s the core of the proposal, the main goal of which is to reduce the amount of templated code while maintaining Go\u0026rsquo;s philosophy of explicitness and simplicity. ? is a syntactic sugar that, when used after a function call that returns multiple values (e.g. (T, error)), automatically checks if the last value is non-zero (indicating an error). The compiler will generate the same code as before for this style of writing, ensuring compatibility.\nIn the formal proposal, ianlancetaylor elaborates on the ? syntax rules:\n? can only appear at the end of an assignment statement or expression statement, and the expression must have a return value For expression statements, ? \u0026ldquo;absorbs\u0026rsquo;\u0026rsquo; the last value of the expression (usually err). For assignment statements, ? \u0026ldquo;absorbs\u0026rdquo; the last value of the expression on the right (usually err) so that the number of values on the right will be one more than the number of variables on the left. This \u0026ldquo;absorbed\u0026rdquo; value is called qvalue and must be of an interface type that implements the error interface. The ? can be followed by a code block. If there is no code block, the function returns immediately if the qvalue is not nil and assigns qvalue to the last return value. If ? is followed by a code block, the code block is executed when qvalue is not nil. A variable named err is implicitly declared within the code block with the same value and type as qvalue. A basic usage scenario might look like this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 r := os.Open(\u0026#34;file.txt\u0026#34;) ? // ?Absorbs error from os.Open, and returns it if it\u0026#39;s not null. func Run() error { Start() ? // If Start returns a non-nil error, return the error immediately. return nil } func process() error { result := doSomething() ? { return fmt.Errorf(\u0026#34;something failed: %v\u0026#34;, err) // qvalue } anotherResult := doAnotherThing(result)? return nil } Pros The most crucial benefit of this proposal is that it will reduce the amount of duplicate code in Go programs, as described in the proposal.\nreduces the error handling boilerplate from 9 tokens to 5, 24 non-whitespace characters to 12, and 3 boilerplate lines to 2.\nCons The biggest drawback is that all Go books and wikis need to be updated, and newcomers may need to understand the concept, as it is not quite the same as any other language implementation. This change will involve a lot of code, including Go src, so the Go Core Team is under much pressure because there is only one chance.\nShadowing ? followed by a block of code that implicitly declares an err variable, which can lead to problems with variable shadowing. An example of this is mentioned in the proposal.\n1 2 3 4 5 6 7 8 9 10 11 for n = 1; !utf8.FullRune(r.buf[:n]); n++ { r.buf[n], err = r.readByte() if err != nil { if err == io.EOF { err = nil // must change outer err break } return } } // code that later returns err In this example, the assignment err = nil has to change the err variable that exists outside of the for loop. Using the ? operator would introduce a new err variable shadowing the outer one.\nIn this example, using the ? operator would cause a compiler error because the assignment err = nil would set a variable that is never used.\nThe Mental Burden of Writing Code Increases 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func F1() error { err := G1() log.Print(err) G2() ? { log.Print(err) } return nil } func F2() error { err := G1() log.Print(err) G2() ? { log.Print(err) } return nil } In this example, both functions are legal. Only the line break of G2 is different, but their behavior is entirely different. This difference cannot be made up by fmt and other means.\nUnchanging Rationality Although Go\u0026rsquo;s error-handling mechanism is often criticized, it remains in place. Therefore, the community needs to consider whether changes are necessary. In the proposal, Ian Lance Taylor repeatedly stated: \u0026ldquo;Perhaps no change is better than this change. Perhaps no change is better than any change.\u0026rdquo; The Go Core Team is not entirely resolute about error handling and improvements. I think\nIt feels more like it was forced by public opinion and pressure from the Go community.\nGeneric: Don\u0026rsquo;t disturb me\nSummary The new proposal highlights the Go Core Team\u0026rsquo;s ongoing attentiveness to the community. The?operator proposal brings a fresh perspective to the Go language\u0026rsquo;s error-handling mechanisms. By introducing a more concise syntax, the proposal aims to significantly reduce the code required for error handling, thereby enhancing the clarity of the main code flow. While some disagreements still exist, it\u0026rsquo;s evident that someone is finally advocating for it.\nLong Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2025-01-11T19:35:01+08:00","image":"https://images.hxzhouh.com/blog-images/2025/02/34ac61629c8b0d315d037016a7283331.png","permalink":"https://huizhou92.com/p/error-handling-in-go-the-new-operator/","title":"Error Handling in Go: The New ? operator"},{"content":"Recently, Dennis Schubert posted a fiery update about a crisis in the “diaspora*” project. The platform\u0026rsquo;s network infrastructure was collapsing due to heavy traffic. But what caused this overload? Shockingly, 70% of the requests came from LLM (Large Language Model) bots operated by major tech companies. These bots ignored robots.txt directives, relentlessly scraping every data they could access.\nDennis discovered that ChatGPT and Amazon bots even went as far as to scrape the entire edit history of Wiki pages—every single revision. He couldn’t help but ask:\n“What are they trying to achieve? Are they analyzing how text evolves?”\nThis data hoarding significantly strained diaspora*’s servers, slowing the platform for legitimate users. Dennis tried several countermeasures:\nUpdating robots.txt: Useless, as the bots ignored it. Rate-limiting: Failed because the bots rotated their IP addresses. Blocking User Agents: Ineffective, as the bots disguised themselves as regular browsers. Frustrated, Dennis likened the situation to a DDoS attack on the entire internet.\nWhy Does Big Tech Need Our Data? The answer lies in AI’s insatiable hunger for training data.\nHigh-quality datasets are the backbone of AI models, and the industry is running out of fresh material to train on. As OpenAI engineer James Betker once wrote:\nAs I’ve spent these hours observing the effects of tweaking various model configurations and hyperparameters, one thing that has struck me is the similarities between all the training runs.\nIt’s becoming clear to me that these models are truly approximating their datasets to an incredible degree\nTo stay ahead in the AI arms race, tech giants are aggressively scraping data from every corner of the web—personal blogs, independent wikis, and small projects. They don’t just scrape; they strip the internet bare.\nCan We Fight Back? Big Tech has teams of experts balancing web scraping and user experience, but small websites and independent projects lack these resources. For individuals, it’s an uphill battle.\nDennis suggested two unconventional methods to fend off bots:\nTarpit Strategy: Generate meaningless random text to trick bots into wasting resources on irrelevant data. JavaScript Traps: Serve bot-detected requests with JavaScript-heavy content, embedding scripts that only bots would execute, such as crypto mining code. While these approaches might work, they’re expensive and technically demanding.\nThe Zero-Click Internet What’s Big Tech’s ultimate goal?\nTo trap users within their ecosystems. By leveraging AI to generate “the best content,” they eliminate the need for users to visit other websites. No more outbound links, no more exploring—their AI serves everything directly, with ads seamlessly integrated.\nFor individual creators, this means:\nSEO doesn’t matter anymore. High-quality content won’t reach users. Revenue dries up. Your work is more than fuel for Big Tech’s data engines in this new reality.\nThe Inevitable Decline of the Open Web Big Tech is reshaping the internet, exploiting data while squeezing value out of independent creators. Fighting back is nearly impossible for small websites. The shift is already happening, and it’s irreversible. Ironically, the open web is dying, and the companies that built it are killing it.\nReferences Dennis Schubert’s Post The IT in AI Models is the Dataset TechSpot: The Zero-Click Internet This version maintains the original tone while aligning with Medium’s concise, conversational style. It’s structured for readability and flows logically to keep readers engaged.\nLong Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2025-01-09T16:02:38+08:00","image":"https://images.hxzhouh.com/blog-images/2025/01/589319fe1c17cafde315ff3dab8b3f24.webp","permalink":"https://huizhou92.com/p/big-tech-is-killing-their-customers/","title":"Big Tech Is Killing Their Customers"},{"content":"In software development, have you ever encountered a situation where you are developing a shopping cart feature that requires storing product information in a database when a user adds an item? You might have designed a straightforward function as follows:\n1 2 3 4 func addToCart(item Item) { // add to db .... } In this method, you assume that the database storage operation will always succeed without considering potential errors such as connection failures, write errors, or incorrect data formats.\nIf you assume that the operation will always succeed and do not account for error conditions, you may encounter issues highlighted by Hyrum\u0026rsquo;s Law.\nWhat is Hyrum\u0026rsquo;s Law? Hyrum\u0026rsquo;s Law is a principle in software development that states:\n“When you depend on an API, you also depend on its implementation details.”\nIn other words, even if an API is defined and documented, the various ways it can be implemented should be considered when using that API. You must consider the implementation details, not just the declared functionality.\nHyrum\u0026rsquo;s Law is named after Google engineer Hyrum Wright, who introduced the concept in a presentation.\nHyrum Wright emphasized that developers should pay more attention to API implementation details, as these aspects can affect their code\u0026rsquo;s future maintainability and stability.\nImportance of Hyrum\u0026rsquo;s Law Hyrum\u0026rsquo;s Law serves several significant purposes:\nAwareness of Implementation Details: This reminds developers that it is crucial to understand an API\u0026rsquo;s functionalities, implementation details, and limitations when using it. APIs are commonplace in software development, helping us quickly implement features and improve productivity.\nImplications for Code Behavior: How APIs are implemented can significantly affect code behavior and may lead to unforeseen issues. Hyrum’s Law emphasizes the need to evaluate implementation details and stability carefully to prevent potential problems and enhance code maintainability and reliability.\nAdaptation to Changes: The law also underscores software development\u0026rsquo;s iterative and evolving nature. As software requirements and technical environments change, API implementations may also grow. Thus, staying abreast of these changes is vital for maintaining software quality and stability.\nA Case Study In Go\u0026rsquo;s source code, there are echoes of Hyrum\u0026rsquo;s Law.\nFor instance, in ./src/net/http/request.go:1199:\n1 2 3 4 func (e *MaxBytesError) Error() string { // Due to Hyrum\u0026#39;s law, this text cannot be changed. return \u0026#34;http: request body too large\u0026#34; } This raises the question: why is it a concern? Changing an error message might affect other users relying on that message.\nI have encountered similar issues where upstream database changes in ErrCode affected my business logic.\nPractical Recommendations for Hyrum\u0026rsquo;s Law Here are some suggestions to effectively implement Hyrum’s Law in practice:\nUnderstand API Documentation: Before using an API, carefully read its documentation and specifications to understand its features, uses, limitations, and potential issues.\nWrite Robust Code: When utilizing APIs, ensure robust error handling and consider various error and exception scenarios to bolster code reliability and stability.\nUse Stable API Versions: When multiple API versions are available, opt for stable versions and avoid deprecated or obsolete ones whenever possible.\nConduct Integration and Unit Tests: Write integration and unit tests to validate the API\u0026rsquo;s correctness and stability and promptly address any emerging issues.\nBe Aware of API Dependencies: Pay attention to API dependencies, avoiding unnecessary dependencies while ensuring that those you do use are reliable and stable.\nPromptly Address API Changes: Stay informed about and adapt to changes in API implementations to maintain software quality and stability amidst evolving requirements.\nIn summary, adhering to these best practices can help you better apply Hyrum\u0026rsquo;s Law, enhance the maintainability and stability of your code, and adapt to changes and innovations in the software development process.\nAnti-Patterns Related to Hyrum\u0026rsquo;s Law In addition to standard practices, here are some prevalent anti-patterns that detract from the principles of Hyrum’s Law:\nDirect Dependency on Implementation: Some developers may ignore API specifications and directly rely on its concrete implementations, leading to tight coupling and increased fragility and maintenance challenges.\nNeglecting API Limitations: Failing to consider API limitations and exceptions can result in code uncertainty and unreliability, as developers might assume the API will always function correctly.\nDirect Use of Low-Level Libraries: Bypassing API encapsulation by using low-level libraries or components can complicate code and hinder maintainability.\nIgnoring API Version Changes: Developers who overlook API version changes may face compatibility issues and maintenance difficulties, risking disconnection from technological advancements.\nUnreasonable Additions or Removals of Dependencies: Improperly managed dependencies can create confusion and complexity, hindering maintainability.\nAvoiding these anti-patterns can ensure better alignment with Hyrum’s Law, ultimately leading to more maintainable and stable code.\nConclusion Hyrum\u0026rsquo;s Law is a vital principle that underscores the importance of focusing on a system\u0026rsquo;s core functionalities and considering various dependencies and side effects present within complex systems.\nAssuming everything is functioning correctly without accounting for dependencies and side effects can result in unexpected issues that might lead to system failures or other significant problems.\nAs developers, we must be mindful of the traps posed by Hyrum\u0026rsquo;s Law and consider best practices to ensure the stability and reliability of our code.\nIn conclusion, Hyrum\u0026rsquo;s Law is of paramount importance. For developers, understanding this principle and applying it in practice will enhance code quality and reliability, subsequently providing users with better experiences.\nLong Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2025-01-08T11:55:37+08:00","image":"https://images.hxzhouh.com/blog-images/2025/01/d84fece1bcb074ea83d61d4e2c43a8a2.webp","permalink":"https://huizhou92.com/p/software-engineering-hyrums-law/","title":"Software Engineering: Hyrum's Law"},{"content":"Many people feel lost, not because they lack a vision for the future, but because they’ve never reflected on the path they’ve traveled. How often do we rush through a year only to find ourselves in the same place—or even further behind?\nLife doesn’t favor the inattentive; it can be merciless if we fail to pause and reflect. A year-end review is more than a formality; it’s a mirror to help you see the gaps, celebrate the wins, and lay the foundation for an even better year ahead.\nThis isn’t about impressing a boss or ticking off a to-do list. It’s about taking stock of your life, giving yourself clarity, and recognizing what truly matters. Without reflection, the past will only repeat itself. Don’t let 2024’s lessons slip away—use them to transform 2025.\nHere are 35 powerful questions to guide your year-end reflection and set you up for success.\nLife Events and Goals: Rediscovering Your Purpose What made me feel most alive and fulfilled in 2024? Why? Was there a moment where I acted with extraordinary bravery? What decision led to that? How did I spend my time and energy this year? Was it worth it? Did I balance my investments in relationships, career, health, hobbies, and finances? Which goal am I most proud of achieving? Which goal remains unmet, and why? Was there a moment this year when I felt I had truly changed? What sparked that shift? Did my 2024 experiences inspire new hopes for the future? How will I achieve them? Have I learned to embrace my imperfections and live authentically? In one sentence, how would I sum up my 2024? Emotions and Relationships: Nurturing the Connections That Matter Who reshaped my understanding of friendship, intimacy, or love this year? Why? Who made the most positive impact on my life? Did I express my gratitude? What did I do this year that made me proud in my relationships? Did someone change my perspective or behavior? Who, and how? Was there a relationship that taught me the power of forgiveness? What happened? Did any relationships disappoint me? What did I learn? Who deserves my heartfelt thanks this year, and why? What emotional experience deepened my appreciation for those around me? Interests and Creativity: Rekindling Joy and Curiosity What was the most exciting or creative thing I tried in 2024? How did it impact me? Did I feel “fully alive” at any point this year? What brought that feeling? Was there an interest that brought me peace or fulfillment? What made it so special? Did I develop a new habit that enriched my life—like writing, painting, or music? Did I discover new hobbies or let go of ones that no longer served me? Was there an interest that led me to a meaningful connection? What united us? Self-Awareness and Influence: Becoming the Best Version of Yourself How have I deepened my understanding of my strengths and weaknesses? Did I positively influence others this year? How did that impact me? What causes or goals did I passionately support? What did they mean to me? If others were to describe my 2024, what growth or changes would they highlight? What lessons did my biggest challenges teach me? How have they made me stronger? Summary and Outlook: Setting the Stage for 2025 What moment this year brought me the most joy? Why? Is there a decision I regret? What would I choose if I could do it over? What was my most significant setback in 2024, and what did I learn from it? What surprise amazed me this year? How did it unfold? Who or what was my most significant source of strength in 2024? What message would I send to my future self? In one sentence, how has 2024 changed me compared to last year? Conclusion Taking time to reflect isn’t just about looking back—it’s about moving forward with intention. Let these 35 questions guide you to a more meaningful, purposeful, and transformative 2025.\n","date":"2025-01-06T10:41:18+08:00","image":"https://images.hxzhouh.com/blog-images/2025/01/71f1e81aafa5fe1fd658694483e83c22.webp","permalink":"https://huizhou92.com/p/reflect-recharge-reignite-35-questions-to-transform-your-2024-into-a-year-of-growth/","title":"Reflect, Recharge, Reignite: 35 Questions to Transform Your 2024 into a Year of Growth"},{"content":" htmx is a novel front-end development tool that complements existing HTML, CSS, and JavaScript technologies rather than completely replacing them.\nhtmx can be used with other technologies and tools like Vue.js, React, and Angular. Its core philosophy is \u0026ldquo;using HTML as the programming language for applications,\u0026rdquo; meaning dynamic behavior is achieved by adding various attributes to HTML tags, eliminating the need to write extensive JavaScript code.\nHtmx Design Philosophy Leverage familiar HTML and CSS knowledge, requiring minimal JavaScript code. Reduce the complexity of front-end development and lessen dependence on complex JavaScript frameworks. Make code more readable and maintainable by specifying behavior directly in HTML through declarative markup. Enable progressive enhancement, allowing web applications to run on varying technology stacks, improving compatibility and accessibility. htmx provides a series of custom HTML attributes that allow developers to simplify JavaScript, enabling dynamic interactive functions that initially require complex scripts using only HTML tags. This allows developers to focus on content and structure rather than implementation details of behavioral logic, thus developing modern web applications more quickly and efficiently.\nInstallation and Usage To install and start using htmx, you can integrate it into your web page in several simple ways.\n1. Inclusion via CDN Add the following \u0026lt;script\u0026gt; tag directly to your HTML file to load htmx. Place this code at the end of the \u0026lt;head\u0026gt; or \u0026lt;body\u0026gt; tag.\n1 \u0026lt;script src=\u0026#34;https://unpkg.com/htmx.org\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Alternatively, you can use other CDN services or specify a particular version of htmx:\n1 \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/htmx.org@1.6.0\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Using a CDN is the easiest and fastest way to start using htmx.\n2. Installation Using Npm or Yarn If your project uses Node.js and npm (or yarn), you can install htmx through a package manager.\nUsing npm: 1 npm install htmx.org --save Using yarn: 1 yarn add htmx.org Then import htmx into your JavaScript module or entry file: 1 import \u0026#34;htmx.org\u0026#34;; Alternatively, you can import it into a specific script file and include it in your final bundled file through a build process (e.g., Webpack or Parcel).\nGolang If you are also a Gopher, then I recommend you to use the html/template package to use HTMX, there is a simple example afterward。\nhtmx provides a set of easy-to-use HTML attributes for implementing various dynamic behaviors, such as:\nhx-get: for loading data asynchronously. hx-post: for submitting form data. hx-trigger: used to define the trigger behavior. By adding these attributes to the HTML markup, we can easily implement a variety of interactive features without having to write a lot of JavaScript code. For example, to load data when a button is clicked, we could write something like this:\n1 2 3 \u0026lt;button hx-get=\u0026#34;/api/data\u0026#34; hx-target=\u0026#34;#result\u0026#34;\u0026gt;Load Data\u0026lt;/button\u0026gt; \u0026lt;div id=\u0026#34;result\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; The above code defines a button; when the user clicks on the button, htmx will use AJAX to load the data returned by the specified URL and populate the result into the selected elements.\nhtmx It also supports controlling the update of elements on the page through server responses。\nYou can utilize the hx-swap attribute to define how the response content should be replaced or inserted in the DOM, or you can use the extended CSS selector syntax to specify the target element for the update.\n为了优To optimize the user experience, htmx also provides a loading indicator that shows the user-visible feedback when an AJAX request is initiated, such as a loading animation.\nhtmx has many other advanced features, such as View Transitions, history support, events, and logging. These features allow developers to create rich and efficient user interfaces while reducing the effort of writing and maintaining JavaScript code.\nMore Examples (using Golang) Full code click here.\n1. AJAX Load Content Suppose you have a server-side URL /get-content that returns some HTML content. You can use the hx-get attribute to create a button that, when clicked, will asynchronously load the content and insert it into the specified element on the page.\n1 2 3 \u0026lt;h2\u0026gt;1. AJAX load\u0026lt;/h2\u0026gt; \u0026lt;button hx-get=\u0026#34;/get-content\u0026#34; hx-target=\u0026#34;#content\u0026#34;\u0026gt;click me\u0026lt;/button\u0026gt; \u0026lt;div id=\u0026#34;content\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 2. Submit Form You can use htmx to submit forms asynchronously. This means that the user can submit the form without refreshing the page, and the response to the form will be displayed directly on the page.\n1 2 3 4 5 6 7 8 9 10 \u0026lt;h2\u0026gt;2. from submit\u0026lt;/h2\u0026gt; \u0026lt;form hx-debug=\u0026#34;true\u0026#34; hx-post=\u0026#34;/submit-form\u0026#34; hx-target=\u0026#34;#form-result\u0026#34; hx-swap=\u0026#34;outerHTML\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;name\u0026#34; placeholder=\u0026#34;inputName\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;summit\u0026#34;\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;div id=\u0026#34;form-result\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 3. Search Using htmx it is also very easy to create live search forms where the search results can be constantly refreshed as the user types in the search box, an example of which is shown below:\n1 2 3 4 5 6 7 8 9 \u0026lt;h2\u0026gt;3. search\u0026lt;/h2\u0026gt; \u0026lt;input name=\u0026#34;q\u0026#34; hx-get=\u0026#34;/search\u0026#34; hx-target=\u0026#34;#search-results\u0026#34; hx-trigger=\u0026#34;keyup changed delay:500ms\u0026#34; placeholder=\u0026#34;input search term\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;search-results\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; The content typed in the search box will be sent to /search?q=\u0026lt;value entered\u0026gt;, and the server needs to return the appropriate search results based on the query parameter q.\n4. Delete With htmx, you can easily realize to delete items without refreshing the page:\n1 2 3 4 5 \u0026lt;h2\u0026gt;4. delete\u0026lt;/h2\u0026gt; \u0026lt;div id=\u0026#34;item-123\u0026#34;\u0026gt; project 123 \u0026lt;button hx-delete=\u0026#34;/delete-item/123\u0026#34; hx-target=\u0026#34;#item-123\u0026#34; hx-swap=\u0026#34;outerHTML\u0026#34;\u0026gt;Delete \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; When the button is clicked, a DELETE request is sent to the server to delete the item with ID 123.\n5. Load More Create a \u0026ldquo;Load More\u0026rdquo; button that, when clicked, loads the next page of content.\n1 2 3 4 5 6 \u0026lt;h2\u0026gt;5. load more\u0026lt;/h2\u0026gt; \u0026lt;div id=\u0026#34;items\u0026#34;\u0026gt; \u0026lt;div\u0026gt;input 1\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;input 2\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;button hx-get=\u0026#34;/get-more-items?page=2\u0026#34; hx-target=\u0026#34;#items\u0026#34; hx-swap=\u0026#34;afterend\u0026#34;\u0026gt;load more\u0026lt;/button\u0026gt; In practice, you will need to dynamically update the page parameter in the hx-get property in order to load the correct content page.\nInstead of relying on JavaScript, htmx allows you to add rich interactivity through HTML, and these examples show just a few of the features of htmx. You can learn more about advanced features and usage by checking out the official htmx documentation.\nList of Command Tags htmx works through a set of custom HTML attributes, which usually begin with the prefix hx-. These attributes can be added to the standard HTML tag to provide various dynamic behaviors. The following are some commonly used htmx attributes (commands and labels):\nCore Attributes hx-get: Initiate an AJAX GET request. hx-post: Initiate an AJAX POST request. hx-put: Initiate an AJAX PUT request. hx-delete: Initiate an AJAX DELETE request. hx-patch: Initiate an AJAX PATCH request. hx-trigger: Define events that trigger AJAX requests, such as click, keyup, etc. hx-target: Specify which element will be replaced or updated by the requested response. hx-swap: Define how to insert the response content into the target element, such as innerHTML, outerHTML, beforebegin, afterend, etc. Enhance Attributes hx-params: Control which parameters are included in the request, such as none, *, vals, etc. hx-include: Clearly specify which element values should be included in the request parameters. hx-indicator: Specify one or more elements and display them as loading indicators when the AJAX request is in progress. hx-push-url: Control whether to push the URL of AJAX request to the browser\u0026rsquo;s history. hx-prompt: Prompt the user to enter the message before initiating the request. hx-confirm: Ask the user to confirm before performing the operation. Response Processing hx-select: Select part of the content from the AJAX response to update instead of the entire response. hx-headers: Allow the setting of custom request headers. Event Attributes hx-on: Specify specific events and their handling, such as hx-on=\u0026quot;click: doSomething\u0026quot;. Web Sockets hx-ws: Define the behavior related to WebSocket connection. Tool Attributes hx-boost: Automatically enhance the links and forms on the page to make them work asynchronously through AJAX. hx-history-elt: Specify an element whose content change will trigger the update of the browser history. My Opinion on HTMX Everyone has many different views on this technology.\nThis is said to have happened 15 years ago, and integrating the front and back ends is not conducive to maintenance. Some people also say that \u0026ldquo;handwriting grammar tree in XML\u0026rdquo; is outdated and unsuitable for modern large-scale software engineering. \u0026ldquo;High power tools for HTML\u0026rdquo; is written on the homepage of the [official website] (https://htmx.org/) of HTMX. The success of a technology depends on the degree of recognition by most people in the future.\nMy current opinion on HTMX is as follows:\nThe core idea of HTMX is to use HTML language to obtain data, as well as asynchronous form submissions and other interactive scenarios. There is no need to use JS knowledge and some frameworks, nor to learn front-end construction engineering.\nThe back-end returns its front-end code, which is suitable for back-end engineers to make simple interactive interfaces. If you need to make complex pages, HTMX does not support or currently has limited support. HTMX has many advanced features, and it is not particularly easy to master.\nHTMX allows developers not only to learn JavaScript but also to learn HTMX, and the learning curve does not decrease.\n参考连接 ","date":"2025-01-01T16:01:07+08:00","image":"https://images.hxzhouh.com/blog-images/2025/01/e6c9c9915295ea19685f460d4e7e13e5.png","permalink":"https://huizhou92.com/p/htmx-first-experience/","title":"HTMX First Experience"},{"content":"0190dffef1ad726bd83fab761dd389c6\nHave you ever seen a string of numbers like this in your database or system? It is most likely a UUID. It\u0026rsquo;s not gibberish; it\u0026rsquo;s been around for decades as an RFC standard and has evolved through 7 versions。 Today, we\u0026rsquo;ll learn about UUID.\nWhat is a UUID? A UUID (Universally Unique Identifier) is a 128-bit value widely used for unique identification in distributed systems. It is formatted as 32 hexadecimal digits separated by hyphens, typically represented as:\n1 xxxxxxxx-xxxx-Mxxx-Nxxx-xxxxxxxxxxxx Here, M indicates the UUID version, and N represents the variant.\nDefined by the Open Software Foundation and documented in RFC 4122, UUIDs ensure uniqueness without central coordination. They are commonly used in databases, file systems, and session identifiers. In 2024, RFC 9562 introduced three additional versions—6, 7, and 8—to address the limitations of earlier versions.\nA Brief History of UUID Versions UUIDs have evolved to meet time-sensitive application needs. The most commonly used versions include:\nUUIDv1: Using time and node information, incorporating timestamps and MAC addresses. While effective for uniqueness, it poses privacy concerns as MAC addresses can expose sensitive information. UUIDv4: Generated randomly, offering simplicity and privacy at the cost of potential (though highly improbable) collisions. This version is widely adopted in scenarios where sequential ordering is not critical. UUIDv3 and UUIDv5: Use hashing (MD5 for v3 and SHA-1 for v5) to derive UUIDs from namespace identifiers and names, ensuring deterministic results for the same inputs.\nThe newer versions introduced in RFC 9562 bring significant improvements: UUIDv6: A restructured version of v1 with enhanced privacy and optimized for time-sequential ordering. UUIDv7: Designed to provide time-based sequential ordering, making it ideal for database indexing and distributed systems. UUIDv8: Allows custom fields for application-specific metadata, offering unparalleled flexibility. Understanding UUIDv7: The Modern Improvement UUIDv7 addresses earlier versions\u0026rsquo; key shortcomings, particularly in database indexing and distributed systems. By using a time-ordered structure, it ensures:\nEfficient Indexing: Time-based sequential ordering reduces fragmentation in database indexes, leading to improved query performance. High Scalability: Suitable for distributed environments where unique, ordered identifiers are essential. Privacy: Avoids incorporating sensitive information like MAC addresses. For example, generating a UUIDv7 involves encoding the timestamp into the identifier, ensuring sequential order even in distributed systems. Tools like Google\u0026rsquo;s UUID library support UUIDv7 generation in various programming languages.\n1 2 3 4 5 6 7 8 9 10 11 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/google/uuid\u0026#34; ) func main() { id, _ := uuid.NewV7() fmt.Println(\u0026#34;Generated UUIDv7:\u0026#34;, id) } For more about the UUIDv7 specification, see RFC 9562 Section 5.\nUUIDv8: Flexibility for the Future（It\u0026rsquo;s not official yet.） UUIDv8 introduces a groundbreaking capability: custom bits for application-specific needs. This version allows embedding metadata directly into the UUID, making it highly adaptable for:\nIoT Devices: Embedding device-specific information. Cross-System Data Transfers: Including contextual metadata for easier tracking. Custom Applications: Tailoring UUIDs for domain-specific requirements. The flexibility of UUIDv8 comes with trade-offs, such as ensuring the custom fields remain unique within the application context. As adoption grows, best practices and libraries will likely emerge to standardize these implementations.\nFor details on UUIDv8, refer to RFC 9562 Section 6.\nComparing UUID Versions ![[Pasted image 20241224151902.png]]\nVersion Generation Method Key Features Use Cases v1 Time + MAC Address High uniqueness, privacy concerns Legacy systems, internal tools v4 Random Simplicity, high privacy Web applications, general-purpose v6 Time-based (restructured) Sequential, privacy improvements Modern databases v7 Time-ordered (RFC 9562) Optimized for indexing Distributed systems, logs v8 Custom fields Highly flexible IoT, specialized applications Beyond UUID: Alternatives and Inspirations The development of UUIDv7 and UUIDv8 was informed by analyzing alternative ID generation methods such as:\nULID: Combines timestamp-based ordering with randomness, ensuring monotonicity. Snowflake: Used by Twitter, includes a timestamp, machine ID, and sequence number. KSUID: A K-sortable unique identifier optimized for distributed systems. While these alternatives are effective in specific contexts, UUIDs offer a standardized, cross-platform solution for most applications.\nConclusion and Recommendations The evolution of UUIDs reflects the growing complexity of distributed systems and the need for efficient, secure, and flexible unique identifiers. As newer versions like UUIDv7 and UUIDv8 gain traction, developers should:\nChoose the Right Version: Use UUIDv7 for time-ordered requirements and UUIDv8 for custom metadata needs. Leverage Libraries: Adopt established libraries to ensure compliance with RFC specifications. Stay Informed: Monitor UUID standards and library updates to exploit emerging features. By understanding and utilizing the correct UUID version for your application, you can ensure scalability, performance, and security in your systems.\nCall to Action\nWhat are your experiences with UUIDs? Have you tried UUIDv7 or UUIDv8 in your projects? Share your thoughts in the comments. Don’t forget to follow for more insights on Golang and distributed systems practices!\n","date":"2024-12-24T14:52:08+08:00","image":"https://images.hxzhouh.com/blog-images/2024/12/60a7013668113f1c9bcb509dc1db2620.png","permalink":"https://huizhou92.com/p/from-uuid-to-uuidv7-and-beyond-the-evolution-of-unique-identifiers/","title":"From UUID to UUIDv7 and Beyond: The Evolution of Unique Identifiers"},{"content":"Go 1.24: New STD-lib synctest\nIntroduction Testing concurrent code in Go has always been challenging. This is especially true for test cases involving time-dependent behavior. Traditional testing methods often rely on the actual system clock and synchronization mechanisms, leading to slow and unreliable tests.\nFor instance, when testing the expiration feature of the go-cache library, a common approach would look like this:\ngo-cache is a popular Go caching library that supports time-to-live (TTL) and periodic cleanup of expired cache items. It is well-suited for caching data and automatically deletes expired entries.\nList 1: TestGoCacheEntryExpires testing cache in the usual way\n1 2 3 4 5 6 7 8 9 10 11 func TestGoCacheEntryExpires(t *testing.T) { c := cache.New(5*time.Second, 10*time.Second) c.Set(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;, cache.DefaultExpiration) v, found := c.Get(\u0026#34;foo\u0026#34;) assert.True(t, found) assert.Equal(t, \u0026#34;bar\u0026#34;, v) time.Sleep(5 * time.Second) v, found = c.Get(\u0026#34;foo\u0026#34;) assert.False(t, found) assert.Nil(t, v) } Running this test requires waiting for 5 seconds for the cache entry to expire:\n1 2 3 ➜ synctest git:(main) ✗ time go test go-cache_test.go ok command-line-arguments 5.012s go test go-cache_test.go 0.38s user 1.27s system 27% cpu 6.049 total The testing/synctest Approach Starting with Go 1.24, the testing/synctest package significantly improves this process. Here\u0026rsquo;s how the same test case can be rewritten using testing/synctest\nTest Example 2: TestGoCacheEntryExpiresWithSynctest testing cache in the synctest\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func TestGoCacheEntryExpiresWithSynctest(t *testing.T) { c := cache.New(2*time.Second, 5*time.Second) synctest.Run(func() { c.Set(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;, cache.DefaultExpiration) if got, exist := c.Get(\u0026#34;foo\u0026#34;); !exist \u0026amp;\u0026amp; got != \u0026#34;bar\u0026#34; { t.Errorf(\u0026#34;c.Get(k) = %v, want %v\u0026#34;, got, \u0026#34;bar\u0026#34;) } time.Sleep(1 * time.Second) if got, exist := c.Get(\u0026#34;foo\u0026#34;); !exist \u0026amp;\u0026amp; got != \u0026#34;bar\u0026#34; { t.Errorf(\u0026#34;c.Get(k) = %v, want %v\u0026#34;, got, \u0026#34;bar\u0026#34;) } time.Sleep(3 * time.Second) if got, exist := c.Get(\u0026#34;foo\u0026#34;); exist { t.Errorf(\u0026#34;c.Get(k) = %v, want %v\u0026#34;, got, nil) } }) } Running this test\n1 2 3 4 ➜ synctest git:(main) ✗ time GOEXPERIMENT=synctest gotip test -run TestGoCacheEntryExpiresWithSynctest PASS ok blog-example/go/go1.24/synctest 0.009s GOEXPERIMENT=synctest gotip test -run TestGoCacheEntryExpiresWithSynctest 0.36s user 1.23s system 171% cpu 0.933 total The test now completes in just 0.009s.\nWhy is it so fast?\nUnveiling testing/synctest testing/synctest simplifies testing concurrent code by using virtual clocks and goroutine groups (also known as \u0026ldquo;bubbles\u0026rdquo;). This makes tests both fast and reliable. The package offers two core functions:\n1 2 func Run(f func()) { synctest.Run(f) } func Wait() { synctest.Wait() } Key Features Run: Executes the provided function f in a new goroutine under its bubble, ensuring that the virtual clock controls all related goroutines. Wait: Synchronizes the bubble’s goroutines and blocks the main goroutine until all others are in a durably blocked state. A durably blocked state occurs when goroutines are waiting on:\nChannel send/receive within the bubble. select statements involving bubble channels sync.Cond.Wait time.Sleep\nHowever, goroutines blocked by system calls or external events are not considered durably blocked. Virtual Clocks Each bubble has a virtual clock starting from 2000-01-01 00:00:00 UTC. The clock advances only when all goroutines are idle. This allows time.Sleep calls will be completed instantly if all goroutines are idle, significantly speeding up tests.\nList 3: TestSynctest Virtual Clocks are not affected by program run time\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func TestSynctest(t *testing.T) { synctest.Run(func() { before := time.Now() fmt.Println(\u0026#34;before\u0026#34;, before) f1 := func() { for i := 0; i \u0026lt; 10e9; i++ { // time consuming, It\u0026#39;s about 3s in my machine } } go f1() synctest.Wait() //wait f1 after := time.Now() // time is not affected by the running time of f1 fmt.Println(\u0026#34;after\u0026#34;, after) }) } Output:\n1 2 3 4 5 6 ➜ synctest git:(main) ✗ time GOEXPERIMENT=synctest gotip test -run TestSynctest before 2000-01-01 08:00:00 +0800 CST m=+946164282.733407335 after 2000-01-01 08:00:00 +0800 CST m=+946164282.733407335 PASS ok blog-example/go/go1.24/synctest 3.131s GOEXPERIMENT=synctest gotip test -run TestSynctest 3.45s user 1.25s system 133% cpu 3.533 total Even computationally intensive functions do not affect the virtual clock’s time.\nConclusion The testing/synctest package introduces a powerful way to test concurrent Go code efficiently and reliably by leveraging virtual clocks and goroutine control. It eliminates the need for real-time waiting and makes time-dependent tests faster and more predictable.\nReference synctest.go\n","date":"2024-12-23T17:44:34+08:00","image":"https://images.hxzhouh.com/blog-images/2024/12/260aeb6207be4b765d34328515c144dc.png","permalink":"https://huizhou92.com/p/go-1.24-new-std-lib-synctest/","title":"Go 1.24: New STD-lib synctest"},{"content":"Previously, I wrote an article about the runtime.SetFinalizer that is used to be called when an object is cleaned up, but some issues with this function cause it to be used less frequently.\nhttps://github.com/golang/go/issues/67535\nSetFinalizer must always refer to the first word of an allocation. This means programmers must know what an \u0026lsquo;allocation\u0026rsquo; is, whereas that distinction isn\u0026rsquo;t generally exposed in the language. There cannot be more than one finalizer on any object. Objects with finalizers involved in any reference cycle will silently fail to be freed, and the finalizer will never run. Objects with finalizers require at least two GC cycles to be freed. For the above reasons, a new function runtime.AddCleanUp has been added in Golang 1.24 to replace runtime.SetFinalizer.\nNeither runtime.AddCleanup nor runtime.SetFinalizer is guaranteed to execute.\nThe design goal of AddCleanup is to address many of the problems with runtime.SetFinalizer, in particular avoids object resurrection, thus allowing for the timely cleanup of objects, and supporting cyclic cleanup of objects.\nAPI\n1 func AddCleanup[T, S any](ptr *T, cleanup func(S), arg S) Cleanup Based on this, a Demo for RAII (Resource Acquisition Is Initialization) can be written. In the go weak article, we implemented a fixed-length cache, changed the code a bit, add a newElemWeak method that automatically calls delete to remove the key from the cache when the oldest elem is evicted. We don\u0026rsquo;t need to manage it manually.\n1 2 3 4 5 6 7 func (c *WeakCache) newElemWeak(elem *list.Element) weak.Pointer[list.Element] { elemWeak := weak.Make(elem) runtime.AddCleanup(elem, func(name string) { delete(c.cache, name) }, elem.Value.(*CacheItem).key) return elemWeak } A Few Things to Keep in Mind AddCleanup places a few constraints on ptr and supports attaching multiple cleanup functions to the same pointer. However, if ptr is reachable from cleanup or arg, it will never be reclaimed (memory leak), and the cleanup function will never run. For the moment, this scenario doesn\u0026rsquo;t pan out. However, a pattern like GODEBUG=gccheckmark=1 may detect this in the future.\nFor example\nhttps://gist.github.com/hxzhouh/5bb1d55259dcb4dab87b37beaef9bea2\nfmt.Println(\u0026quot;close f\u0026quot;) will not be printed, and the file will not be closed.\nWhen binding multiple cleanups to a single ptr, its running order may be variable since cleanup runs in a separate goroutine.\nIn particular, if several objects point to each other and become unreachable simultaneously, their cleanup functions can all be run and in any order. This is true even if the objects form a loop (runtime.SetFinalizer creates a memory leak in this case).\nfor example\nhttps://gist.github.com/hxzhouh/ca402c723faa78726baba7e56bff573a\nReferences https://github.com/golang/go/issues/67535 ","date":"2024-12-17T18:37:49+08:00","image":"https://images.hxzhouh.com/blog-images/2024/12/53b24938b44977786f01d0db8768c34e.png","permalink":"https://huizhou92.com/p/golang-1.24-runtime.addcleanup/","title":"Golang 1.24: runtime.AddCleanup"},{"content":"Go 1.24 introduces a new std-lib package called weak, which allows you to create safe references to *T without preventing the garbage collector (GC) from reclaiming the memory associated with *T.\nThe weak package provides ways to safely reference memory weakly without preventing the garbage collector\u0026rsquo;s reclamation.\nMuch like OS.ROOT, weak is a feature that has existed in other programming languages for some time, including:\nIn Java, WeakReference and SoftReference are classic implementations primarily used for caching and object pools. These references allow automatic garbage collection when the JVM detects a memory shortage. In Python, the weakref module allows the creation of weak references, commonly used to prevent circular reference issues or for caching. In C++, std::weak_ptr was introduced alongside std::shared_ptr to solve circular dependency problems with shared pointers. In Rust, Weak is the weak reference version of Rc and Arc, which helps prevent circular references and provides more flexible memory management. Simple Definition of weak The weak package is defined, with just a Make method and a Value method.\nBy using weak.Make, a weak.Pointer is created, and if the original *T has not been GC, we can access its address through weak.Pointer.Value. If the object has been collected, the Value will return nil.\nYou can see an example implementation of weak here: Example Code on Gist\nExample Output: 1 2 3 4 5 6 7 8 ➜ weak git:(main) ✗ gotip version go version devel go1.24-18b5435 Sun Dec 15 21:41:28 2024 -0800 darwin/arm64 ➜ weak git:(main) ✗ gotip run main.go originalObject:addr 14000010050 weakPtr addr:{1400000e0d0},size:8 First GC :value: Hello, World! originalObject clean at: 1734340907 Second GC: Weak reference value is nil In this example:\nWe create a string variable originalObject and use weak.Make to create a weak.Pointer called weakPtr. During the first garbage collection (GC), since originalObject is still in use, weakPtr.Value returns the address of originalObject. In the second GC, since originalObject is no longer used, it is collected by the GC, and weakPtr.Value returns nil. Also, runtime.AddCleanup, a new feature in Go 1.24, works similarly to runtime.SetFinalizer by allowing you to execute code when an object is garbage collected. This feature will be covered in more detail in a later post.\nKey Takeaways: weak.Make creates a weak.Pointer, which hides the real memory address but does not affect the GC. If the real object is garbage collected, weak.Pointer.Value will return nil. Since we don\u0026rsquo;t know when the real object will be reclaimed, always check the return value of weak.Pointer.Value. The Practical Use of weak Canonicalization Maps You might remember the unique feature introduced in Go 1.23, which uses a single pointer (8 bytes) to represent multiple identical strings, saving memory. The weak package can achieve similar functionality (In fact, in Go 1.24, unique has been refactored using weak).\nImplementing a Fixed-Size Cache 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 type WeakCache struct { cache map[string]weak.Pointer[list.Element] // Use weak references to store values mu sync.Mutex storage Storage } // Storage is a fixed-length cache based on doubly linked tables type Storage struct { capacity int // Maximum size of the cache list *list.List } // Set func (c *WeakCache) Set(key string, value any) { // If the element already exists, update the value and move it to the head of the chain table if elem, exists := c.cache[key]; exists { if elemValue := elem.Value(); elemValue != nil { elemValue.Value = \u0026amp;CacheItem{key: key, value: value} c.storage.list.MoveToFront(elemValue) elemWeak := weak.Make(elemValue) c.cache[key] = elemWeak return } else { c.removeElement(key) } } // Remove the oldest unused element if capacity is full if c.storage.list.Len() \u0026gt;= c.storage.capacity { c.evict() } // Add new element elem := c.storage.list.PushFront(\u0026amp;CacheItem{key: key, value: value}) elemWeak := weak.Make(elem) c.cache[key] = elemWeak } In this example, we use a fixed-size list and a Map to store the position of each key in the list, where the value is a weak.Pointer to a list.Element. When we add a new cache item, we first check if the list is full. If so, the oldest item is evicted. When a key exists in the cache, Map[key].Value will return the address of the list element. If the item is evicted, Map[key].Value returns nil.\nThis design helps to create an efficient, fixed-size cache system. Using weak with a lock-free queue structure allows for even more efficient data handling.\nI find weak to be quite helpful in specific scenarios. It\u0026rsquo;s easy to use, and I plan to incorporate it into my projects.\nFurther Reading on weak: Go 1.24 Weak Documentation Go GitHub Issue #67552 ","date":"2024-12-16T19:06:40+08:00","image":"https://images.hxzhouh.com/blog-images/2024/12/edbf334955e117265be0194bba455ee3.png","permalink":"https://huizhou92.com/p/go1.24-new-std-lib-weak/","title":"GO1.24: New Std-Lib weak"},{"content":"Golang 1.24 has entered a freeze period, and many features are now available in Go 1.24 Release Notes. In the next few days, I\u0026rsquo;ll learn about the new features and changes that will be added to Golang 1.24. If you want to know about the latest developments in Go, please follow me.\nIn this article, we will learn about the new Standard library os.Root.\nProposal Directory traversal vulnerabilities are a typical class of vulnerability in which an attacker tricks a program into opening a file it did not intend. These attacks often provide a relative pathname such as ../../../etc/passwd, which results in access outside an intended location. CVE-2024-3400 is a recent, real-world example of directory traversal leading to an actively exploited remote code execution vulnerability.\nThere are already similar implementations in other languages and operating systems, such as:\nPython\u0026rsquo;s chroot: Limit the root directory to a specific directory through os.chroot(). Linux file system namespace: limit the view of the process through mount and chroot. We can write a demo to test it.\nFirst, construct a \u0026ldquo;confidential\u0026rdquo; file\n1 echo \u0026#39;password123\u0026#39; \u0026gt;\u0026gt; /tmp/password Then, write a Golang function that opens a file in the current directory.\n1 2 3 4 5 6 7 8 9 10 11 12 func main() { fileName := os.Args[1] //readFile localFilePath := \u0026#34;.\u0026#34; filePath := fmt.Sprintf(\u0026#34;%s/%s\u0026#34;, localFilePath, fileName) content, err := os.ReadFile(filePath) if err != nil { fmt.Printf(\u0026#34;Error reading file %s: %s\\n\u0026#34;, fileName, err) return } fmt.Printf(\u0026#34;File %s opened successfully. file content %s\\n\u0026#34;, fileName, content) } However because of the unreliable parameters passed in, the code could access places outside the scope of the privilege.\n1 2 3 4 5 6 ➜ os_root git:(main) ✗ pwd /Users/hxzhouh/workspace/github/me/blog-example/go/go1.24/os_root ➜ os_root git:(main) ✗ ./main ../../../../../../../../../tmp/password ./../../../../../../../../../tmp/password File ../../../../../../../../../tmp/password opened successfully. file content password123 In Go 1.24, a new type of OS.Root was added, allowing file system operations in a specific directory. The entire system is centered around this new type. The corresponding core function is OS.OpenRoot, which opens a directory and returns an OS.Root. Methods on os.Root` are only allowed to operate within a directory and are not allowed to point to paths to locations outside the directory**, including paths that follow symbolic links outside the directory. (Defends against the scope of the attack mentioned in the background of the previous proposal)\nLet\u0026rsquo;s modify the code in the same way that Go1.24 did\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func main() { fileName := os.Args[1] root, err := os.OpenRoot(\u0026#34;.\u0026#34;) if err != nil { panic(err) } file, err := root.Open(fileName) if err != nil { fmt.Println(fmt.Sprintf(\u0026#34;Error opening file %s: %s\\n\u0026#34;, fileName, err.Error())) return } content := make([]byte, 1024) c, err := file.Read(content) if err != nil { panic(err) } content = content[:c] fmt.Printf(\u0026#34;File %s opened successfully. file content %s\\n\u0026#34;, fileName, content) } Running again\n1 2 3 4 5 ➜ os_root git:(main) ✗ gotip version go version devel go1.24-fafd447 Wed Dec 11 15:57:34 2024 -0800 darwin/arm64 ➜ os_root git:(main) ✗ gotip build -o go1.24 main.go ➜ os_root git:(main) ✗ ./go1.24 ../../../../../../../../../tmp/password Error opening file ../../../../../../../../../tmp/password: openat ../../../../../../../../../tmp/password: path escapes from parent An error will be reported if the folder is out of the parent hierarchy.\nPlease refer to the official documentation for more APi interfaces. After 1.24 is officially released, I guess many third-party libraries will be adapted as soon as possible; basically, all the other languages have this function.\n","date":"2024-12-10T20:32:53+08:00","permalink":"https://huizhou92.com/p/go1.24-new-std-lib-os.root/","title":"GO1.24: New Std-Lib os.Root"},{"content":"For a long time, VPNs have been the preferred technology for secure remote access. However, with the popularization and normalization of remote/hybrid work, traditional VPNs have proven inadequate in dealing with complex network environments and new security threats, exposing many fatal flaws.\nThis article will introduce the nine hottest VPN alternative technologies in the next few years.\nWhy Do We Need to Replace VPN? The application of VPN in large-scale remote work has shown various limitations, including:\nExpanded Attack Surface: VPN connections extend the insecure network where users are located to the enterprise network, increasing the possibility of attacks. Limited Encryption Capability: VPNs usually only encrypt transmission traffic and lack comprehensive security stack support. Weak User Authentication: Many VPNs do not enforce multi-factor authentication (MFA), making them easy to invade. Frequent Vulnerabilities: For example, SonicWall SSLVPN and Pulse Secure VPN have been found to have serious vulnerabilities many times and have become targets of attacks. The famous Colonial Pipeline Ransomware Attack precisely took advantage of the leaked usernames and passwords of VPN devices, resulting in the complete control of the network.\nThe risks and shortcomings of traditional VPNs are already obvious. Enterprises must make strategic investments in VPN alternative technology solutions and evaluate some key factors when considering alternative remote access solutions. The most important thing is to include the zero-trust principle: requiring strong authentication for each connection attempt, evaluating compliance, implementing the principle of least privilege, and establishing a trusted connection whenever an attempt is made to access company data or services.\nAnother key focus point when choosing a VPN alternative technology solution is to support modern management. Centralized management is the first step, and automated functions (such as patch management, policies (authentication, encryption, risk scoring, etc.), and integration with other components of the security stack) can mitigate modern risks and attack vectors.\nThe following is a detailed analysis of nine VPN alternative technologies. It not only lists their core functions but also explores practical application scenarios and key considerations in implementation, providing enterprises with richer security solutions:\nZero Trust Network Access (ZTNA) Zero Trust Network Access (ZTNA) is essentially proxy access to applications and data on the network. Before access is granted, users and devices will be challenged and verified. The zero-trust approach can perform the essential functions of a VPN, such as granting access to certain systems and networks, but adds an extra layer of security through least-privilege access, authentication, employment verification, and credential storage. Therefore, if an attacker successfully infects a system, the damage is limited to what that system can access. The zero-trust model can also include network monitoring solutions to detect suspicious behavior.\nCore Functions:\nContinuous Authentication: Conduct multi-factor authentication (MFA) for users and devices. Principle of Least Privilege: Limit user access and only grant the minimum privileges required to complete tasks. Dynamic Access Control: Analyze risks in real-time and adjust access privileges according to the risk level. Application Scenarios:\nDistributed Teams: Help multinational companies protect the access of distributed employees. Sensitive Data Environments include access control of customer data in financial institutions. Implementation Key Points:\nIntegrating with existing identity management systems (such as Active Directory) is necessary. Configure detailed access policies to reduce the impact of false positives on the user experience. Representative Manufacturers:\nZscaler Okta Palo Alto Networks Secure Access Service Edge (SASE) In the Zero Trust Network Access (ZTNA) model, each user and device will be verified and inspected before being allowed access, both at the network and application levels. However, zero trust is only part of the solution; all traffic from one endpoint to another cannot be monitored. SASE provides simplified management and operation, reduced costs, and improved visibility and security through an additional layer of network functions and an underlying cloud-native security architecture.\nCore Functions:\nIntegration of Network and Security Functions, Including SD-WAN, cloud-native firewalls (FWaaS), and DNS protection. Cloud-Native Architecture: Improve network performance and reduce hardware deployment costs. Global Coverage: Achieve secure access for global users through distributed data centers. Application Scenarios:\nGlobal Business Expansion: Help enterprises quickly deploy secure remote connections in multiple countries. Remote Learning Platforms: Protect the data privacy of students and teaching staff. Implementation Key Points:\nEnsure the interoperability of different components, such as the seamless integration of ZTNA, SWG, and CASB. Select SASE suppliers with high availability to ensure the continuity of critical business. Representative Manufacturers:\nCisco Netskope Fortinet Software Defined Perimeter (SDP) Software Defined Perimeter (SDP) is usually implemented within a broader zero-trust strategy. It is a network perimeter based on software rather than hardware and is an effective alternative to traditional VPN solutions. It allows MFA to divide the network and supports rules that restrict access by specific users. Once suspicious behavior is detected, SDP can more easily block access to resources, isolate potential threats, minimize the damage caused by attacks, and maintain productivity in the case of false positives instead of completely disabling devices and making users unable to do meaningful work. The software-defined aspect of SDP also enables automation, allowing other tools in the network to interact with SDP and mitigate these risks in real-time when dangerous behavior is identified. In the era of intelligent and automated cyberattacks, the automation capabilities of SDP are critical.\nCore Functions:\nLogical Isolation: Access control is based on identity rather than network location. Dynamic Threat Isolation: Automatically block access after detecting abnormal activities. High Degree of Automation: Real-time adjustment and response through machine learning. Application Scenarios:\nDynamic Workplaces: Support employees working in different locations while ensuring security. Partner Access Control: Limit the network access rights of external partners. Implementation Key Points:\nIntegrate with existing network tools, such as SIEM (Security Information and Event Management). Design flexible rules to balance security and business efficiency. Representative Manufacturers:\nPerimeter 81 Appgate Cloudflare Software Defined Wide Area Network (SD-WAN) VPNs rely on a router-centered model to distribute control functions in the network, where routers route traffic according to IP addresses and access control lists (ACL). However, the Software Defined Wide Area Network (SD-WAN) relies on software and centralized control functions. It can handle traffic according to the organization’s needs based on priority, security, and quality of service requirements, thus guiding the traffic in the WAN. SD-WAN is particularly important because edge computing accounts for an increasingly high proportion of enterprise networks. SD-WAN can dynamically manage these scattered connections without using hundreds or thousands of sensors (many of which are deployed in less secure locations) that require VPN connections or firewall rules.\nCore Functions:\nDynamic Traffic Management: Optimize traffic routing based on application priority and network conditions. Built-in Security: Support encryption, authentication, and real-time threat detection. Centralized Management: Manage multiple branch office networks through a single interface. Application Scenarios:\nIndustrial Internet of Things (IIoT): Manage and protect large-scale IoT device networks. Multi-branch Enterprises include retail chains or warehousing and logistics across regions. Implementation Key Points:\nConsider real-time sensitive applications\u0026rsquo; quality of service (QoS) requirements (such as video conferencing). Regularly update software and configurations to respond to emerging threats. Representative Manufacturers:\nVMware Silver Peak (HPE) Aryaka Identity and Access Management (IAM) and Privileged Access Management (PAM) Compared with traditional VPNs that usually only require passwords, solutions that adopt a comprehensive verification process to confirm the validity of login attempts provide higher protection. With Identity and Access Management (IAM), network administrators can ensure that each user has authorized access rights and can track each network session. IAM is an alternative to VPN and a viable solution for protecting applications and services. It is the foundation of many other solutions in this article. Simplified identity and authentication policy management enhances every system that uses it for authentication and utilizes risk-based authentication and MFA to enhance security when appropriate.\nCore Functions:\nIAM: Centralize the management of user authentication and authorization, support risk assessment, and single sign-on (SSO). PAM: Protect high-privilege accounts and support regular password rotation and activity monitoring. Application Scenarios:\nHigh-Sensitivity Operations: Protect IT administrators and key system accounts. Industries with Strict Compliance Requirements, Such as HIPAA compliance in the medical industry. Implementation Key Points:\nConfigure dynamic access risk assessment combined with artificial intelligence (AI). Regularly train users to reduce the possibility of credential abuse. Representative Manufacturers:\nOkta CyberArk OneLogin Unified Endpoint Management Tools (UEM) Andrew Hewitt, a senior analyst at Forrester, said that conditional access through Unified Endpoint Management (UEM) tools can provide a VPN-free experience; the agent running on the device will evaluate various conditions before allowing someone to access specific resources. “For example, the solution can evaluate device compliance, identity information, and user behavior to determine whether the person can access enterprise data. Usually, UEM providers will integrate with ZTNA providers to increase protection.”\nCore Functions:\nConditional Access: Before granting access, check the device status (patches, encryption, configuration). Real-Time Monitoring: Support remote locking, data erasure, and threat detection. Comprehensive Compatibility: Cover multiple devices such as desktops, laptops, smartphones, and tablets. Application Scenarios:\nMobile Office: Applicable to employees using personal devices to access enterprise resources. High-Frequency Device Updates include the POS systems in retail stores. Implementation Key Points:\nEnsure compatibility with other security tools such as ZTNA or SASE. Clarify user privacy policies to avoid disputes caused by device management. Representative Manufacturers:\nVMware Workspace ONE Microsoft Intune Ivanti Virtual Desktop Infrastructure (VDI) or Desktop as a Service (DaaS) Hewitt pointed out that the Virtual Desktop Infrastructure (VDI) or Desktop as a Service solution “essentially transmits computing from the cloud (or local server), so there will be no local data on the device.” He added that sometimes organizations will use it as an alternative to VPN but still need to conduct checks at the device level and user authentication to protect access. “However, the advantage of this is that, unlike traditional VPNs, VDI does not copy any data from the virtual session to the local client.”\nCore Functions:\nData Isolation: All operations are carried out in a virtual environment, and data is not stored locally. Centralized Control: Simplify patch management and the implementation of security policies. Flexible Expansion: Quickly add or remove users according to requirements. Application Scenarios:\nSensitive Industries: Protect customer data in the legal, insurance, and medical fields. Disaster Recovery Environments: Quickly recover critical business during a disaster. Implementation Key Points:\nInvest in a high-performance virtualization platform to ensure the user experience. Configure additional access controls to avoid damage by malicious users. Representative Manufacturers:\nCitrix VMware Horizon Amazon WorkSpaces Secure Web Gateway (SWG) The Secure Web Gateway (SWG) can protect Web applications hosted in local or private clouds. Although SWG is a component of the SASE architecture, it can also be implemented independently of the overall SASE strategy to implement policies around authentication, URL filtering, and data loss prevention. It can even prevent malware from passing through connections. SWG is usually directly connected to line-of-business applications. Software agents can also be installed in the local network to connect to applications or services. This flexibility makes SWG a simple choice for improving the security situation of enterprises without major changes to the architecture.\nCore Functions:\nURL Filtering: Block access to malicious websites based on policies. Data Loss Prevention (DLP): Prevent sensitive information from being leaked through Web channels. Threat Protection: Detect and block malware transmitted through the network. Application Scenarios:\nRemote Access: Protect employees’ access to enterprise applications through public Wi-Fi. Cloud Application Security: Protect services deployed by enterprises in private or public clouds. Implementation Key Points:\nIntegrate with the organization’s SIEM and SOAR systems to enhance incident response capabilities. Ensure compliance with industry compliance requirements (such as GDPR). Representative Manufacturers:\nZscaler Symantec Forcepoint Cloud Access Security Broker (CASB) The Cloud Access Security Broker (CASB) is a component of SASE and can be independently deployed to supplement or replace the needs of a VPN. CASB can implement security policies (authentication requirements, encryption configurations, malware detection, managed/non-managed device access, etc.) between end users and SaaS applications. Although this use case does not meet the definition of a VPN alternative (requiring access to local company resources), it replaces some enterprise controls that traditionally could only be achieved by guiding users through a central control point. It is a typical VPN use case.\nCore Functions:\nUnified Policy Management: Monitor the interaction between users and cloud services. Device Visibility: Distinguish between managed and unmanaged devices. Real-Time Threat Detection: Identify suspicious behavior and block malicious operations. Application Scenarios:\nMulti-Cloud Management: Ensure consistent security policies in a multi-cloud environment. SaaS Application Security: Includes enterprise-level applications like Salesforce and Office 365. Implementation Key Points:\nConfigure detailed policies to cover multiple devices and access scenarios. Continuously monitor user activities and optimize access rights. Representative Manufacturers:\nNetskope McAfee MVISION Cloud Microsoft Defender for Cloud Apps ","date":"2024-12-09T15:07:08+08:00","image":"https://miro.medium.com/v2/resize:fit:1400/1*UNopDeZUWU-b3bzZWVpW2A.png","permalink":"https://huizhou92.com/p/top-9-vpn-alternative-technologies-for-future-remote-access/","title":"Top 9 VPN Alternative Technologies For Future Remote Access"},{"content":"BigCache[1] is a high-performance in-memory caching library designed for applications that demand high concurrency and low latency. This article explores BigCache\u0026rsquo;s performance optimization techniques, including its sharding mechanism, efficient hashing algorithms, usage of read-write locks, and more, with detailed explanations based on the source code.\nShard \u0026amp; Locking From a user\u0026rsquo;s perspective, a cache functions like a large hashtable to store key-value pairs (k/v). Could this be achieved with map[string][]byte and sync.RWMutex?\nYes, but only for low-performance requirements. Although sync.RWMutex optimizes reads and writes, it serializes concurrent writes, even for different keys. This creates bottlenecks under high write concurrency, blocking multiple goroutines and allowing only one writer at a time.\nBigCache addresses this by adopting a sharding mechanism inspired by Java\u0026rsquo;s ConcurrentMap. It divides a large hashtable into multiple smaller shards, each protected by its lock. This strategy, commonly used in high-concurrency scenarios like MongoDB\u0026rsquo;s sharding, effectively reduces contention. Golang also has a third-party implementation: concurrent-map[2].\nSource Code Example: Sharding in BigCache 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // https://github.com/allegro/bigcache/blob/a2f05d7cbfdc7000a8b7e1c40f27d3f239c204df/bigcache.go#L58 type BigCache struct { shards []*cacheShard shardMask uint64 config Config } func newBigCache(ctx context.Context, config Config, clock clock) (*BigCache, error) { ...... cache := \u0026amp;BigCache{ shards: make([]*cacheShard, config.Shards), lifeWindow: lifeWindowSeconds, clock: clock, hash: config.Hasher, config: config, shardMask: uint64(config.Shards - 1), close: make(chan struct{}), } ...... for i := 0; i \u0026lt; config.Shards; i++ { cache.shards[i] = initNewShard(config, onRemove, clock) } ...... return cache, nil } Each cache object calculates its hash based on its key: hash(key) % N. Ideally, concurrent requests are evenly distributed across shards, minimizing lock contention and reducing latency\nOptimal Number of Shards (N) Is a larger N always better? No. Excessive shards incur unnecessary overhead. It\u0026rsquo;s crucial to balance performance and cost by choosing a reasonable N, typically powers of 2 (e.g., 16, 32, 64). This allows modulo operations to use fast bitwise calculations:\n1 2 3 4 // https://github.com/allegro/bigcache/blob/a2f05d7cbfdc7000a8b7e1c40f27d3f239c204df/bigcache.go#L253 func (c *BigCache) getShard(hashedKey uint64) (shard *cacheShard) { return c.shards[hashedKey \u0026amp; c.shardMask] } Since for a power N of 2, for any x, the following equation holds.\nx mod N = (x \u0026amp; (N - 1)).\nSo, you only need to use a bitwise AND (\u0026amp;) once to find its remainder.\nChoosing the Right Hashing Algorithm Computer scientists have invented many Hash algorithms, and gopher has implemented many. An ideal hashing algorithm should meet these criteria:\nHigh randomness in hash values. Fast computation. Minimal memory allocations to reduce garbage collection (GC) pressure. BigCache\u0026rsquo;s default implementation uses the fnv64a algorithm, leveraging bitwise operations on the stack to avoid heap allocations.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type fnv64a struct{} const ( offset64 = 14695981039346656037 prime64 = 1099511628211 ) func (f fnv64a) Sum64(key string) uint64 { var hash uint64 = offset64 for i := 0; i \u0026lt; len(key); i++ { hash ^= uint64(key[i]) hash *= prime64 } return hash } Memory Optimization In Go, the garbage collector checks every element in the map during the mark and scan phases. If the cache contains millions of cached objects, the garbage collector\u0026rsquo;s pointless checking of these objects leads to unnecessary time overhead.\nThat\u0026rsquo;s because if the elements in the hashtable contain pointers, the GC scans every element that contains pointers at mark; if they don\u0026rsquo;t, they are skipped at the mark stage.\nWe can test this with a simple example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 var pointMap map[int]*int var noPointMap map[int]int func BenchmarkPointMap(b *testing.B) { pointMap = make(map[int]*int) for i := 0; i \u0026lt; 10e6; i++ { pointMap[i] = \u0026amp;i } b.ResetTimer() for i := 0; i \u0026lt; b.N; i++ { delete(pointMap, i) pointMap[i] = \u0026amp;i } } func BenchmarkNoPointMap(b *testing.B) { noPointMap = make(map[int]int) for i := 0; i \u0026lt; 10e6; i++ { noPointMap[i] = i } b.ResetTimer() for i := 0; i \u0026lt; b.N; i++ { delete(noPointMap, i) noPointMap[i] = i } } Result\n1 2 3 4 5 6 7 ➜ gc git:(main) ✗ GOMAXPROCS=1 go test --bench=. goos: darwin goarch: arm64 pkg: blog-example/go/gc cpu: Apple M1 Pro BenchmarkPointMap 5273188 209.4 ns/op BenchmarkNoPointMap 7037848 178.5 ns/op Then, run two tests separately to analyze the GC\n1 2 go test -bench=BenchmarkPointMap -trace trace_point.out go test -bench=BenchmarkNoPointMap -trace trace_no_point.out The Wall Duration of NoPointMap is only 2% of PointMap. Although the concurrency of PointMap is small and there is no competition for a single goroutine, the garbage collection takes hundreds of milliseconds to mark and traverse in the mark/scan phase due to the large number of elements.\nHow does bigcache solve this problem? Disable your users from storing data with pointers on bigcache.\nJust kidding, if you did, your users would abandon you. There are several ways to solve this problem.\nRefer to offheap[4] to use customized Malloc() and Free() to manage memory manually while bypassing the runtime garbage collection, but this can be more likely to lead to memory leaks. The second way is to use freecache[5]. freecache implements map with zero GC overhead by reducing the number of pointers. it stores the keys and values in a ringbuffer and uses the index to look up the object.\nThe way bigcache implements this is to use the hash value as the key of map[int]int. Serialize the cached object into a pre-allocated large byte array, and then use its offset in the array as the value of map[int]int. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 //https://github.com/allegro/bigcache/blob/a2f05d7cbfdc7000a8b7e1c40f27d3f239c204df/shard.go#L18 type cacheShard struct { hashmap map[uint64]uint32 entries queue.BytesQueue lock sync.RWMutex entryBuffer []byte onRemove onRemoveCallback isVerbose bool statsEnabled bool logger Logger clock clock lifeWindow uint64 hashmapStats map[uint64]uint32 stats Stats } func (s *cacheShard) set(key string, hashedKey uint64, entry []byte) error { currentTimestamp := uint64(s.clock.Epoch()) s.lock.Lock() if previousIndex := s.hashmap[hashedKey]; previousIndex != 0 { // Check for old entries with the same hash key if previousEntry, err := s.entries.Get(int(previousIndex)); err == nil { resetHashFromEntry(previousEntry) // reset hash of old entries delete(s.hashmap, hashedKey) // Remove old entries from the hash table } } if !s.cleanEnabled { // If cleanup is not enabled if oldestEntry, err := s.entries.Peek(); err == nil { s.onEvict(oldestEntry, currentTimestamp, s.removeOldestEntry) // Process the oldest entries } } w := wrapEntry(currentTimestamp, hashedKey, key, entry, \u0026amp;s.entryBuffer) for { if index, err := s.entries.Push(w); err == nil { // Try to push new entries into the queue s.hashmap[hashedKey] = uint64(index)// Update the hash table s.lock.Unlock() return nil } if s.removeOldestEntry(NoSpace) != nil { // If there is not enough space, delete the oldest entry s.lock.Unlock() return errors.New(\u0026#34;entry is bigger than max shard size\u0026#34;) } } } func (s *cacheShard) getValidWrapEntry(key string, hashedKey uint64) ([]byte, error) { wrappedEntry, err := s.getWrappedEntry(hashedKey) if err != nil { return nil, err } if !compareKeyFromEntry(wrappedEntry, key) { s.collision() if s.isVerbose { s.logger.Printf(\u0026#34;Collision detected. Both %q and %q have the same hash %x\u0026#34;, key, readKeyFromEntry(wrappedEntry), hashedKey) } return nil, ErrEntryNotFound } s.hitWithoutLock(hashedKey) return wrappedEntry, nil } queue.BytesQueue is an array of bytes, on demand. When a []byte is added, it copies the data to the end.\nWhen bigcache deletes a cached element, it just removes the index it was added from from map[uint64]uint32 and sets its data in the queue.BytesQueue queue to 0. The deletion operation creates a lot of \u0026quot; Empty holesand thesewormholeswill not be organized or removed. Because it is implemented using a byte array underneath, the removal of the \u0026quot;holes\u0026quot; is a time-consuming operation that causes locks to be held for too long.bigcache` can only wait for the cleanup mechanism to remove these \u0026ldquo;holes\u0026rdquo;.\nSome other details:\nCache objects in bigcache do not have the ability to refresh their expiration time. all caches will eventually expire. The life cycle of all caches is configured by config.LifeWindow and cannot be set for individual keys. ","date":"2024-12-05T22:15:37+08:00","image":"https://images.hxzhouh.com/blog-images/2024/12/d1288ea91074f2a22a8ac4fec04d8b90.png","permalink":"https://huizhou92.com/p/bigcache-high-performance-memory-caching-in-go/","title":"BigCache: High-Performance Memory Caching in Go"},{"content":"Introduction There\u0026rsquo;s a widespread misconception:\nSince TCP port numbers are 16-bit unsigned integers with a maximum value of 65535, a single server can support at most 65536 TCP socket connections.\nEven experienced network programmers may believe this conclusion to be proper. Let\u0026rsquo;s debunk this myth using both theoretical and practical perspectives.\nTheoretical Analysis In *nix systems, each TCP connection is uniquely identified by a 4-tuple structure: {local_ip, local_port, remote_ip, remote_port}.\nFor IPv4, the system can theoretically manage up to 2^(32 + 16 + 32 + 16) connections—equivalent to 2^96.\nIPv4 can be viewed as a 32-bit unsigned number.\nBreaking Down the 4-Tuple Limits A single server typically uses only one local_ip, meaning the server could theoretically manage 2^(16 + 32 + 16) connections. A single service (e.g., an Nginx process) usually listens on one local_port, which reduces the capacity further to 2^(32 + 16). If one remote machine (client) connects to a server, fixing local_ip, local_port, and remote_ip leaves only the remote_port variable. Since the remote_port range is 16-bit, there are only 2^16 = 65536 possible connections from a single client to a specific server endpoint. This limitation is the root of the classic misunderstanding.\nFor protocols beyond TCP (like UDP), the tuple expands to include the protocol type, making it a 5-tuple, adding even more possibilities.\nThe actual limits for a server depend on factors beyond the tuple structure.\nPractical Limitations File Descriptors In Linux, everything is treated as a file, and sockets are no exception. The number of file descriptors determines the maximum number of simultaneous TCP connections.\nMaximum file descriptors supported by the system:\n1 2 [root@test1 ~]# cat /proc/sys/fs/file-max 1616352 Maximum file descriptors per process:\n1 2 [root@test1 ~]# ulimit -n 1024 Both values can be modified. For instance, during stress testing, these limits are often increased manually.\nip_local_port_range When a client connects to the same TCP endpoint (ip:port), each connection requires a unique local TCP endpoint. For clients with a single IP, the available local ports determine the connection limit.\nOn *nix systems, the default local port range is typically from 32768 to 61000. Check this range using:\n1 2 [root@test1 ~]# cat /proc/sys/net/ipv4/ip_local_port_range 32768 60999 This means a single client can establish around 30,000 connections to the same server endpoint during stress tests.\nOperating systems automatically reuse local ports if multiple distinct remote endpoints (ip:port) are involved.\nMemory \u0026amp; CPU Each TCP socket in the ESTABLISHED state consumes approximately 3.3 KB of memory. While CPU usage for idle connections is minimal, the server\u0026rsquo;s memory capacity significantly impacts the total number of connections.\nIn practice, hardware resources like memory and CPU limit the maximum number of TCP connections a server can handle, long before reaching the theoretical 4-tuple limit.\nSummary The maximum number of TCP connections a server can support is an astronomical 2^96. However, practical constraints like memory, CPU, file descriptors, and port availability impose far lower limits.\nThere\u0026rsquo;s no universal number; the limit depends on hardware, configuration, and workload.\nThe background Photo by Alina Grubnyak on Unsplash ","date":"2024-12-02T15:44:12+08:00","image":"https://images.hxzhouh.com/blog-images/2024/12/5a04e98368531b4e0208f40a907399f0.png","permalink":"https://huizhou92.com/p/debunking-the-misconception-maximum-number-of-tcp-connections-on-a-server/","title":"Debunking the Misconception: Maximum Number of TCP Connections on a Server"},{"content":"@BenjDicken add a test to execute a two-tiered loop, 10,000 * 100,000 = 1,000,000,000,000 loops, to see which programming language is faster. A motion graphic of the test was created to visualize this.\nActually, the best part of this kind of program is the ISSUE. Enthusiastic developers contribute versions in various languages, such as Zig, Julia, Perl, Elixir, Fortan, C#, Lua\nAlso, there are discussions on how to optimize the code.\nFor Example :\n@dolanor mentions a PR # optimize go loops with goroutine, arguing that Golang\u0026rsquo;s strength is in concurrent programming; it\u0026rsquo;s not as efficient as C or Rust in a single thread, and should be optimized with goroutine.\n@Brandon-T In # Benchmark Issues, we discuss the problems of existing benchmarks and the direction of improvement. The core idea is that tests should not include extraneous time such as program startup, printing, etc., but should focus on the code execution.\nUnconsciously, I almost finished reading the whole issue.\nThis project reminds me of 1BRC from earlier in the year. It was a great diversion from the boring life of coding. I also learned some performance optimization techniques and participated in discussions with people from all over the world.\nI wish there were more events like this.\n[1] Photo by Tim van der Kuip on Unsplash\n","date":"2024-11-28T11:18:40+08:00","image":"https://images.hxzhouh.com/blog-images/2024/11/6d3e4010fa1e42b2686048757189eab8.png","permalink":"https://huizhou92.com/p/which-programming-language-is-the-fastest/","title":"Which Programming Language Is The Fastest?"},{"content":"Recently, I decided to explore infrastructure concepts more deeply and set up a personal learning environment. To build the environment, I used my MacBook Pro 2021 with Docker without access to additional hardware. The initial setup went smoothly.\nInstalling GitLab The M1 chip in MacBooks uses the ARM architecture, so I used the yrzr/gitlab-ce-arm64v8:latest Docker image.\nCreate Required Directories First, create three working directories for GitLab: etc, log, and opt. Otherwise, errors will occur. Modify the volumes section in the Docker Compose configuration to point to these directories. Two ports, 9922 and 9980, are exposed. Since this setup is for local use, 443 is not enabled.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 version: \u0026#34;3.8\u0026#34; services: gitlab-ce: image: yrzr/gitlab-ce-arm64v8:latest container_name: gitlab-ce privileged: true restart: always ports: - \u0026#34;9922:22\u0026#34; - \u0026#34;9980:9980\u0026#34; volumes: - /Users/hxzhouh/tools/gitlab/etc:/etc/gitlab:z - /Users/hxzhouh/tools/gitlab/log:/var/log/gitlab:z - /Users/hxzhouh/tools/gitlab/opt:/var/opt/gitlab:z deploy: resources: limits: memory: 4096M tty: true stdin_open: true Note: Exposing port 9980 avoids issues like this one.\nUpdate GitLab Configuration Once the container is running, modify the GitLab configuration file:\n1 2 docker exec -it gitlab-ce /bin/bash vi /etc/gitlab/gitlab.rb Add the following lines at the end:\n1 2 3 external_url \u0026#39;http://127.0.0.1:9980\u0026#39; gitlab_rails[\u0026#39;gitlab_ssh_host\u0026#39;] = \u0026#39;127.0.0.1\u0026#39; gitlab_rails[\u0026#39;gitlab_shell_ssh_port\u0026#39;] = 9922 Change the Default Password Run the following commands to update the root password:\n1 2 3 4 gitlab-rails console -e production user = User.where(id: 1).first user.password = \u0026#39;AIl+mVN(:bk\\#5%c\u0026#39; user.save! Finally, reconfigure and restart GitLab:\n1 2 gitlab-ctl reconfigure gitlab-ctl restart Access GitLab Visit http://127.0.0.1:9980 in your browser. The default root username and the password you just set should work.\nAdding SSH To enable SSH-based access like GitHub, create an SSH key pair and add the public key to GitLab, and update your local SSH configuration in ~/.ssh/config:\n1 2 3 4 5 6 Host 127.0.0.1 HostName 127.0.0.1 Port 9922 IdentityFile ~/.ssh/ssh PreferredAuthentications publickey User root Test the connection:\n1 ssh -T git@127.0.0.1 You should see:\n1 Welcome to GitLab, @root! Create a new project and clone it\n1 2 3 4 git clone ssh://git@127.0.0.1:9922/root/hello-world.git some modify ..... git push origin --force 1 2 3 ➜ hello-world git:(main) git push origin --force To ssh://127.0.0.1:9922/root/hello-world.git b4dd724..e45f213 main -\u0026gt; main Configuring GitLab Runners Go to Settings -\u0026gt; CI/CD -\u0026gt; Runners, and follow the instructions under \u0026ldquo;Show runner installation and registration instructions.\u0026rdquo;\nInstall and Register the Runner 1 2 3 4 5 sudo curl --output /usr/local/bin/gitlab-runner https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-darwin-arm64 sudo chmod +x /usr/local/bin/gitlab-runner gitlab-runner install gitlab-runner start gitlab-runner register --url http://127.0.0.1:9980 --registration-token GR13489416KQ-jitR3JhfMgr8f-9G Key Points:\nTags: Use tags to specify runner tasks in .gitlab-ci.yml. Executor: For this example, use the shell executor. For more information about GitLab Runner, please refer to the article on the official website, which will not be expanded here.\nTesting CI/CD Pipeline Create a .gitlab-ci.yml file in your project:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 stages: - build build_job: stage: build tags: - golang-local-shell script: - go build -o myapp only: - main - tags artifacts: paths: - myapp Push the code to GitLab, and the build pipeline should execute automatically.\nArtifacts can be found at:\n1 http://127.0.0.1:9980/root/hello-world/-/artifacts Next Steps This setup concludes the local GitLab environment. In the next article, I’ll demonstrate setting up a local Kubernetes cluster and automating Docker image deployment from GitLab to Kubernetes for a complete CI/CD pipeline.\n","date":"2024-11-16T14:57:45+08:00","image":"https://images.hxzhouh.com/blog-images/2024/11/0d1e85b332916907aaf183fc74feb3d7.png","permalink":"https://huizhou92.com/p/01jcsw6tpwcvwz75ptpkt7mem7/","title":"How To Set Up a GitLab Environment Locally on MacBook Pro"},{"content":"Introduction The previous article discussed two lock-free programming strategies: eliminating shared data and avoiding concurrent access to the same data through careful design. While achieving lock-free programming is ambitious, it could be more practical. Generally, when discussing lock-free techniques, the aim is to avoid locks, and using Compare-And-Swap (CAS) is a good alternative. If CAS is insufficient, we can minimize the granularity of Mutex or replace it with RWMutex.\nAdvantages of Lock-Free Programming Reduces thread blocking and waiting time It avoids thread priority inversion. Improves concurrency performance. Eliminates issues like race conditions, deadlocks, and starvation. Simplifies and clarifies code. This article will explore and implement several lock-free programming techniques in Go.\nChannel The Go language advocates for the principle of sharing memory through communication, as stated in its official blog:\nDo not communicate by sharing memory; instead, share memory by communicating.\nChannels enable lock-free programming because:\nChannel transfer ownership of data. The channel uses internal locks for synchronization. For example:\n1 2 3 4 5 6 7 8 9 type Resource string func Poller(in, out chan *Resource) { for r := range in { // Process the URL // Send the processed Resource to the output channel out \u0026lt;- r } } Data flows between in and out channels without multiple goroutines simultaneously operating on the same data, achieving a lock-free design.\nCAS (Compare-And-Swap) Modern CPUs support atomic CAS operations, allowing atomic data exchange in multithreaded environments. CAS helps avoid data inconsistencies caused by unpredictable execution orders and interruptions.\nmore about cas [[解密go 使用atomic 包 解决并发问题-en]]\nLock-Free Stack Using CAS A stack can typically be implemented with slice and RWMutex for concurrency safety. Alternatively, CAS can be used to implement a lock-free stack. Below is an example:\nLock-Free Stack Implementation\nBenchmarking A simple benchmark test shows the lock-free stack outperforms the slice + RWMutex implementation by approximately 20%. The performance gains are even more significant in high-concurrency scenarios.\n1 2 3 4 5 6 7 8 9 10 11 12 13 func BenchmarkConcurrentPushLockFree(b *testing.B) { stack := NewStackByLockFree() for i := 0; i \u0026lt; b.N; i++ { stack.Push(i) } } func BenchmarkConcurrentPushSlice(b *testing.B) { stack := NewStackBySlice() for i := 0; i \u0026lt; b.N; i++ { stack.Push(i) } } 1 2 3 4 ➜ lock-free git:(main) ✗ go test --bench=. BenchmarkConcurrentPushLockFree-10 22657522 49.66 ns/op BenchmarkConcurrentPushSlice-10 29135671 59.04 ns/op Struct Copy: Trading Space for Time Struct Copy is a technique that avoids data contention by duplicating shared resources. Each goroutine operates on its own copy, eliminating the need for locks.\nExample Struct Copy Implementation\nEach goroutine owns a unique instance by wrapping shared resources with a BufferWrapper, avoiding data races, and using sync.Pool further optimizes memory allocation.\nBenchmarking 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 func BenchmarkSingleBuffer_print(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { run_buff_bench(singleBuff) } } func BenchmarkBufferWrapper_print(b *testing.B) { bufferWrapper := NewBufferWrapper() for i := 0; i \u0026lt; b.N; i++ { run_buff_bench(bufferWrapper) } } func run_buff_bench(buff printId) { wg := sync.WaitGroup{} f := func(id int) { defer wg.Done() for j := 0; j \u0026lt; 10000; j++ { buff.print(id) } } for j := 0; j \u0026lt; 100; j++ { wg.Add(1) go f(j) } wg.Wait() } 1 2 BenchmarkSingleBuffer_print-10 4 261640427 ns/op BenchmarkBufferWrapper_print-10 67 18320110 ns/op A similar technique is used in fastjson\u0026rsquo;s Parser.\nConclusion This article explored three practical techniques for lock-free programming in Go, with performance tests validating their effectiveness. While fully lock-free programming is challenging, proper design and technique selection can significantly reduce lock usage and enhance concurrency performance.\n","date":"2024-11-15T18:17:55+08:00","permalink":"https://huizhou92.com/p/go-high-performance-programming-ep11-lock-free-practice/","title":"Go High-Performance Programming EP11: lock-free practice"},{"content":"Inlock-free programming, the absence of locks is more superficial. Programmers aim to organize data and processing to eliminate data races. Traditional concurrent programming often uses critical sections, exclusive locks, or read-write locks to protect data from incorrect reads or writes during processing. In contrast, lock-free programming resolves these issues by eliminating shared data or concurrent operations on the same data.\nTwo of the most common techniques in lock-free programming include:\nStructure Copy Bumper Loop Other methods are derivatives of the same principles.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nThe “other methods” mentioned here refer to algorithm designs that do not rely on locker or CAS. This article discusses designing data structures and algorithms that avoid locker and CAS.\nSome lock-free solutions, such as RingBuffer libraries, forcefully eliminate race conditions on shared data. However, they depend highly on the specific data structure and CPU design. These are constrained approaches with limited abstraction and generality. For instance, a well-designed RingBuffer might need to be more efficient on single-core CPUs or single vCPUs. Therefore, this article focuses on more general lock-free techniques that work across different programming languages and targets, regardless of their support for threads or coroutines.\nStructure-Copy Structure-Copy is a classic lock-free programming technique. The key idea is to manage mutable data through a structure, often called an Entry, and then process data within this structure. Since the data processing only operates on its associated structure, the data remains non-shared, eliminating the potential for data races.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 type Worker struct { closed int32 // avoid duplicated release of resources in Close() entries []*Entry } type Entry struct { Count int Data []any } func (w *Worker) New(data ...any) *Entry { e := \u0026amp;Entry{Data: data} w.entries = append(w.entries, e) return e } func (e *Entry) Run() { // process e.Data } In the above example, data []any is provided to the Worker, who creates an Entry based on it. The Entry.Run method then processes the initialized data, ensuring that the data package (Count, Data) is exclusive and independent and avoiding shared data issues.\nSometimes, breaking up the data package is tricky. In such cases, you can use a derived technique to decompose it, either by redesigning the data flow into steps (e.g., step1, step2, etc.) or by splitting the data into smaller sub-projects, which are then combined later (Map-Reduce?).\nRegardless of the decomposition strategy, the principle remains the same: the data operated on by each processing step must be an independent copy, thus eliminating the need for shared locking and unlocking.\nChallenges Handling too many small memory chunks (like Entry structures) can introduce overhead. This can lead to increased memory consumption or excessive allocation and deallocation costs, especially when dealing with large lists or arrays. Languages with garbage collection may reduce this pressure, and tools like sync.Pool can help alleviate frequent small memory allocations.\nExample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 //https://github.com/golang/go/blob/f2d118fd5f7e872804a5825ce29797f81a28b0fa/src/log/slog/logger.go#L111 type Logger struct { handler Handler // for structured logging } type Handler interface { Handle(context.Context, Record) error } func (l *Logger) Debug(msg string, args ...any) { l.log(context.Background(), LevelDebug, msg, args...) } func (l *Logger) log(ctx context.Context, level Level, msg string, args ...any) { ... r := NewRecord(time.Now(), level, msg, pc) ... _ = l.Handler().Handle(ctx, r) } In slog, each log entry is wrapped into a NewRecord, avoiding data races. The Entry pattern is widely used and is not necessarily limited to lock-free programming, but it provides the benefit of lock-free operation wherever applied.\nBumper-Loop Bumper-Loop is a design pattern.\nA classic example of the Bumper Loop is Redis. While Redis uses a multithreaded network model, it employs a single-threaded model for transaction processing.\nThe core concept of the Bumper Loop is to serialize parallel events and then distribute them to specific handlers. Since only one handler runs simultaneously, shared data remains exclusive, eliminating race conditions. The Bumper-Loop becomes an efficient consumption model if handlers complete their tasks quickly and without blocking.\n1 2 3 4 5 6 7 8 func (w *Worker) runLoop() { for { select { case ev := \u0026lt;-fileWatcherCh: onFileChanged(ev) } } } The Bumper-Loop is ideal for low-frequency event distribution, locker remove and improved performance. It is often used in TCP/IP servers to handle incoming data. A new connection request is dealt with by creating an independent connectionProcessor using the Entry pattern, which then accepts input data through a Bumper Loop, dispatching it to specific handlers based on the recognized input patterns.\nChallenges The downside of a Bumper-Loop is that any block in the loop can break the entire cycle, leading to delays in handling subsequent events. High-frequency events can cause bottlenecks or even data loss. Despite these challenges, the Bumper-Loop is still widely used in TCP/IP server-client programming due to its simplicity and efficiency.\nCombining Structure-Copy and Bumper-Loop The Bumper Loop needs to be better suited for high-frequency events. It performs best when event frequency is around 80ms or higher, meaning each event should be processed in less than 80ms. In less demanding environments (e.g., non-financial high-frequency trading), a costly event handler might spawn a goroutine to handle the event asynchronously, allowing the main bumper loop to continue processing subsequent events. The Entry pattern can then copy minimal state, helping decouple race conditions.\n","date":"2024-10-19T12:45:09+08:00","permalink":"https://huizhou92.com/p/go-high-performance-programming-ep10-two-useful-golang-lock-free-programming-tips/","title":"Go High-Performance Programming EP10: Two Useful Golang Lock-Free Programming Tips"},{"content":"Introduction In 2022, ByteDance proposed an issue recommending that Golang adopt SwissTable for its map implementation. In 2023, Dolt published a blog post titled SwissMap: A Smaller, Faster Golang Hash Table, detailing their design of a swisstable, which garnered widespread attention. The Go core team is reevaluating the swisstable design and has added some related code in the runtime. With some free time during the holidays, let\u0026rsquo;s delve deeper into the principles, compare it to the runtime map, and understand why it might become the standard for map implementation.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nThis article will not fully explain the principles of hashtable or swisstable. A basic understanding of hashtable is required.\nA hashtable provides a mapping from a key to a corresponding value by converting the key to some \u0026ldquo;position\u0026rdquo; using a hash function. From this position, we can directly retrieve the desired value.\nTraditional Hashtable A hashtable provides a mapping from a key to a corresponding value by converting the key to some \u0026ldquo;position\u0026rdquo; using a hash function. However, even if the hash function is perfect, conflicts are still inevitable when mapping an infinite number of keys into a finite memory space: two different keys will be mapped to the same position. To solve this, traditional hashtables have several conflict resolution strategies, most commonly chaining and linear probing.\nChaining Chaining is the most common method where if multiple keys map to the same position, these keys and values are stored in a linked list. During lookups, the hash function is used to find the position, and then the linked list at that position is traversed to find the matching key. Its structure is similar to this:\nFigure 1: Chaining implementation of hashtable\nChaining is straightforward to implement, with fewer boundary conditions to consider. Data insertion and deletion are quick, using head insertion to add new entries and adjusting the next pointer for deletion. Chaining can also prevent performance degradation by converting overly long chains into search trees. However, chaining is not cache-friendly, and performance can be affected if there are many conflicts. Different slots can be widely spread in memory, leading to poor spatial locality of the data structure overall.\nLinear Probing Linear probing is another standard hash table conflict resolution method. Unlike chaining, when a hash conflict occurs, it searches sequentially from the conflict position until it finds an empty slot or loops back to the conflict position. At this point, it resizes and rehashes the entries.\nFigure 2: Linear probing animation\nLookups work similarly: calculate the hash position for the key and then compare each key starting from that position, skipping any deleted entries until an empty slot is reached, indicating the key is not in the table. Deletion uses a tombstone marker.\nFigure 3: Linear probing lookup animation\nLinear probing has comparable time complexity to chaining. Its advantages include being cache-friendly and implementable with tight data structures like arrays. However, its drawbacks are:\nComplex implementation with three states for a slot: occupied, empty, and deleted. Chain reaction of conflicts, leading to more frequent resizing than chaining and potentially more significant memory usage. Higher likelihood of lookup process degradation to O(n) without the ability to convert heavily conflicted areas into search trees. Many libraries use chaining due to the difficulties with element deletion and conflict chain reactions in linear probing. Despite its drawbacks, linear probing\u0026rsquo;s cache friendliness and memory efficiency provide significant performance advantages on modern computers, leading to its use in languages like golang and Python.\nGo Map Data Storage Let\u0026rsquo;s recap how Go Map stores data:\n**Figure 4: Go map **\nQuick summary:\nGo map uses a hash function to map keys to multiple buckets, each with a fixed number of key-value slots for storage. Each bucket stores up to 8 key-value pairs. The conflicting keys and values are stored within the same bucket when conflicts occur. The map uses a hash function to compute the key\u0026rsquo;s hash value and locate the corresponding bucket. If a bucket is complete (all 8 slots used), an overflow bucket is generated to continue storing new key-value pairs. For lookups, the key\u0026rsquo;s hash value is calculated, the corresponding bucket is determined, and each slot within the bucket is checked. If there is an overflow bucket, its keys are also checked sequentially. SwissTable: An Efficient Hashtable Implementation SwissTable is a hashtable implementation based on an improved linear probing method. Its core idea is to optimize performance and memory usage by enhancing the hashtable structure and metadata storsage. SwissTable uses a new metadata control mechanism to significantly reduce unnecessary key comparisons and leverages SIMD instructions to boost throughput.\nReviewing the two standard hashtable implementations shows they either waste memory, need to be cache-friendly or suffer performance drops in lookup, insertion, and deletion operations after conflicts. These issues persist even with a \u0026ldquo;perfect hash function.\u0026rdquo; Using a suboptimal hash function dramatically increases the probability of key conflicts and worsens performance, possibly even falling short of linear searches in an array.\nThe industry sought a hash table algorithm that was friendly to cache and resistant to lookup performance degradation. Many focused on developing better hash functions close to \u0026ldquo;perfect hash function\u0026rdquo; quality while optimizing calculation performance; others worked on improving the hash table\u0026rsquo;s structure to balance cache friendliness, performance, and memory usage. swisstable belongs to the latter.\nSwissTable\u0026rsquo;s time complexity is similar to linear probing, while its space complexity is between chaining and linear probing. The implementation I\u0026rsquo;ve referenced is primarily based on dolthub/swiss.\nBasic Structure of SwissTable Although the name has changed, swisstable is still a hashtable utilizing an improved linear probing method for hash collisions. Its underlying structure resembles an array. Now, let\u0026rsquo;s delve into the structure of swisstable:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 type Map[K comparable, V any] struct { ctrl []metadata groups []group[K, V] hash maphash.Hasher[K] resident uint32 dead uint32 limit uint32 } type metadata [groupSize]int8 type group[K comparable, V any] struct { keys [groupSize]K values [groupSize]V } In swisstable, ctrl is an array of metadata, corresponding to the group[K, V] array. Each group has 8 slots.\nThe hash is divided into 57 bits for H1 to determine the starting groups, and the remaining 7 bits called H2, stored in metadata as the hash signature of the current key for subsequent search and filtering.\nThe key advantage of swisstable over traditional hash tables lies in the metadata called ctrl. Control information includes:\nWhether a slot is empty: 0b10000000 Whether a slot has been deleted: 0b11111110 The key\u0026rsquo;s hash signature (H2) in a slot: 0bh2 The unique values of these states allow the use of SIMD instructions, maximizing performance.\nAdding Data The process of adding data in swisstable involves several steps:\nCalculate the hash value and split it into h1 and h2. Using h1, determine the starting groups. Use metaMatchH2 to check the current group\u0026rsquo;s metadata for a matching h2. If found, further check for the matching key and update the value if they match. If no matching key is found, use metaMatchEmpty to check for empty slots in the current group. Insert the new key-value pair if an empty slot is found and update the metadata and resident count. If no empty slots are available in the current group, perform linear probing to check the next groups. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 func (m *Map[K, V]) Put(key K, value V) { if m.resident \u0026gt;= m.limit { m.rehash(m.nextSize()) } hi, lo := splitHash(m.hash.Hash(key)) g := probeStart(hi, len(m.groups)) for { // inlined find loop matches := metaMatchH2(\u0026amp;m.ctrl[g], lo) for matches != 0 { s := nextMatch(\u0026amp;matches) if key == m.groups[g].keys[s] { // update m.groups[g].keys[s] = key m.groups[g].values[s] = value return } } matches = metaMatchEmpty(\u0026amp;m.ctrl[g]) if matches != 0 { // insert s := nextMatch(\u0026amp;matches) m.groups[g].keys[s] = key m.groups[g].values[s] = value m.ctrl[g][s] = int8(lo) m.resident++ return } g += 1 // linear probing if g \u0026gt;= uint32(len(m.groups)) { g = 0 } } } func metaMatchH2(m *metadata, h h2) bitset { return hasZeroByte(castUint64(m) ^ (loBits * uint64(h))) } func nextMatch(b *bitset) uint32 { s := uint32(bits.TrailingZeros64(uint64(*b))) *b \u0026amp;= ^(1 \u0026lt;\u0026lt; s) return s \u0026gt;\u0026gt; 3 } Although the steps are few, they involve complex bit operations. Normally, h2 needs to be compared with all keys sequentially until the target is found. swisstable cleverly achieves this by:\nMultiplying h2 by 0x0101010101010101 to get a uint64, allowing simultaneous comparison with 8 ctrl values. Performing xor with meta. If h2 exists in metadata, the corresponding bit will be zero. The metaMatchH2 function helps us understand this process. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func TestMetaMatchH2(t *testing.T) { metaData := make([]metadata, 2) metaData[0] = [8]int8{0x7f, 0, 0, 0x7f, 0, 0, 0, 0x7f} m := \u0026amp;metaData[0] h := 0x7f metaUint64 := castUint64(m) h2Pattern := loBits * uint64(h) xorResult := metaUint64 ^ h2Pattern fmt.Printf(\u0026#34;metaUint64: %b\\n\u0026#34;, xorResult) r := hasZeroByte(xorResult) fmt.Printf(\u0026#34;r: %b\\n\u0026#34;, r) for r != 0 { fmt.Println(nextMatch(\u0026amp;r)) } } ---- output // metaUint64: 00000000 11111110 11111110 11111110 0000000 01111111 01111111 00000000 // r: 10000000 00000000 00000000 00000000 10000000 00000000 00000000 10000000 // 0 // 3 // 7 Advantages of SwissTable Reviewing SwissTable implementation reveals several key benefits:\nOperations are shifted from slots to ctrl, which are smaller and easily placed in the CPU cache, speeding up operations despite the additional step of locating slots. Records hash signatures, reducing meaningless key comparisons—the main cause of linear probing performance drops. Batch operations on ctrl for slots increase throughput significantly. Metadata and memory layout are optimized for SIMD instructions, maximizing performance. Slot optimizations, such as compressing large data, increase cache hit rates. swisstable solves spatial locality issues and exploits modern CPU features for batch element operations, significantly boosting performance.\nFinally, running a benchmark on a local MacBook M1 (without SIMD support) shows significant performance improvements in large map scenarios.\nFigure 5: Official swisstable benchmark\nConclusion Currently, the official implementation of swisstable in Go is still under discussion, and there are a few community implementations like concurrent-swiss-map and swiss. However, they are not perfect; in small map scenarios, swisstable may even underperform compared to runtime_map. Nonetheless, the potential demonstrated by swisstable in other languages indicates that it is worth anticipation.\nReferences Dolthub: SwissMap SwissTable principles: Abseil SwissTables Original SwissTable proposal at cppcon: cppcon talk Improvements to SwissTable algorithm: YouTube link Bit manipulation primer: Stanford Bit Hacks Hash function comparisons: Hash function tests An additional bit manipulation article: Fast Modulo Reduction ","date":"2024-09-30T11:45:52+08:00","image":"https://images.hxzhouh.com/blog-images/2024/09/f08c74aea8485d51d461d18494df416e.png","permalink":"https://huizhou92.com/p/swisstable-a-high-performance-hash-table-implementation/","title":"SwissTable: A High-Performance Hash Table Implementation"},{"content":"Memory leaks are a common issue regardless of the programming language used. However, it\u0026rsquo;s relatively complex to write memory-leaking code in Go. This article will illustrate several scenarios where memory leaks may occur, allowing us to learn how to avoid them by studying these anti-patterns.\nThis article was first published in the Medium MPP plan. If you\u0026rsquo;re a Medium user, please follow me on Medium. Thank you!\nResource Leaks Not Closing Opened Files When you finish an open file, you should always call its Close method. Failing to do so may result in reaching the maximum number of file descriptors, preventing you from opening new files or connections. This can lead to a \u0026ldquo;too many open files\u0026rdquo; error, as shown below:\nCode Example 1: Not closing files leads to file descriptor exhaustion.\n1 2 3 4 5 6 7 8 9 10 11 12 13 func main() { files := make([]*os.File, 0) for i := 0; ; i++ { file, err := os.OpenFile(\u0026#34;test.log\u0026#34;, os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0644) if err != nil { fmt.Printf(\u0026#34;Error at file %d: %v\\n\u0026#34;, i, err) break } else { _, _ = file.Write([]byte(\u0026#34;Hello, World!\u0026#34;)) files = append(files, file) } } } Error Output:\n1 Error at file 61437: open test.log: too many open files On my Mac, the maximum number of file handles a process can open is 61,440. Go processes typically open three file descriptors (stderr, stdout, stdin), leaving a maximum of 61,437 open files. You can manually adjust this limit.\nNot Closing http.Response.Body Go has a well-known bug where forgetting to close the body of an HTTP request can lead to memory leaks. For example:\nCode Example 2: Not closing the HTTP body causes a memory leak.\n1 2 3 4 5 6 7 8 9 10 11 12 13 func makeRequest() { client := \u0026amp;http.Client{} req, err := http.NewRequest(http.MethodGet, \u0026#34;http://localhost:8081\u0026#34;, nil) res, err := client.Do(req) if err != nil { fmt.Println(err) } _, err = ioutil.ReadAll(res.Body) // defer res.Body.Close() if err != nil { fmt.Println(err) } } For more details on this issue, you can refer to:\nIs resp.Body.Close() necessary if we don\u0026rsquo;t read anything from the body? Must Close GoLang HTTP Response String and Slice Memory Leaks The Go spec does not explicitly state whether substrings share memory with their original string. However, the compiler allows this behavior, which is generally good as it reduces memory and CPU usage. But this can sometimes lead to temporary memory leaks.\nCode Example 3: Memory leak caused by strings.\nhttps://gist.github.com/hxzhouh/e09587195e2d7aa2d5f6676777c6cb16\nTo prevent temporary memory leaks, we can use strings.Clone().\nCode Example 4: Using strings.Clone() to avoid temporary memory leaks.\n1 2 3 4 5 6 func Demo1() { for i := 0; i \u0026lt; 10; i++ { s := createStringWithLengthOnHeap(1 \u0026lt;\u0026lt; 20) // 1MB packageStr1 = append(packageStr1, strings.Clone(s[:50])) } } Goroutine Leaks Goroutine Handler Most memory leaks are due to goroutine leaks. For instance, the example below can quickly exhaust memory, resulting in an OOM (Out of Memory) error.\nCode Example 5: goroutine handler leak.\n1 2 3 4 5 6 for { go func() { time.Sleep(1 * time.Hour) }() } } Misusing Channels Incorrect usage of channels can also easily lead to goroutine leaks. For unbuffered channels, both the producer and consumer must be ready before writing to the channel, or it will block. In the example below, the function exits early, causing a goroutine leak.\nCode Example 6: Unbuffered channel misuse causes a goroutine leak.\n1 2 3 4 5 6 7 8 9 10 11 12 13 func Example() { a := 1 c := make(chan error) go func() { c \u0026lt;- err return }() // Example exits here, causing a goroutine leak. if a \u0026gt; 0 { return } err := \u0026lt;-c } Simply changing it to a buffered channel can solve this issue: c := make(chan error, 1).\nMisusing range with Channels Channels can be iterated using range. However, if the channel is empty,the range will wait for new data, potentially blocking the goroutine.\nCode Example 7: Misusing range causes a goroutine leak.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func main() { wg := \u0026amp;sync.WaitGroup{} c := make(chan any, 1) items := []int{1, 2, 3, 4, 5} for _, i := range items { wg.Add(1) go func() { c \u0026lt;- i }() } go func() { for data := range c { fmt.Println(data) wg.Done() } fmt.Println(\u0026#34;close\u0026#34;) }() wg.Wait() time.Sleep(1 * time.Second) } To fix this, close the channel after calling wg.Wait().\nMisusing runtime.SetFinalizer If two objects are both set with runtime.SetFinalizer and they reference each other, they will not be garbage collected, even if they\u0026rsquo;re no longer in use. For more information, you can refer to my other article.\ntime.Ticker This was an issue before Go 1.23. If ticker.Stop() is not called, it could cause a memory leak. As of Go 1.23, this issue has been fixed. More details here.\nMisusing defer Although using defer to release resources does not directly cause memory leaks, it can lead to temporary memory leaks in two ways:\nExecution time: defer is always executed when the function ends. If your function runs for too long or never ends, the resources in defer may not be released in time. Memory usage: Each defer adds a call point in memory. If used inside a loop, this may cause a temporary memory leak. Code Example 8: Temporary memory leak caused by defer.\n1 2 3 4 5 6 7 8 9 10 11 func ReadFile(files []string) { for _, file := range files { f, err := os.Open(file) if err != nil { fmt.Println(err) return } // do something defer f.Close() } } This code causes a temporary memory leak and can lead to a \u0026ldquo;too many open files\u0026rdquo; error. Avoid using defer excessively unless necessary.\nConclusion This article covered several behaviors in Go that can lead to memory leaks, with goroutine leaks being the most common. The improper use of channels, especially with select and range, can make detecting leaks more difficult. When faced with memory leaks, pprof can help quickly locate the problem, ensuring you write more robust code.\nReferences https://go101.org/article/memory-leaking.html ","date":"2024-09-25T18:24:49+08:00","permalink":"https://huizhou92.com/p/common-causes-of-memory-leaks-in-go-how-to-avoid-them/","title":"Common Causes of Memory Leaks in Go: How to Avoid Them"},{"content":"Rate limiting is a critical mechanism in software architecture that controls resource usage and safeguards system security. By limiting the number of requests handled within a given time frame, rate limiting prevents system overloads, ensuring stability under high-load conditions. This article aims to analyze and implement several expected rate-limiting algorithms and discuss their pros and cons. Finally, we\u0026rsquo;ll explore rate-limiting designs in real-world business code.\nThis article was first published in the Medium MPP plan. If you\u0026rsquo;re a Medium user, please follow me on Medium. Thank you very much.\nTraffic Counter Mode Assuming performance tests indicate that our system can handle a maximum load of 10 TPS, the most straightforward approach is to set a timer that allows ten requests per second and rejects any excess traffic.\nCode1: Windows Limit Example. You can run the code example here.\nHowever, this approach has a flaw. Suppose we receive 10 TPS requests within 2 seconds, between 0.9s and 1.1s. Even though the request rate does not exceed the threshold of 10 TPS in each period, the system still experiences 20 TPS within 1 second, exceeding the limit. The root cause of this issue lies in the discrete time-based counting, and to fix this, a \u0026ldquo;sliding window\u0026rdquo; rate limiting mode is designed to smooth the time-based calculations.\nSliding Window Mechanism The Sliding Window Algorithm is widely used in various fields of computer science, such as congestion control in TCP protocols. This algorithm divides time into smaller segments, each maintaining an independent counter. When a request arrives, it is assigned to the current time segment, and the corresponding counter is checked. If the counter has not reached its limit, the request is allowed, and the counter is incremented. Otherwise, the request is rejected. Older segments fade out of the window as time progresses while new segments are added.\nCode2: Sliding Window Example. You can try this code here.\nAlthough the Sliding Window Algorithm provides more granular traffic control, it doesn’t fully resolve sudden traffic bursts. It also struggles to reshape traffic curves with finer granularity and does not achieve peak smoothing. Below are two additional rate-limiting algorithms designed for more stringent control.\nLeaky Bucket Algorithm The Leaky Bucket Algorithm simulates a bucket with a fixed capacity and processes requests at a constant rate, regardless of the incoming traffic. This ensures that requests are handled evenly, smoothing out traffic and preventing spikes.\nCode3: Leaky Bucket Example. Run the code here.\nThe Leaky Bucket Algorithm is well-suited for scenarios requiring steady request processing, such as network traffic control and API rate limiting. The Leaky Bucket Algorithm effectively prevents system overload due to sudden traffic surges by controlling the rate at which tokens are added. The challenge lies in determining the bucket size and outflow rate. The system may still experience traffic surges if the bucket is too large. If it’s too tiny, legitimate requests could be unnecessarily rejected.\nToken Bucket Algorithm The Token Bucket Algorithm is similar to real-world queueing systems, where tokens are issued at a fixed rate. For example, if we want to limit the system to a maximum of Y requests over X seconds, tokens are added to the bucket every X/Y seconds. Incoming requests must first acquire a token from the bucket to be processed. If no tokens are available, the request is rejected.\nCode 4: Token Bucket Example. You can try this code here.\nThe Token Bucket Algorithm is ideal for handling burst traffic, such as network communications or API calls. It effectively balances traffic and prevents overload by controlling the token generation rate and the bucket\u0026rsquo;s capacity. Additionally, if system resources are sufficient, it allows for quickly handling more requests. The token generation speed is critical and must be carefully considered.\nRate Limiting in Real-world Applications Distributed Rate Limiting Algorithms The four rate-limiting algorithms discussed above store their state in memory, which works for single-node systems. However, in microservice architectures, these algorithms are most effective at the gateway level for cluster-wide traffic control but not for fine-grained control within individual service nodes. We need to store the state in a shared cluster to enable rate-limiting coordination across multiple nodes. Below is an example of using Redis to implement a distributed token bucket algorithm. This is the method I often use in production.\nCode 5: Distributed Token Bucket Example using Redis\nPrioritization Server resources are valuable, and we want to allocate these resources to serve VIP clients during resource-constrained periods. A \u0026ldquo;quota\u0026rdquo; system based on distributed rate limiting offers a flexible solution. Unlike traditional token bucket algorithms, where access is binary (allow/deny), the quota system will enable users to consume resources as a \u0026ldquo;currency.\u0026rdquo; VIP users may be assigned higher quotas, while regular users face tighter limits. When a user\u0026rsquo;s quota is depleted, they must request more tokens to continue accessing services.\nCode 6: Quota System Example. You can try this code here.\nConclusion This article has covered several expected rate-limiting algorithms, including traffic counters, sliding windows, leaky buckets, and token buckets, and analyzed their application scenarios. Traffic counters are simple but need help with sudden traffic spikes. The sliding window algorithm provides a smoother approach but is still limited. The leaky bucket algorithm ensures steady request handling, while the token bucket algorithm allows flexible handling of burst traffic.\nIn real-world applications, especially in microservices, distributed rate limiting becomes essential. Using middleware like Redis, a distributed token bucket algorithm can effectively manage traffic across multiple nodes. Additionally, quota-based rate limiting offers greater flexibility and improves the experience for VIP customers. Rate limiting protects systems from overload and enhances overall stability and performance.\nLong Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2024-09-25T09:32:50+08:00","image":"https://images.hxzhouh.com/blog-images/2024/09/f146e1aedafc425732135df81c5cd244.png","permalink":"https://huizhou92.com/p/common-rate-limiting-algorithms-analysis-and-implementation/","title":"Common Rate Limiting Algorithms: Analysis and Implementation"},{"content":"Background JSON is a lightweight data interchange format that has been favored by developers for its flexible features since the inception of Go\u0026rsquo;s encoding/json package ten years ago. Developers can customize the JSON representation of struct fields through struct tags, and also allow Go types to customize their own JSON representation. However, with the development of Go language and JSON standards, some functional deficiencies and performance limitations have gradually been exposed.\nMissing functionality: For example, it is not possible to specify custom formatting for the time.Time type, and it is not possible to omit specific Go values during serialization, etc. API deficiencies: For example, there is no simple way to correctly deserialize JSON from an io.Reader. Performance limitations: The performance of the standard json package is not satisfactory, especially when dealing with large amounts of data. Behavioral flaws: For example, the error handling for JSON syntax is not strict enough, and case-insensitive deserialization, etc. Just like math/v2, Go official proposes encoding/json/v2 to solve the above problems. The main goal of this article is to analyze some issues about null values in encoding/json and how they are resolved in encoding/json/v2. This article does not involve other modifications in encoding/json/v2.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nOmitempty In the encoding/json package, there is a description of omitempty as follows:\nThe \u0026ldquo;omitempty\u0026rdquo; option specifies that the field should be omitted from the encoding if the field has an empty value, defined as false, 0, a nil pointer, a nil interface value, and any empty array, slice, map, or string.\nHowever, this predefined \u0026ldquo;empty\u0026rdquo; value judgment logic does not meet the needs of all actual scenarios. Let\u0026rsquo;s look at an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 type Post struct { Id int64 `json:\u0026#34;id,omitempty\u0026#34;` CreateTime time.Time `json:\u0026#34;create_time,omitempty\u0026#34;` TagList []Tag `json:\u0026#34;tag_list,omitempty\u0026#34;` Name string `json:\u0026#34;name,omitempty\u0026#34;` Score float64 `json:\u0026#34;score,omitempty\u0026#34;` Category Category `json:\u0026#34;category,omitempty\u0026#34;` LikePost map[string]Post `json:\u0026#34;like,omitempty\u0026#34;` } type Tag struct { ID string `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` } type Category struct { ID float64 `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` } func main() { b, _ := json.Marshal(new(Post)) fmt.Println(string(b)) } The output result is:\n1 {\u0026#34;create_time\u0026#34;:\u0026#34;0001-01-01T00:00:00Z\u0026#34;,\u0026#34;category\u0026#34;:{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;\u0026#34;}} Although omitempty is added to each field of Post, the result is not as expected.\nomitempty cannot handle empty struct, such as Post.Category The way omitempty handles time.Time is not what we understand as UTC =0, that is, 1970-01-01 00:00:00, but 0001-01-01T00:00:00Z. Omitzero Tag In encoding/json/v2, a new tag omitzero will be added, which adds two functions to solve the above two problems. (This feature is still under development, but you can experience the new features in advance through go-json-experiment/json)\nBetter handling of time.Time. Support for custom IsZero function.\nFor example, the following code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; v2_json \u0026#34;github.com/go-json-experiment/json\u0026#34; \u0026#34;math\u0026#34; \u0026#34;time\u0026#34;) type Post struct { Id int64 `json:\u0026#34;id,omitempty,omitzero\u0026#34;` CreateTime time.Time `json:\u0026#34;create_time,omitempty,omitzero\u0026#34;` TagList []Tag `json:\u0026#34;tag_list,omitempty\u0026#34;` Name string `json:\u0026#34;name,omitempty\u0026#34;` Score ScoreType `json:\u0026#34;score,omitempty,omitzero\u0026#34;` Category Category `json:\u0026#34;category,omitempty,omitzero\u0026#34;` LikePost map[string]Post `json:\u0026#34;like,omitempty\u0026#34;` } type ScoreType float64 func (s ScoreType) IsZero() bool { return s \u0026lt; math.MinInt64 } type Tag struct { ID string `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` } type Category struct { ID float64 `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` } func main() { v1String, _ := json.Marshal(new(Post)) fmt.Println(string(v1String)) v2String, _ := v2_json.Marshal(new(Post)) fmt.Println(string(v2String)) } Compared with encoding/json, encoding/json/v2 solves the above problems.\n1 2 {\u0026#34;create_time\u0026#34;:\u0026#34;0001-01-01T00:00:00Z\u0026#34;,\u0026#34;category\u0026#34;:{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;\u0026#34;}} {\u0026#34;score\u0026#34;:0} Conclusion By introducing the omitzero tag, Go has taken an essential step in solving the pain points of \u0026ldquo;empty\u0026rdquo; value processing in JSON encoding. This solution not only meets developers\u0026rsquo; needs for a more flexible definition of \u0026ldquo;empty\u0026rdquo; values but also maintains compatibility with existing systems. The landing time of omitzero is not determined yet, and it will not be available until the Go 1.24 version at the earliest. In addition, encoding/xml and other packages will also follow the json package and add the omitzero tag.\nencoding/json/v2 also includes updates in other aspects, such as performance. Interested Gophers can learn about these changes in advance, and this blog will continue to pay attention to this proposal.\nGo language is beginning to mature.\n","date":"2024-09-17T19:54:38+08:00","image":"https://images.hxzhouh.com/blog-images/2024/09/5cfcc61c5e45994e3e35ceb82bfa6bc8.png","permalink":"https://huizhou92.com/p/go-proposal-a-new-json-lib-ep1/","title":"Go Proposal: A New Json Lib Ep1"},{"content":"Sometimes in the development process, we need to simulate slow disk conditions to verify if our code can still function on low-performance machines. Typically, we would use cgroup or Docker for this purpose, but it can be cumbersome on a MacBook. However, there\u0026rsquo;s a built-in tool on macOS that can help us achieve this: dmc.\nUsing dmc macOS comes with dmc, which we can explore using dmc -h.\n1 2 3 4 5 6 7 8 9 10 11 ➜ /tmp dmc -h usage: dmc \u0026lt;commands...\u0026gt; # commands: start \u0026lt;mount\u0026gt; (profile_name|profile_index [-boot]) stop \u0026lt;mount\u0026gt; status \u0026lt;mount\u0026gt; [-json] show profile_name|profile_index list select \u0026lt;mount\u0026gt; (profile_name|profile_index) configure \u0026lt;mount\u0026gt; \u0026lt;type\u0026gt; \u0026lt;access_time\u0026gt; \u0026lt;read_throughput\u0026gt; \u0026lt;write_throughput\u0026gt; [\u0026lt;ioqueue_depth\u0026gt; \u0026lt;maxreadcnt\u0026gt; \u0026lt;maxwritecnt\u0026gt; \u0026lt;segreadcnt\u0026gt; \u0026lt;segwritecnt\u0026gt;] help | -h It offers various disk profiles to choose from:\n1 2 3 4 5 6 7 8 9 ➜ /tmp dmc list 0: Faulty 5400 HDD 1: 5400 HDD 2: 7200 HDD 3: Slow SSD 4: SATA II SSD 5: SATA III SSD 6: PCIe 2 SSD 7: PCIe 3 SSD Each profile corresponds to different speed modes, such as:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ➜ /tmp dmc show 0 Profile: Faulty 5400 HDD Type: HDD Access time: 52222 us Read throughput: 50 MB/s Write throughput: 50 MB/s I/O Queue Depth: 16 Max Read Bytes: 16777216 Max Write Bytes: 16777216 Max Read Segments: 128 Max Write Segments: 128 ➜ /tmp dmc show 7 Profile: PCIe 3 SSD Type: SSD Access time: 3 us Read throughput: 3072 MB/s Write throughput: 2560 MB/s I/O Queue Depth: 256 Max Read Bytes: 67108864 Max Write Bytes: 67108864 Max Read Segments: 256 Max Write Segments: 256 Using it is straightforward. Suppose our disk is mounted on /tmp/data, and we want to set it to level 0. We simply execute:\n1 sudo dmc start /tmp/data 0 Then, to verify the status:\n1 2 3 4 5 6 7 8 9 10 11 12 ➜ /tmp dmc status /tmp/data Disk Mount Conditioner: ON Profile: Custom Type: HDD Access time: 52222 us Read throughput: 50 MB/s Write throughput: 50 MB/s I/O Queue Depth: 16 Max Read Bytes: 1048576 Max Write Bytes: 1048576 Max Read Segments: 128 Max Write Segments: 128 Verification We can validate using the fio tool:\n1 ➜ /tmp fio --filename=./data/test1 -direct=1 --rw=write --ioengine=posixaio --bs=1m --iodepth=32 --size=1G --numjobs=1 --runtime=60 --time_base=1 --group_reporting --name=test-seq-read --log_avg_msec=1000 Writing a 1GB file sequentially yields a speed of only 95.4MB/s, with IOPS at 91:\n1 2 Run status group 0 (all jobs): WRITE: bw=91.0MiB/s (95.4MB/s), 91.0MiB/s-91.0MiB/s (95.4MB/s-95.4MB/s), io=5468MiB (5734MB), run=60073-60073msec Now, with dmc turned off and the same command:\n1 2 /tmp sudo dmc stop /tmp/data /tmp fio --filename=./data/test1 -direct=1 --rw=write --ioengine=posixaio --bs=1m --iodepth=32 --size=1G --numjobs=1 --runtime=60 --time_base=1 --group_reporting --name=test-seq-read --log_avg_msec=1000 The test results in 3211MB/s and IOPS of 3061, which represents the true speed of the disk:\n1 WRITE: bw=3062MiB/s (3211MB/s), 3062MiB/s-3062MiB/s (3211MB/s-3211MB/s), io=179GiB (193GB), run=60006-60006msec In conclusion, dmc is quite handy in testing scenarios. I hadn\u0026rsquo;t known about this software before; if you find it useful, give it a try.\nLong Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2024-09-06T11:34:00Z","permalink":"https://huizhou92.com/p/mac-slowing-down-your-disk-speed-by-60x/","title":"Mac: Slowing Down Your Disk Speed by 60x"},{"content":"Recently, I discovered that the Go standard library includes a built-in implementation of varint, found in encoding/binary/varint.go. This implementation is similar to the varint used in protobuf. Using the Golang standard library\u0026rsquo;s varint source code, we will systematically learn and review the concept of varint.\nIf you\u0026rsquo;re familiar with protobuf, you probably already know that all integer types (except fixed types like fixed32 and fixed64) are encoded using varint.\nvarint mainly solves two issues:\nSpace Efficiency: Take uint64 as an example, representing values as large as 18,446,744,073,709,551,615. In most real-world scenarios, however, our integer values are much smaller. If your system needs to process values as low as 1, you\u0026rsquo;d still use 8 bytes to represent this value in transmission, wasting space since most bytes store no useful data. varint encoding uses a variable-length byte sequence to represent integers, reducing the space required for smaller values.\nCompatibility: varint allows us to handle integers of different sizes without altering the encoding/decoding logic. This means fields can be upgraded from smaller types (like uint32) to larger ones (like uint64) without breaking backward compatibility.\nThis article will dive into Golang varint implementation, exploring its design principles and how it addresses the challenges of encoding negative numbers.\nThis article was first published under the Medium MPP plan. Follow me on Medium if you\u0026rsquo;re a Medium user.\nThe Design Principles of varint varint is designed based on simple principles:\n7-bit Grouping: The binary representation of an integer is divided into 7-bit groups. From the least significant bit to the most significant bit, every 7-bit group becomes a unit. Continuation Bit: A flag bit is added before each 7-bit group, forming an 8-bit byte. If more bytes follow, the flag bit is set to 1; otherwise, it’s set to 0.\nFor example, the integer uint64(300) has a binary representation of 100101100. Dividing this into two groups—10 and 0101100—and adding flag bits results in two bytes: 00000010 and 10101100, which is the varint encoding of 300. Compared to uint64, which uses 4 bytes, varint reduces the storage by 75%.\nlist1: uint64 to varint 1 2 3 4 5 6 7 8 9 func main() { v := uint64(300) bytes := make([]byte, 0) bytes = binary.AppendUvarint(bytes, v) fmt.Println(len(bytes)) for i := len(bytes); i \u0026gt; 0; i-- { fmt.Printf(\u0026#34;%08b \u0026#34;, bytes[i-1]) } } varint for Unsigned Integers The Go standard library provides two sets of varint functions: one for unsigned integers (PutUvarint, Uvarint) and another for signed integers (varint, Putvarint).\nLet\u0026rsquo;s first look at the unsigned integer varint implementation:\nlist2: go src PutUvarint\n1 2 3 4 5 6 7 8 9 10 func PutUvarint(buf []byte, x uint64) int { i := 0 for x \u0026gt;= 0x80 { buf[i] = byte(x) | 0x80 x \u0026gt;\u0026gt;= 7 i++ } buf[i] = byte(x) return i + 1 } There is a very important constant in the code: 0x80, which corresponds to the binary code 1000 0000. This constant is very important for the logic that follows:\nx \u0026gt;= 0x80: This checks if x requires more than 7 bits for representation. If it does, x needs to be split. byte(x) | 0x80: This applies a bitwise OR with 0x80 (1000 0000), ensuring the highest bit is set to 1 and extracting the lowest 7 bits of x. x \u0026gt;\u0026gt;= 7: Shift x right by 7 bits to process the next group. buf[i] = byte(x): When the loop ends, the highest bits are all zeros, so no further action is needed. Uvarint is the reverse of PutUvarint.\nIt should be noted that: varint splits integers into 7-bit groups, meaning large integers may face inefficiencies. For example, uint64\u0026rsquo;s maximum value requires 10 bytes instead of the usual 8 (64/7 ≈ 10).\nEncoding Negative Numbers: Zigzag Encoding Though varint is efficient, it doesn’t account for negative numbers. In computing, numbers are stored as two’s complement, which means a small negative number might have a sizeable binary representation.\nFor example, -5 in 32-bit form is represented as 11111111111111111111111111111011, requiring 5 bytes in varint encoding\nGo uses zigzag encoding to solve this problem:\nFor positive numbers n, map them to 2n. For negative numbers -n, map them to 2n-1.\nThis way, positive and negative numbers alternate without conflict, hence the name zigzag encoding.\nFor example, after zigzag encoding int32(-5), the value becomes 9 (00000000000000000000000000001001), which varint can represent with just 1 byte. Here’s the Golang implementation:\nlist3: go src Putvarint\n1 2 3 4 5 6 7 func Putvarint(buf []byte, x int64) int { ux := uint64(x) \u0026lt;\u0026lt; 1 if x \u0026lt; 0 { ux = ^ux } return PutUvarint(buf, ux) } From the code, we can see that for the implementation of varint for signed integers, the Go standard library breaks it down into two steps:\nFirst, the integer is converted using ZigZag encoding. Then, the converted value is encoded using varint. For negative numbers, there is an extra step: ux = ^ux. This part might be confusing—why does this transformation result in 2n - 1?\nWe can roughly deduce the process, assuming we have an integer -n:\nFirst, the original value is shifted left, then inverted. This can be viewed as: first invert the value, then shift left, and finally add 1. This results in 2*(~(-n)) + 1. the two’s complement of a negative number is the bitwise inversion of its absolute value plus 1. So, how do we derive the absolute value from the two’s complement? There is a formula: |A| = ~A + 1. Substituting this formula into the first step: 2*(n - 1) + 1 = 2n - 1. This perfectly matches the ZigZag encoding for negative numbers (mathematics is indeed excellent). In the Go standard library, calling PutUvarint only applies varint encoding, while calling PutVarint first applies ZigZag encoding and then varint encoding.\nIn protobuf, if the type is int32, int64, uint32, or uint64, only varint encoding is used. However, for sint32 and sint64, ZigZag encoding is applied first, followed by varint encoding.\nWhen varint Is Not Suitable Despite its benefits, varint isn\u0026rsquo;t ideal for all scenarios:\nLarge integers: varint can be less efficient than fixed-length encoding for huge numbers. Random data access: Since varint uses variable lengths, indexing specific integers directly is challenging. Frequent mathematical operations: varint-encoding data requires decoding before operations, potentially affecting performance. Security-sensitive applications: varint encoding may leak information about the original integer\u0026rsquo;s size, which could be unacceptable in secure environments. ","date":"2024-09-06T17:52:42+08:00","image":"https://images.hxzhouh.com/blog-images/2024/09/8f70bb43e8ae963c5655a667adf6b88d.png","permalink":"https://huizhou92.com/p/decrypt-go-varint/","title":"Decrypt Go: varint"},{"content":"If we want to do some resource release before an object is GC, we can use returns.SetFinalizer. It’s like executing defer to free resources before a function returns.\nFor example:\nList 1: Using runtime.SetFinalizer\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type MyStruct struct { Name string Other *MyStruct } func main() { x := MyStruct{Name: \u0026#34;X\u0026#34;} runtime.SetFinalizer(\u0026amp;x, func(x *MyStruct) { fmt.Printf(\u0026#34;Finalizer for %s is called\\n\u0026#34;, x.Name) }) runtime.GC() time.Sleep(1 * time.Second) runtime.GC() } The official documentation explains that SetFinalizer associates a finalizer function with an object. When the garbage collector (GC) detects that an unreachable object has an associated finalizer, it will execute the finalizer and disassociate it. The object will be collected on the next GC cycle if it is unreachable and no longer has an associated finalizer.\nImportant Considerations While runtime.SetFinalizer can be helpful, there are a few critical points to keep in mind:\nDeferred Execution: The SetFinalizer function will not execute until the object is selected for garbage collection. Therefore, avoid using SetFinalizer for actions like flushing in-memory content to disk. Extended Object Lifecycle: SetFinalizer can inadvertently extend an object\u0026rsquo;s lifecycle. The finalizer function executes during the first GC cycle, and the target object may become reachable again, delaying its final destruction. This can be problematic in high-concurrency algorithms with numerous object allocations. Memory Leaks with Cyclic References: Using runtime.SetFinalizer, in conjunction with cyclic references, can lead to memory leaks. List 2: Memory Leak with runtime.SetFinalizer\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 type MyStruct struct { Name string Other *MyStruct } func main() { x := MyStruct{Name: \u0026#34;X\u0026#34;} y := MyStruct{Name: \u0026#34;Y\u0026#34;} x.Other = \u0026amp;y y.Other = \u0026amp;x runtime.SetFinalizer(\u0026amp;x, func(x *MyStruct) { fmt.Printf(\u0026#34;Finalizer for %s is called\\n\u0026#34;, x.Name) }) time.Sleep(time.Second) runtime.GC() time.Sleep(time.Second) runtime.GC() } In this code, the object x will never be released. The correct approach explicitly removes the finalizer when the object is no longer needed: runtime.SetFinalizer(\u0026amp;x, nil).\nPractical Applications While runtime.SetFinalizer is rarely used in business code (I\u0026rsquo;ve never used it); it is more commonly employed within the Go source code itself. For instance, consider the following usage in the net/http package:\n1 2 3 4 5 6 7 8 9 10 11 12 13 func (fd *netFD) setAddr(laddr, raddr Addr) { fd.laddr = laddr fd.raddr = raddr runtime.SetFinalizer(fd, (*netFD).Close) } func (fd *netFD) Close() error { if fd.fakeNetFD != nil { return fd.fakeNetFD.Close() } runtime.SetFinalizer(fd, nil) return fd.pfd.Close() } The go-cache library also demonstrates a use case for SetFinalizer:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 func New(defaultExpiration, cleanupInterval time.Duration) *Cache { items := make(map[string]Item) return newCacheWithJanitor(defaultExpiration, cleanupInterval, items) } func newCacheWithJanitor(de time.Duration, ci time.Duration, m map[string]Item) *Cache { c := newCache(de, m) C := \u0026amp;Cache{c} if ci \u0026gt; 0 { runJanitor(c, ci) runtime.SetFinalizer(C, stopJanitor) } return C } func runJanitor(c *cache, ci time.Duration) { j := \u0026amp;janitor{ Interval: ci, stop: make(chan bool), } c.janitor = j go j.Run(c) } func stopJanitor(c *Cache) { c.janitor.stop \u0026lt;- true } func (j *janitor) Run(c *cache) { ticker := time.NewTicker(j.Interval) for { select { case \u0026lt;-ticker.C: c.DeleteExpired() case \u0026lt;-j.stop: ticker.Stop() return } } } In newCacheWithJanitor, when the ci parameter is greater than 0, a background goroutine is started to clean up expired cache entries periodically via a ticker. The asynchronous goroutine exits once a value is read from the stop channel.\nThe stopJanitor function defines a finalizer for the Cache pointer C. When there are no more references to Cache in the business code, the GC process triggers the stopJanitor function, which writes a value to the internal stop channel. This notifies the asynchronous cleanup goroutine to exit, providing a graceful and business-agnostic way to reclaim resources.\n","date":"2024-09-04T20:25:18+08:00","image":"https://images.hxzhouh.com/blog-images/2024/09/44dc3c5b905dd33169c691b6ec42b4cf.png","permalink":"https://huizhou92.com/p/decrypt-go-runtime.setfinalizer/","title":"Decrypt Go: `runtime.SetFinalizer`"},{"content":"In the previous article, I implemented a simple RPC interface using the net/rpc package and tried out the Gob encoding that comes with net/rpc and JSON encoding to learn some basics of Golang RPC. In this post, I\u0026rsquo;ll combine net/rpc with protobuf and create my protobuf plugin to help us generate code, so let\u0026rsquo;s get started.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nWe must have used gRPC and protobuf during our work, but they are not bound. gRPC can be encoded using JSON, and protobuf can be implemented in other languages.\nProtocol Buffers (Protobuf) is a free and open-source cross-platform data format used to serialize structured data. It is useful in developing programs that communicate with each other over a network or for storing data. The method involves an interface description language that describes the structure of some data and a program that generates source code from that description for generating or parsing a stream of bytes that represents the structured data.\nAn example of using protobuf First we write a proto file hello-service.proto that defines a message \u0026ldquo;String\u0026rdquo;\n1 2 3 4 5 6 7 syntax = \u0026#34;proto3\u0026#34;; package api; option go_package=\u0026#34;api\u0026#34;; message String { string value = 1; } Then use the protoc utility to generate the Go code for the message String\n1 protoc --go_out=. hello-service.proto Then we modify the Hello function\u0026rsquo;s arguments to use the String generated by the protobuf file.\n1 2 3 type HelloServiceInterface = interface { Hello(request api.String, reply *api.String) error } Using it is no different from before, even, it is not as convenient as using string directly. So why should we use protobuf? As I said earlier, using Protobuf to define language-independent RPC service interfaces and messages, and then using the protoc tool to generate code in different languages, is where its real value lies. For example, use the official plugin protoc-gen-go to generate gRPC code.\n1 protoc --go_out=plugins=grpc. hello-service.proto Plugin system for protoc To generate code from protobuf files, we must install the protoc , but the protoc doesn\u0026rsquo;t know what our target language is, so we need plugins to help us generate code. how does protoc\u0026rsquo;s plugin system work? Take the above grpc as an example.\nThere is a --go_out parameter here. Since the plugin we\u0026rsquo;re calling is protoc-gen-go, the parameter is called go_out; if the name was XXX, the parameter would be called XXX_out.\nWhen protoc is running, it will first parse the protobuf file and generate a set of Protocol Buffers-encoded descriptive data. It will first determine whether or not the go plugin is included in protoc, and then it will try to look for protoc-gen-go in $PATH, and if it can\u0026rsquo;t find it, it will report an error, and then it will run protoc-gen-go. protoc-gen-go command and sends the description data to the plugin command via stdin. After the plugin generates the file contents, it then inputs Protocol Buffers encoded data to stdout to tell protoc to generate the specific file.\nplugins=grpc is a plugin that comes with protoc-gen-go in order to invoke it. If you don\u0026rsquo;t use it, it will only generate a message in Go, but you can use this plugin to generate grpc-related code.\nCustomize a protoc plugin If we add Hello interface timing to protobuf, can we customize a protoc plugin to generate code directly?\n1 2 3 4 5 6 7 8 9 syntax = \u0026#34;proto3\u0026#34;; package api; option go_package=\u0026#34;./api\u0026#34;; service HelloService { rpc Hello (String) returns (String) {} } message String { string value = 1; } Objective For this article, my goal was to create a plugin that would then be used to generate RPC server-side and client-side code that would look something like this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // HelloService_rpc.pb.go type HelloServiceInterface interface { Hello(String, *String) error } func RegisterHelloService( srv *rpc.Server, x HelloServiceInterface, ) error { if err := srv.RegisterName(\u0026#34;HelloService\u0026#34;, x); err != nil { return err } return nil } type HelloServiceClient struct { *rpc.Client } var _ HelloServiceInterface = (*HelloServiceClient)(nil) func DialHelloService(network, address string) ( *HelloServiceClient, error, ) { c, err := rpc.Dial(network, address) if err != nil { return nil, err } return \u0026amp;HelloServiceClient{Client: c}, nil } func (p *HelloServiceClient) Hello( in String, out *String, ) error { return p.Client.Call(\u0026#34;HelloService.Hello\u0026#34;, in, out) } This would change our business code to look like the following\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 // service func main() { listener, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:1234\u0026#34;) if err != nil { log.Fatal(\u0026#34;ListenTCP error:\u0026#34;, err) } _ = api.RegisterHelloService(rpc.DefaultServer, new(HelloService)) for { conn, err := listener.Accept() if err != nil { log.Fatal(\u0026#34;Accept error:\u0026#34;, err) } go rpc.ServeConn(conn) } } type HelloService struct{} func (p *HelloService) Hello(request api.String, reply *api.String) error { log.Println(\u0026#34;HelloService.proto Hello\u0026#34;) *reply = api.String{Value: \u0026#34;Hello:\u0026#34; + request.Value} return nil } // client.go func main() { client, err := api.DialHelloService(\u0026#34;tcp\u0026#34;, \u0026#34;localhost:1234\u0026#34;) if err != nil { log.Fatal(\u0026#34;net.Dial:\u0026#34;, err) } reply := \u0026amp;api.String{} err = client.Hello(api.String{Value: \u0026#34;Hello\u0026#34;}, reply) if err != nil { log.Fatal(err) } log.Println(reply) } Based on the generated code, our workload is already much smaller and the chances of error are already very small. A good start.\nBased on the api code above, we can pull out a template file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 const tmplService = ` package {{.PackageName}} import ( \u0026#34;net/rpc\u0026#34;) {{$root := .}} type {{.ServiceName}}Interface interface { {{- range $_, $m := .MethodList}} {{$m.MethodName}}({{$m.InputTypeName}}, *{{$m.OutputTypeName}}) error {{- end}}} func Register{{.ServiceName}}( srv *rpc.Server, x {{.ServiceName}}Interface,) error { if err := srv.RegisterName(\u0026#34;{{.ServiceName}}\u0026#34;, x); err != nil { return err } return nil} type {{.ServiceName}}Client struct { *rpc.Client} var _ {{.ServiceName}}Interface = (*{{.ServiceName}}Client)(nil) func Dial{{.ServiceName}}(network, address string) ( *{{.ServiceName}}Client, error,) { c, err := rpc.Dial(network, address) if err != nil { return nil, err } return \u0026amp;{{.ServiceName}}Client{Client: c}, nil} {{range $_, $m := .MethodList}} func (p *{{$root.ServiceName}}Client) {{$m.MethodName}}( in {{$m.InputTypeName}}, out *{{$m.OutputTypeName}},) error { return p.Client.Call(\u0026#34;{{$root.ServiceName}}.{{$m.MethodName}}\u0026#34;, in, out)} {{end}} ` The whole template is clear, and there are some placeholders in it, such as MethodName, ServiceName, etc., which we\u0026rsquo;ll cover later.\nHow to develop a plug-in? Google released the Go language API 1, which introduces a new package google.golang.org/protobuf/compile R/protogen, which greatly reduces the difficulty of plugins development:\nFirst of all, we create a go language project, such as protoc-gen-go-spprpc Then we need to define a protogen.Options, then call its Run method, and pass in a func(*protogen.Plugin) error callback. This is the end of the main process code. We can also set the ParamFunc parameter of protogen.Options, so that protogen will automatically parse the parameters passed by the command line for us. Operations such as reading and decoding protobuf information from standard input, encoding input information into protobuf and writing stdout are all handled by protogen. What we need to do is to interact with protogen.Plugin to implement code generation logic. The most important thing for each service is the name of the service, and then each service has a set of methods. For the method defined by the service, the most important thing is the name of the method, as well as the name of the input parameter and the output parameter type. Let\u0026rsquo;s first define a ServiceData to describe the meta information of the service:\n1 2 3 4 5 6 7 8 9 10 11 12 // ServiceData type ServiceData struct { PackageName string ServiceName string MethodList []Method } // Method type Method struct { MethodName string InputTypeName string OutputTypeName string } Then comes the main logic, and the code generation logic, and finally the call to tmpl to generate the code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 func main() { protogen.Options{}.Run(func(gen *protogen.Plugin) error { for _, file := range gen.Files { if !file.Generate { continue } generateFile(gen, file) } return nil }) } // generateFile function definition func generateFile(gen *protogen.Plugin, file *protogen.File) { filename := file.GeneratedFilenamePrefix + \u0026#34;_rpc.pb.go\u0026#34; g := gen.NewGeneratedFile(filename, file.GoImportPath) tmpl, err := template.New(\u0026#34;service\u0026#34;).Parse(tmplService) if err != nil { log.Fatalf(\u0026#34;Error parsing template: %v\u0026#34;, err) } packageName := string(file.GoPackageName) // Iterate over each service to generate code for _, service := range file.Services { serviceData := ServiceData{ ServiceName: service.GoName, PackageName: packageName, } for _, method := range service.Methods { inputType := method.Input.GoIdent.GoName outputType := method.Output.GoIdent.GoName serviceData.MethodList = append(serviceData.MethodList, Method{ MethodName: method.GoName, InputTypeName: inputType, OutputTypeName: outputType, }) } // Perform template rendering err = tmpl.Execute(g, serviceData) if err != nil { log.Fatalf(\u0026#34;Error executing template: %v\u0026#34;, err) } } } Debug plugin Finally, we put the compiled binary execution file protoc-gen-go-spprpc in $PATH, and then run protoc to generate the code we want.\n1 protoc --go_out=.. --go-spprpc_out=.. HelloService.proto Because protoc-gen-go-spprpc has to depend on protoc to run, it\u0026rsquo;s a bit tricky to debug. We can use\n1 fmt.Fprintf(os.Stderr, \u0026#34;Fprintln: %v\\n\u0026#34;, err) To print the error log to debug.\nSummary That\u0026rsquo;s all there is to this article. We first implemented an RPC call using protobuf and then created a protobuf plugin to help us generate the code. This opens the door for us to learn protobuf + RPC, and is our path to a thorough understanding of gRPC. I hope everyone can master this technology.\nReference https://taoshu.in/go/create-protoc-plugin.html https://chai2010.cn/advanced-go-programming-book/ch4-rpc/ch4-02-pb-intro.html ","date":"2024-08-18T15:11:02+08:00","image":"https://images.hxzhouh.com/blog-images/2024/09/f12cd0683ca01b5929359e404d7a0ebe.png","permalink":"https://huizhou92.com/p/rpc-action-ep2-using-protobuf-and-creating-a-custom-plugin/","title":"RPC Action EP2: Using Protobuf and Creating a Custom Plugin"},{"content":" tmux is a terminal multiplexer: it enables a number of terminals to be created, accessed, and controlled from a single screen. tmux may be detached from a screen and continue running in the background, then later reattached.\nMy first encounter with tmux left me unimpressed—I didn\u0026rsquo;t see the appeal. However, as I spent more time working with the terminal and faced various challenges, I revisited tmux and was amazed at how it revolutionized my workflow. This article aims to give a ten-minute introduction to the basics of tmux and its practical applications.\nUnderstanding Terminal Sessions Reflect on how you typically use the terminal in your daily work. You open an iTerm2 window, use SSH to connect to a remote machine, navigate to a specific directory, and start working. Once done, you close the iTerm2 window. This entire process is a terminal session. Its lifecycle is tied to the terminal\u0026rsquo;s lifecycle, meaning the session ends when you close the window. How can you decouple the session from the terminal, eliminating the need to repeat these steps each time? This is where tmux comes in handy.\nLet\u0026rsquo;s explore how to use tmux to achieve this functionality.\nIn this demonstration, I create a tmux session using tmux new -s test, log into my development machine, detach the session, return to the iTerm2 terminal, and then use tmux attach-session to reconnect to the same development machine session exactly as I left it. This is the basic application of tmux: detaching and maintaining session states.\nTL;DR tmux enables us to:\nAccess multiple sessions within a single window, useful for running several command-line programs simultaneously. Attach new windows to existing sessions. Allow multiple connected windows for each session, enabling real-time session sharing among multiple users. Support arbitrary vertical and horizontal window splitting. Basic Usage of tmux Installing tmux On Mac, you can install tmux using brew:\n1 brew install tmux For other environments, refer to Installing tmux.\nStarting and Exiting tmux Once installed, type tmux in the terminal to start a tmux session. To exit the tmux session and return to the original terminal screen, simply enter exit.\nPrefix Key Unlike other software, all shortcuts in tmux are combined with the prefix key ⌃b (where ⌃ is the control key on Mac). This reduces conflicts with other software. You can view all shortcuts by pressing ⌃b+?. tmux shortcuts generally fall into three categories: window, pane, and session management.\nSession Management Running the tmux command multiple times will open multiple tmux sessions. Within a tmux session, you can manage sessions using the prefix key ⌃b along with the following shortcuts:\n⌃b + $ — Rename the current session ⌃b + s — Choose from a list of sessions ⌃b + d — Detach the current session, returning to the terminal\u0026rsquo;s main screen. In the terminal, sessions can be managed with the following commands:\ntmux new -s foo — Create a new session named foo tmux ls — List all tmux sessions tmux a — Attach to the last session tmux a -t foo — Attach to a session named foo; sessions are named numerically by default tmux kill-session -t foo — Delete the session named foo tmux kill-server — Delete all sessions Using aliases can enhance your experience. For example, here are my custom configurations:\n1 2 3 4 5 alias tnew=\u0026#39;tmux new -s\u0026#39; # Create a new session alias tls=\u0026#39;tmux ls\u0026#39; alias td=\u0026#39;tmux detach\u0026#39; # Detach session, preserving the state before detachment alias ta=\u0026#39;tmux attach -t\u0026#39; # Attach to a session alias tkss=\u0026#39;tmux kill-session -t\u0026#39; Pane Management tmux can split windows into multiple panes, with each pane running a different command. These commands are executed within the tmux window.\n⌃b + % — Split the pane horizontally ⌃b + \u0026quot; — Split the pane vertically ⌃b + x — Close the current pane ⌃b + { — Move the current pane to the left ⌃b + } — Move the current pane to the right ⌃b + ; — Switch to the last used pane ⌃b + o — Switch to the next pane (you can also use arrow keys) ⌃b + space — Cycle through pane layouts; tmux has five built-in layouts, switchable via ⌥1 to ⌥5 ⌃b + z — Maximize the current pane; repeat to restore to original size ⌃b + q — Display pane numbers; press the corresponding number to switch to that pane Window Management tmux also supports the concept of windows. When panes become crowded, you can open a new window. Here are some commonly used shortcuts for managing windows:\n⌃b + c — Create a new window; this will switch to the new window without affecting the state of the existing window ⌃b + p — Switch to the previous window ⌃b + n — Switch to the next window ⌃b + w — Choose from a list of windows; use ⌃p and ⌃n to navigate on macOS ⌃b + \u0026amp; — Close the current window ⌃b + , — Rename the window; supports Chinese characters for easy identification in the tmux status bar ⌃b + 0 — Switch to window 0; use other numbers to switch to corresponding windows ⌃b + f — Search and select windows by name; supports fuzzy matching Conclusion This article provides a basic summary of how to use tmux and its shortcuts. Many more advanced use cases exist, such as integrating with vim for more efficient coding. I hope this overview encourages you to try tmux and enhance your productivity with this powerful tool.\n","date":"2024-08-16T17:55:19+08:00","permalink":"https://huizhou92.com/p/exploring-the-benefits-of-using-tmux-a-quick-guide-to-terminal-multiplexing/","title":"Exploring the Benefits of Using `tmux`: A Quick Guide to Terminal Multiplexing"},{"content":"From the perspective of the Go compiler, memory allocation happens in two places: the stack and the heap. For most developers, this difference doesn\u0026rsquo;t matter much, as the Go compiler handles memory allocation automatically. However, from a performance standpoint, there is a significant difference between allocating memory on the stack versus the heap. If memory is allocated on the stack within a function, it is automatically reclaimed when the function completes. If it\u0026rsquo;s allocated on the heap, it\u0026rsquo;s managed by the garbage collector (GC), which involves a more complex process and can introduce performance overhead. This phenomenon of allocating memory on the heap when it could be on the stack is known as escape. To optimize performance, developers should minimize heap allocations.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nWhy Does Escape Happen? The primary reasons for memory escape are simple: either the compiler cannot determine the variable\u0026rsquo;s lifetime, or the stack does not have enough space for the required memory. The Go compiler uses escape analysis to decide whether a variable should be allocated on the stack or the heap. This process happens during the compilation stage and can be observed using the -gcflags=-m command to determine if a variable escapes to the heap.\nImpact of Escape on Performance A straightforward benchmark test illustrates the impact of memory escaping. In the example below, BenchmarkInt uses a pointer array, causing escape with \u0026amp;j, whereas BenchmarkInt2 does not cause escape.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func BenchmarkInt(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { a := make([]*int, 100) for j := 0; j \u0026lt; 100; j++ { a[j] = \u0026amp;j } } } func BenchmarkInt2(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { a := make([]int, 100) for j := 0; j \u0026lt; 100; j++ { a[j] = j } } } The results show that BenchmarkInt2 is 30 times faster than BenchmarkInt, with no memory allocations, whereas BenchmarkInt has 101 memory allocations. This test highlights the importance of conducting escape analysis.\nCommon Scenarios of Memory Escaping Variable and Pointer Escaping When a variable\u0026rsquo;s lifecycle extends beyond the function scope, the compiler allocates it on the heap, leading to memory variable escape or pointer escape.\n1 2 3 4 5 6 7 8 9 10 11 12 13 package main import \u0026#34;fmt\u0026#34; func main() { s := makeString() fmt.Println(s) } func makeString() *string { str := \u0026#34;Hello, World!\u0026#34; return \u0026amp;str } Stack Overflow The operating system limits the stack size used by kernel threads; on a 64-bit system, this limit is typically 8 MB. You can check the maximum allowed stack size with the ulimit -a command. The Go runtime dynamically allocates stack space as needed for goroutines, starting with an initial size of 2 KB. If local variables exceed a certain size, typically 64 KB, they escape to the heap. For instance, attempting to create an array of 8193 bytes:\n1 2 3 4 5 6 func main() { a := make([]int64, 8176) b := make([]int64, 8192) c := make([]int64, 8193) println(a, b, c) } Arrays larger than 8192 bytes escape to the heap.\nVariables with Unknown Size 1 2 3 4 5 6 7 8 func main() { a := generate(3) println(a) } func generate(n int) []int { return make([]int, n) } Since the size of generate\u0026rsquo;s parameter is determined at runtime, the compiler cannot allocate it on the stack, causing it to escape to the heap.\ninterface{} Dynamic Type Escape In Go, the empty interface interface{} can represent any type. If a function parameter is interface{}, the compiler cannot determine its type during compilation, leading to memory escape.\n1 2 3 4 func main() { v := \u0026#34;Hello,World\u0026#34; fmt.Printf(\u0026#34;addr of v in bar = %p\\n\u0026#34;, \u0026amp;v) } Since fmt.Printf accepts parameters of type any (equivalent to interface{}), v escapes to the heap.\nClosure In the following example, the function Increase() returns a closure that accesses the external variable n. Thus, n persists beyond the function\u0026rsquo;s scope, causing it to escape to the heap.\n1 2 3 4 5 6 7 8 9 10 11 12 13 func Increase() func() int { n := 0 return func() int { n++ return n } } func main() { in := Increase() fmt.Println(in()) // 1 fmt.Println(in()) // 2 } Manually Avoiding Escapes In the interface{} Dynamic Type Escape example, memory escape occurs even when simply printing \u0026ldquo;Hello, World\u0026rdquo;. To prevent v from escaping, we can use the following function from the Go runtime code:\n1 2 3 4 5 // $GOROOT/src/runtime/stubs.go func noescape(p unsafe.Pointer) unsafe.Pointer { x := uintptr(p) return unsafe.Pointer(x ^ 0) } This function, widely used in the Go standard library and runtime, converts a pointer to a value and then back to a pointer, breaking the escape analysis data flow and preventing the pointer from escaping.\n1 2 3 4 5 6 7 8 9 10 11 func noescape(p unsafe.Pointer) unsafe.Pointer { x := uintptr(p) return unsafe.Pointer(x ^ 0) } func main() { v := \u0026#34;Hello,World\u0026#34; v2 := \u0026#34;Hello,World1\u0026#34; fmt.Printf(\u0026#34;addr of v in bar = %p \\n\u0026#34;, (*int)(noescape(unsafe.Pointer(\u0026amp;v)))) fmt.Printf(\u0026#34;addr of v in bar = %p\\n\u0026#34;, \u0026amp;v2) } The output shows that v does not escape, while v2 does.\nConclusion In typical development scenarios, memory escape analysis is rarely a concern. Based on experience, optimizing a lock can yield better performance improvements than conducting multiple memory escape analyses. During regular development, remember:\nPassing values copies the entire object, while passing a pointer only copies the pointer address, pointing to the same object. Passing a pointer can reduce value copying but may lead to memory allocation escaping to the heap, increasing the garbage collection (GC) burden. The GC overhead from passing pointers can significantly impact performance in frequent object creation and deletion scenarios.\nGenerally, passing a pointer to modify original object values or larger struct objects would be best. Passing values offer better performance for read-only, smaller struct objects. References Five Things That Make Go Fast ICSE20 Paper Understanding Go Escape Analysis by Example Go\u0026rsquo;s Hidden Pragmas ","date":"2024-08-15T16:33:07+08:00","image":"https://images.hxzhouh.com/blog-images/2024/08/317a5c9d3df72bec002eb7c0bd64c06d.png","permalink":"https://huizhou92.com/p/go-high-performance-programming-ep9-escape-analysis/","title":"Go High-Performance Programming EP9: Escape Analysis"},{"content":"RPC(Remote Procedure Call)is a widely used communication method between different nodes in distributed systems and a foundational technology of the Internet era. Go\u0026rsquo;s standard library provides a simple implementation of RPC under the net/rpc package. This article aims to help you understand RPC by walking you through implementing a simple RPC interface using the net/rpc package.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nTo enable a function to be remotely called in net/rpc, it must meet the following five conditions:\nThe method’s type is exported. The method is exported. The method has two arguments, both of which are exported (or built-in) types. The method’s second argument is a pointer. The method has a return type of error. In other words, the function signature must be:\n1 func (t *T) MethodName(argType T1, replyType *T2) error Creating a Simple RPC Request Based on these five conditions, we can construct a simple RPC interface:\n1 2 3 4 5 6 type HelloService struct{} func (p *HelloService) Hello(request string, reply *string) error { log.Println(\u0026#34;HelloService Hello\u0026#34;) *reply = \u0026#34;hello:\u0026#34; + request return nil } Next, you can register an object of the HelloService type as an RPC service:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func main() { _ = rpc.RegisterName(\u0026#34;HelloService\u0026#34;, new(HelloService)) listener, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:1234\u0026#34;) if err != nil { log.Fatal(\u0026#34;ListenTCP error:\u0026#34;, err) } for { conn, err := listener.Accept() if err != nil { log.Fatal(\u0026#34;Accept error:\u0026#34;, err) } go rpc.ServeConn(conn) } } The client-side implementation is as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 func main() { conn, err := net.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;:1234\u0026#34;) if err != nil { log.Fatal(\u0026#34;net.Dial:\u0026#34;, err) } client := rpc.NewClient(conn) var reply string err = client.Call(\u0026#34;HelloService.Hello\u0026#34;, \u0026#34;hello\u0026#34;, \u0026amp;reply) if err != nil { log.Fatal(err) } fmt.Println(reply) } First, the client dials the RPC service using rpc.Dial, then invokes a specific RPC method via client.Call(). The first parameter is the RPC service name and method name combined with a dot, the second is the input, and the third is the return value, which is a pointer. This example demonstrates how easy it is to use RPC.\nIn both the server and client code, we need to remember the RPC service name HelloService and the method name Hello. This can easily lead to errors during development, so we can wrap the code slightly by abstracting the common parts. The complete code is as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 // server.go const ServerName = \u0026#34;HelloService\u0026#34; type HelloServiceInterface = interface { Hello(request string, reply *string) error } func RegisterHelloService(srv HelloServiceInterface) error { return rpc.RegisterName(ServerName, srv) } type HelloService struct{} func (p *HelloService) Hello(request string, reply *string) error { log.Println(\u0026#34;HelloService Hello\u0026#34;) *reply = \u0026#34;hello:\u0026#34; + request return nil } func main() { _ = RegisterHelloService(new(HelloService)) listener, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:1234\u0026#34;) if err != nil { log.Fatal(\u0026#34;ListenTCP error:\u0026#34;, err) } for { conn, err := listener.Accept() if err != nil { log.Fatal(\u0026#34;Accept error:\u0026#34;, err) } go rpc.ServeConn(conn) } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 // client.go type HelloServiceClient struct { *rpc.Client } var _ HelloServiceInterface = (*HelloServiceClient)(nil) const ServerName = \u0026#34;HelloService\u0026#34; func DialHelloService(network, address string) (*HelloServiceClient, error) { conn, err := net.Dial(network, address) client := rpc.NewClient(conn) if err != nil { return nil, err } return \u0026amp;HelloServiceClient{Client: client}, nil } func (p *HelloServiceClient) Hello(request string, reply *string) error { return p.Client.Call(ServerName+\u0026#34;.Hello\u0026#34;, request, reply) } func main() { client, err := DialHelloService(\u0026#34;tcp\u0026#34;, \u0026#34;localhost:1234\u0026#34;) if err != nil { log.Fatal(\u0026#34;net.Dial:\u0026#34;, err) } var reply string err = client.Hello(\u0026#34;hello\u0026#34;, \u0026amp;reply) if err != nil { log.Fatal(err) } fmt.Println(reply) } Does it look familiar?\nImplementing JSON Codec with Go\u0026rsquo;s net/rpc Package By default, Go\u0026rsquo;s standard RPC library uses Go\u0026rsquo;s proprietary Gob encoding. However, it\u0026rsquo;s straightforward to implement other encodings, such as Protobuf or JSON, on top of it. The standard library already supports jsonrpc encoding, and we can implement JSON encoding by making minor changes to the server and client code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // server.go func main() { _ = rpc.RegisterName(\u0026#34;HelloService\u0026#34;, new(HelloService)) listener, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:1234\u0026#34;) if err != nil { log.Fatal(\u0026#34;ListenTCP error:\u0026#34;, err) } for { conn, err := listener.Accept() if err != nil { log.Fatal(\u0026#34;Accept error:\u0026#34;, err) } go rpc.ServeCodec(jsonrpc.NewServerCodec(conn)) //go rpc.ServeConn(conn) } } //client.go func DialHelloService(network, address string) (*HelloServiceClient, error) { conn, err := net.Dial(network, address) //client := rpc.NewClient(conn) client := rpc.NewClientWithCodec(jsonrpc.NewClientCodec(conn)) if err != nil { return nil, err } return \u0026amp;HelloServiceClient{Client: client}, nil } The JSON request data object internally corresponds to two structures: on the client side, it\u0026rsquo;s clientRequest, and on the server side, it\u0026rsquo;s serverRequest. The content of clientRequest and serverRequest structures is essentially the same:\n1 2 3 4 5 6 7 8 9 10 type clientRequest struct { Method string `json:\u0026#34;method\u0026#34;` Params [1]any `json:\u0026#34;params\u0026#34;` Id uint64 `json:\u0026#34;id\u0026#34;` } type serverRequest struct { Method string `json:\u0026#34;method\u0026#34;` Params *json.RawMessage `json:\u0026#34;params\u0026#34;` Id *json.RawMessage `json:\u0026#34;id\u0026#34;` } Here, Method represents the service name composed of serviceName and Method. The first element of Params is the parameter, and Id is a unique call number maintained by the caller, used to distinguish requests in concurrent scenarios.\nWe can use nc to simulate the server and then run the client code to see what information the JSON-encoded client sends to the server:\n1 nc -l 1234 The nc command receives the following data:\n1 {\u0026#34;method\u0026#34;:\u0026#34;HelloService.Hello\u0026#34;,\u0026#34;params\u0026#34;:[\u0026#34;hello\u0026#34;],\u0026#34;id\u0026#34;:0} This is consistent with serverRequest.\nWe can also run the server code and use nc to send a request:\n1 2 3 echo -e \u0026#39;{\u0026#34;method\u0026#34;:\u0026#34;HelloService.Hello\u0026#34;,\u0026#34;params\u0026#34;:[\u0026#34;Hello\u0026#34;],\u0026#34;Id\u0026#34;:1}\u0026#39; | nc localhost 1234 --- {\u0026#34;id\u0026#34;:1,\u0026#34;result\u0026#34;:\u0026#34;hello:Hello\u0026#34;,\u0026#34;error\u0026#34;:null} Conclusion This article introduced the rpc package from Go\u0026rsquo;s standard library, highlighting its simplicity and powerful performance. Many third-party rpc libraries are built on top of the rpc package. This article serves as the first installment in a series on RPC research. In the next article, we will combine protobuf with RPC and eventually implement our own RPC framework.\n","date":"2024-08-12T18:09:15+08:00","image":"https://images.hxzhouh.com/blog-images/2024/08/d4dddb722300af56ecbb03cefdf1aec1.png","permalink":"https://huizhou92.com/p/rpc-action-ep1-implement-a-simple-rpc-interface-in-go/","title":"RPC Action EP1: Implement a simple RPC interface in Go"},{"content":"When developing with Golang, we typically don\u0026rsquo;t focus too much on memory management since Golang\u0026rsquo;s runtime efficiently handles garbage collection (GC). However, understanding GC can be significantly beneficial in performance optimization scenarios. This article explores optimizing GC and enhancing code performance using go trace through an example XML parsing service.\nSpecial thanks to Arden Lions for their excellent presentation.\nhttps://www.youtube.com/watch?v=PYMs-urosXs\u0026t=23s If you're not familiar with `go trace`, check out @Vincent's [article on the trace package](https://medium.com/a-journey-with-go/go-discovery-of-the-trace-package-e5a821743c3c). All examples were run on my MacBook Pro M1, which has ten cores.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nOur goal is to create a program that processes multiple RSS XML files and searches for items containing the keyword go in the title. We\u0026rsquo;ll use the RSS XML file from my blog as an example and parse this file 100 times to simulate stress.\nComplete code: GitHub Repository\nSingle-Threaded Approach List 1: Counting Keywords with a Single Goroutine\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 func freq(docs []string) int { var count int for _, doc := range docs { f, err := os.OpenFile(doc, os.O_RDONLY, 0) if err != nil { return 0 } data, err := io.ReadAll(f) if err != nil { return 0 } var d document if err := xml.Unmarshal(data, \u0026amp;d); err != nil { log.Printf(\u0026#34;Decoding Document [Ns] : ERROR :%+v\u0026#34;, err) return 0 } for _, item := range d.Channel.Items { if strings.Contains(strings.ToLower(item.Title), \u0026#34;go\u0026#34;) { count++ } } } return count } func main() { trace.Start(os.Stdout) defer trace.Stop() files := make([]string, 0) for i := 0; i \u0026lt; 100; i++ { files = append(files, \u0026#34;index.xml\u0026#34;) } count := freq(files) log.Println(fmt.Sprintf(\u0026#34;find key word go %d count\u0026#34;, count)) } The code is straightforward; we use a for loop to complete the task and then execute it:\n1 2 3 4 5 6 ➜ go_trace git:(main) ✗ go build ➜ go_trace git:(main) ✗ time ./go_trace 2 \u0026gt; trace_single.out -- result -- 2024/08/02 16:17:06 find key word go 2400 count ./go_trace 2 \u0026gt; trace_single.out 1.99s user 0.05s system 102% cpu 1.996 total Then, we use go trace to view trace_single.out.\nRunTime: 2031ms STW (Stop-the-World): 57ms GC Occurrences: 252ms GC STW AVE: 0.227ms GC time accounts for approximately 57 / 2031 ≈ 0.02 of the total runtime. The maximum memory usage is around 11.28MB.\nFigure 1: Single Thread - Run Time\nFigure 2: Single Thread - GC\nFigure 3: Single Thread - Max Heap\nCurrently, we are using only one core, resulting in low resource utilization. To speed up the program, it\u0026rsquo;s better to use concurrency, which is where Golang excels.\nConcurrent Approach List 2: Counting Keywords Using FinOut\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 func concurrent(docs []string) int { var count int32 g := runtime.GOMAXPROCS(0) wg := sync.WaitGroup{} wg.Add(g) ch := make(chan string, 100) go func() { for _, v := range docs { ch \u0026lt;- v } close(ch) }() for i := 0; i \u0026lt; g; i++ { go func() { var iFound int32 defer func() { atomic.AddInt32(\u0026amp;count, iFound) wg.Done() }() for doc := range ch { f, err := os.OpenFile(doc, os.O_RDONLY, 0) if err != nil { return } data, err := io.ReadAll(f) if err != nil { return } var d document if err = xml.Unmarshal(data, \u0026amp;d); err != nil { log.Printf(\u0026#34;Decoding Document [Ns] : ERROR :%+v\u0026#34;, err) return } for _, item := range d.Channel.Items { if strings.Contains(strings.ToLower(item.Title), \u0026#34;go\u0026#34;) { iFound++ } } } }() } wg.Wait() return int(count) } Run the program using the same method:\n1 2 3 4 5 go build time ./go_trace 2 \u0026gt; trace_pool.out --- 2024/08/02 19:27:13 find key word go 2400 count ./go_trace 2 \u0026gt; trace_pool.out 2.83s user 0.13s system 673% cpu 0.439 total RunTime: 425ms STW: 154ms GC Occurrences: 39 GC STW AVE: 3.9ms GC time accounts for approximately 154 / 425 ≈ 0.36 of the total runtime. The maximum memory usage is 91.60MB.\nFigure 4: Concurrent - GC Count\nFigure 5: Concurrent - Max Heap\nThe concurrent version is about five times faster than the single-threaded version. In the go trace results, we can see that GC occupies 36% of the runtime in the concurrent version. Is there a way to optimize this time? Fortunately, in Go 1.19, we have two parameters to control GC.\nGOGC \u0026amp; GOMEMLIMIT In Go 1.19, two parameters were added to control GC. GOGC controls the frequency of garbage collection, while GOMEMLIMIT limits the maximum memory usage of a program. For detailed information on GOGC and GOMEMLIMIT, refer to the official documentation gc-guide.\nGOGC According to the official documentation, the formula is as follows:\n$New heap memory = (Live heap + GC roots) * GOGC / 100$\nTheoretically, if we set GOGC to 1000, it will reduce the frequency of GC by ten times at the cost of increasing memory usage tenfold (this is a theoretical model, and reality is more complex). Let\u0026rsquo;s give it a try.\n1 2 3 ➜ go_trace git:(main) ✗ time GOGC=1000 ./go_trace 2 \u0026gt; trace_gogc_1000.out 2024/08/05 16:57:29 find key word go 2400 count GOGC=1000 ./go_trace 2 \u0026gt; trace_gogc_1000.out 2.46s user 0.16s system 757% cpu 0.346 total RunTime: 314ms STW: 9.572ms GC Occurrences: 5 GC STW AVE: 1.194ms GC time accounts for approximately 9.572 / 314 ≈ 0.02 of the total runtime. The maximum memory usage is 451MB.\nFigure 6: GOGC - Max Heap\nFigure 7: GOGC - GC Count\nGOMEMLIMIT GOMEMLIMIT is used to set a program\u0026rsquo;s memory usage limit. It is typically used when automatic GC is disabled, allowing us to manage the total memory usage manually. When the allocated memory reaches the limit, GC will be triggered. Note that even though GC works hard, the memory usage may still exceed the GOMEMLIMIT.\nOur program uses 11.28MB of memory in the single-threaded version. In the concurrent version, ten goroutines run simultaneously. According to the gc-guide, we need to reserve 10% of the memory for emergencies. Therefore, we can set GOMEMLIMIT to 11.28MB * 1.1 ≈ 124MB.\n1 2 3 ➜ go_trace git:(main) ✗ time GOGC=off GOMEMLIMIT=124MiB ./go_trace 2 \u0026gt; trace_mem_limit.out 2024/08/05 18:10:55 find key word go 2400 count GOGC=off GOMEMLIMIT=124MiB ./go_trace 2 \u0026gt; trace_mem_limit.out 2.83s user 0.15s system 766% cpu 0.389 total RunTime: 376.455ms STW: 41.578ms GC Occurrences: 14 GC STW AVE: 2.969ms GC time accounts for approximately 41.578 / 376.455 ≈ 0.11 of the total runtime. The maximum memory usage is 120MB, close to our set limit.\nFigure 8: GOMEMLIMIT - GC Max Heap\nFigure 9: GOMEMLIMIT - GC Count\nAs shown in the trace below, increasing the GOMEMLIMIT parameter can yield better results, such as with GOMEMLIMIT=248MiB.\nFigure 10: GOMEMLIMIT=248MiB - GC\nRunTime: 320.455ms STW: 11.429ms GC Occurrences: 5 GC STW AVE: 2.285ms However, it is not without limits. For instance, with GOMEMLIMIT=1024MiB, RunTime has already reached 406ms.\nFigure 11: GOMEMLIMIT=1024MiB - GC\nRisks The Suggested Uses section of the official documentation provides clear recommendations. Do not use these two parameters unless you are very familiar with your program\u0026rsquo;s runtime environment and workload. Be sure to read the gc-guide.\nConclusion Let\u0026rsquo;s summarize the optimization process and results:\nFigure 12: Result Comparison\nUsing GOGC and GOMEMLIMIT in suitable scenarios can effectively improve performance. It provides a sense of control over an uncertain aspect. However, it must be applied judiciously in controlled environments to ensure performance and reliability. Use caution in resource-sharing or uncontrolled environments to avoid performance degradation or program crashes due to improper settings.\nReferences YouTube Video Uber Blog Golang GC Guide ","date":"2024-08-05T19:37:41+08:00","image":"https://images.hxzhouh.com/blog-images/2024/08/42af44f7ae36f49965e3ad6452fca825.png","permalink":"https://huizhou92.com/p/go-high-performance-programming-ep8-optimizing-gc-in-golang-using-go-trace/","title":"Go High-Performance Programming EP8:  Optimizing GC in Golang Using  go trace"},{"content":"When we attempt to optimize code performance, the first step is understanding the current performance to establish a baseline. The Go language standard library includes the testing framework, which provides benchmarking capabilities. This article will cover how to use benchmarking for performance testing, how to improve the accuracy of benchmarks and introduce two tools to help facilitate benchmarking.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nThe environment highly influences performance testing. To ensure the repeatability of tests, strive to keep the testing environment as stable as possible:\nIdle Machine: Ensure the machine is idle and do not run other tasks or share hardware resources with others during testing. Power-Saving Mode: Disable power-saving mode, which is often enabled by default on laptops. Avoid Virtual Machines and Cloud Servers: Avoid using virtual machines and cloud servers for testing, as they typically oversell CPU and memory resources, leading to unstable performance results. Multiple Runs: A single test run is meaningless; statistically significant results require multiple runs. For most benchmarks, I typically execute the test 20 times. How Benchmarking Works Benchmarking involves repeatedly calling a function and recording execution time and other metrics to measure its performance. Here is a typical benchmark function:\n1 2 3 4 5 6 7 8 9 10 11 12 13 func fib(n int) int { if n \u0026lt; 2 { return n } return fib(n-1) + fib(n-2) } func BenchmarkFib20(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { // Call the function we\u0026#39;re benchmarking fib(20) } } Benchmark functions must start with Benchmark. We can run it as follows:\n1 2 go test -bench . go test -bench=\u0026#34;BenchmarkFib20\u0026#34; Understanding b.N The benchmark test parameter b *testing.B includes an attribute b.N, which represents the number of times the test case needs to run. The value of b.N differs for each test case.\nHow is this value determined? b.N starts at 1, and if the test case can complete within 1 second, the value of b.N will increase and execute again. The value of b.N increases approximately in the sequence of 1, 2, 3, 5, 10, 20, 30, 50, 100, etc., accelerating as it goes.\nOther Parameters count: Controls the number of runs, independent of b.N. benchtime: Controls the run time, limited to 1 second. benchmem: Measures the number of memory allocations. cpu: Specifies the number of CPU cores to use; by default, it uses go. Additionally, benchmark tests support other go test parameters like cpuprofile, memprofile, and trace.\nImproving Accuracy Reducing System Noise with perflock perflock limits CPU clock frequency, thereby minimizing the system\u0026rsquo;s impact on performance tests and reducing noise, resulting in more reliable and consistent performance measurement results.\nUsing ResetTimer If preparation work is required before the benchmark starts, and the preparation work is time-consuming, you should ignore the time consumed by this part of the code.\nUsing StopTimer and StartTimer StopTimer and StartTimer follow the same principle. When preparation and cleanup work is needed before and after each function call, use StopTimer to pause timing and StartTimer to resume timing.\nMeasuring Benchmark Results benchstat is an official comparison tool used to compare performance differences between two benchmark runs.\nBenchstat computes statistical summaries and A/B comparisons of Go benchmarks.\nWe use benchstat to compare the performance of bubbleSort and quickSort.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func softNums() { //bubbleSort(nums) //quickSort(nums) } func initNums(count int) { rand.Seed(time.Now().UnixNano()) nums = make([]int, count) for i := 0; i \u0026lt; count; i++ { nums[i] = rand.Intn(count) } } func BenchmarkSoftNums(b *testing.B) { initNums(10000) b.ResetTimer() for i := 0; i \u0026lt; b.N; i++ { softNums() } } 1 2 3 4 5 6 7 8 9 10 go test -bench=\u0026#34;BenchmarkSoftNums\u0026#34; -count=10 |tee quicksoft.txt go test -bench=\u0026#34;BenchmarkSoftNums\u0026#34; -count=10 |tee bubblesoft.txt ➜ benchmark git:(main) ✗ benchstat bubblesoft.txt quicksoft.txt goos: darwin goarch: arm64 pkg: blog-example/go/benchmark │ bubblesoft.txt │ quicksoft.txt │ │ sec/op │ sec/op vs base │ SoftNums-10 31942.4µ ± 1% 775.8µ ± 2% -97.57% (p=0.000 n=10) Isn\u0026rsquo;t this result very clear? You can include it in reports or GitHub issues, making it look very professional.\nThird-Party Tools bench bench is a benchmarking tool for Go programs that provides integrated performance measurement, automatic performance locking, statistical analysis, and color indication.\nfuncbench funcbench is a Prometheus project tool used to automate Go code benchmarking and performance comparison. Here are its main features and functions:\nSupports comparing local branches with GitHub branches. Features performance locking and performance comparison to improve development efficiency. If the project is large, consider using this approach. References https://dave.cheney.net/high-performance-go-workshop/gophercon-2019.html#welcome https://golang.design/under-the-hood/zh-cn/part3tools/ch09analysis/perf/ ","date":"2024-07-24T20:01:59+08:00","image":"https://images.hxzhouh.com/blog-images/2024/07/7c30776c74ed13c1b32cfd9ed3690a56.png","permalink":"https://huizhou92.com/p/golang-high-performance-programming-ep5-benchmark/","title":"Golang High-Performance Programming EP5: Benchmark"},{"content":"Using cached database data to accelerate queries is common in developing business applications. However, this architecture presents challenges like cache penetration, avalanche, and cache breakdown. Cache breakdown occurs in high-concurrency systems when numerous requests simultaneously query an expired key, leading to a surge in database requests and significantly increasing the database load. If we can combine multiple identical requests into one within a short period, the database load can be reduced from N requests to just 1.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nSingleFlight is a concurrency primitive that addresses this problem. This article introduces the usage and basic principles of SingleFlight and details its implementation.\nFigure 1: Merging Identical Requests with SingleFlight\nUsing SingleFlight The Go language team has placed singleflight in the golang.org/x directory, indicating that it may undergo future changes. This package provides a mechanism to suppress redundant function calls, allowing for the prevention of simultaneous identical function calls. If a function is called once, subsequent duplicate calls wait, and once the first call completes, all calls receive the result. Thus, although the function is executed only once, all callers get the final result.\nListing 1: Merging Database Requests with SingleFlight\nhttps://gist.github.com/hxzhouh/bf7d6276d6a703628abbd812964c3081\nAs shown, we simulated five requests with a cache miss, but only one request accessed the database. All five requests received the same result. This optimization is significant for backend services as it prevents the database from being overwhelmed by cache penetration.\nIn the Go source code, you can see the use of SingleFlight, for example, in /src/net/lookup.go#L165 and /src/cmd/go/internal/vcs/vcs.go#L1385. The caching library groupcache also uses a similar implementation to prevent cache penetration.\nSingleFlight Analysis The singleflight package defines a structure type named Group, representing a category of work and forming a namespace where duplicate suppression can execute work units. The primary data structure of SingleFlight is Group, which provides three methods:\nDo: This method executes a function and returns its result. You need to provide a key; for the same key, only one function executes at a time, while concurrent requests wait. The result of the first executed request is the return result. The fn function is parameterless and returns a result or error, while the Do method returns the function\u0026rsquo;s result or error, and shared indicates whether v is returned to multiple requests. DoChan: Similar to the Do method, but returns a channel. Once the fn function completes and produces a result, it can be received from this channel. Forget: Instructs the Group to forget a key, allowing future requests for this key to execute fn instead of waiting for the result of a previously incomplete fn function. Implementation Details Request Blocking Internally, singleflight uses WaitGroup to block all subsequent requests with the same key except the first one. Other requests are returned only after the first request is executed and returns from fn. If fn takes a long time to execute, all subsequent requests will be blocked. In this scenario, you can use DoChan with time.After + select for timeout control.\nlist2: Use doChan to control the timeout.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // list2: user doChan func doChan(name string) User { result := g.DoChan(\u0026#34;user\u0026#34;, func() (interface{}, error) { data := GetUserFromDB(name) return data, nil }) select { case v := \u0026lt;-result: log.Println(\u0026#34;get user from db\u0026#34;) return v.Val.(User) case \u0026lt;-time.After(1 * time.Second): log.Println(\u0026#34;timeout\u0026#34;) return User{} } } Forget The singleflight implementation dictates that if the first request fails, all waiting requests return the same error. To address this, you can periodically forget a key based on database usage, allowing more requests to reach subsequent logic.\n1 2 3 4 go func() { time.Sleep(100 * time.Millisecond) g.Forget(name) }() For example, if 100 requests come in within one second, normally, only the first request executes GetUserFromDB, while the subsequent 99 requests are blocked. By adding this Forget mechanism, one request executes GetUserFromDB every 100ms, providing multiple attempts and placing a greater load on the database. This trade-off needs to be carefully considered based on the specific scenario.\n","date":"2024-07-23T16:47:43+08:00","image":"https://images.hxzhouh.com/blog-images/2024/07/d516998be001327a93e663f8f504840a.png","permalink":"https://huizhou92.com/p/go-high-performance-programming-ep7-use-singleflight-to-merge-the-same-request/","title":"Go High-Performance Programming EP7: Use singleflight To Merge The Same Request"},{"content":"Introduction The primary mission of Golang is to simplify asynchronous programming. When faced with operations requiring batch processing and lengthy execution times, traditional single-threaded execution becomes cumbersome, prompting the need for asynchronous parallel processing. This article introduces some tips for asynchronous programming in Golang.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nLet\u0026rsquo;s start by introducing a library that simplifies concurrent programming: conc. It encapsulates many useful tools, such as WaitGroup and iter.Map. While we may not necessarily use conc in production code, learning from its concepts is still beneficial.\nUsage Using go The simplest and most common method: use the go keyword.\n1 2 3 4 5 6 7 8 func main() { go func() { fmt.Println(\u0026#34;hello world1\u0026#34;) }() go func() { fmt.Println(\u0026#34;hello world2\u0026#34;) }() } Or:\n1 2 3 4 5 6 7 func main() { go Announce(\u0026#34;hello world1\u0026#34;) go Announce(\u0026#34;hello world2\u0026#34;) } func Announce(message string) { fmt.Println(message) } Using anonymous functions to pass parameters:\n1 2 3 4 5 data := \u0026#34;Hello, World!\u0026#34; go func(msg string) { // Use msg to perform asynchronous task logic processing fmt.Println(msg) }(data) This method doesn\u0026rsquo;t require consideration of return values. If return values are needed, the following method can be used.\nImplementing Timeout Control with Goroutines and Channels 1 2 3 4 5 6 7 8 9 10 11 12 13 ch := make(chan int, 1) timer := time.NewTimer(time.Second) go func() { time.Sleep(2 * time.Second) ch \u0026lt;- 1 close(ch) }() select { case \u0026lt;-timer.C: fmt.Println(\u0026#34;timeout\u0026#34;) case result := \u0026lt;-ch: fmt.Println(result) } Using sync.WaitGroup sync.WaitGroup is used to wait for a collection of goroutines to finish their tasks. The Add() method increases the number of goroutines to wait for, the Done() method marks a goroutine as completed, and the Wait() method blocks until all goroutines are finished.\n1 2 3 4 5 6 7 8 9 10 var wg sync.WaitGroup // Start multiple goroutines for i := 0; i \u0026lt; 5; i++ { wg.Add(1) go func(index int) { defer wg.Done() // Asynchronous task logic }(i) } wg.Wait() Error Handling with errgroup for Goroutine Groups The errgroup package is useful for easily capturing errors from goroutines. It’s a utility in the Go standard library for managing a group of goroutines and handling their errors.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 var eg errgroup.Group for i := 0; i \u0026lt; 5; i++ { eg.Go(func() error { return errors.New(\u0026#34;error\u0026#34;) }) eg.Go(func() error { return nil }) } if err := eg.Wait(); err != nil { // Handle error } Tips and Techniques Using Range and Close Operations with Channels The range can be used to iterate over the values received on a channel until the channel is closed. Use the close function to close the channel, signaling no more values will be sent.\n1 2 3 4 5 6 7 8 9 10 11 12 13 ch := make(chan int) go func() { for i := 0; i \u0026lt; 5; i++ { ch \u0026lt;- i // Send values to the channel } close(ch) // Close the channel }() // Use range to iterate over received values for val := range ch { // Process received values } Waiting Multiple Goroutines With Select 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ch1 := make(chan int) ch2 := make(chan string) go func() { // Asynchronous task 1 logic ch1 \u0026lt;- result1 }() go func() { // Asynchronous task 2 logic ch2 \u0026lt;- result2 }() // Wait for multiple asynchronous tasks to complete in the main goroutine select { case res1 := \u0026lt;-ch1: // Process result 1 case res2 := \u0026lt;-ch2: // Process result 2 } Implementing Timeout Control with Select and time.After() If you need to set a timeout for asynchronous operations, you can use the select statement in conjunction with the time.After() function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ch := make(chan int) go func() { // Asynchronous task logic time.Sleep(2 * time.Second) ch \u0026lt;- result }() // Set a timeout select { case res := \u0026lt;-ch: // Process result case \u0026lt;-time.After(3 * time.Second): // Handle timeout } Using time.Tick() and time.After() for Timed Operations The time.Tick() function returns a channel that sends time values periodically, useful for executing timed operations. The time.After() function returns a channel that sends a time value after a specified duration.\n1 2 3 4 5 6 7 8 9 10 11 12 13 tick := time.Tick(1 * time.Second) // Execute an operation every second for { select { case \u0026lt;-tick: // Perform timed operation } } select { case \u0026lt;-time.After(5 * time.Second): // Execute operation after 5 seconds } Using sync.Mutex or sync.RWMutex for Concurrent Safe Access When multiple goroutines concurrently access shared data, it’s essential to ensure data access safety. sync.Mutex and sync.RWMutex provide mutual exclusion locks and read-write locks for locking before accessing shared resources, preventing data races.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 var mutex sync.Mutex var data int // Write operation protected by a mutex mutex.Lock() data = 123 mutex.Unlock() // Read operation protected by a read lock mutex.RLock() value := data mutex.RUnlock() var rwMutex sync.RWMutex var sharedData map[string]string // Read operation protected by a read lock func readData(key string) string { rwMutex.RLock() defer rwMutex.RUnlock() return sharedData[key] } // Write operation protected by a write lock func writeData(key, value string) { rwMutex.Lock() defer rwMutex.Unlock() sharedData[key] = value } Note: sync.Mutex locks cannot be nested. sync.RWMutex read locks (RLock()) can be nested if there are no write locks, and multiple read locks can be acquired.\nUsing sync.Cond for Conditional Variable Control sync.Cond is a conditional variable used for communication and synchronization between goroutines. It can block and wait until a specified condition is met, then wake up waiting goroutines when the condition is satisfied.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 var cond = sync.NewCond(\u0026amp;sync.Mutex{}) var ready bool go func() { // Asynchronous task logic ready = true // Notify waiting goroutines that the condition is met cond.Broadcast() }() // Wait for the condition to be met cond.L.Lock() for !ready { cond.Wait() } cond.L.Unlock() Managing Object Pools with sync.Pool sync.Pool is an object pool for caching and reusing temporary objects, improving allocation and recycling efficiency.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 type MyObject struct { // Object structure } var objectPool = sync.Pool{ New: func() interface{} { // Create a new object return \u0026amp;MyObject{} }, } // Get an object from the pool obj := objectPool.Get().(*MyObject) // Use the object // Put the object back into the pool objectPool.Put(obj) Ensuring One-Time Execution with sync.Once sync.Once ensures that an operation is executed only once, regardless of how many goroutines attempt to execute it. It\u0026rsquo;s commonly used for initialization or loading resources.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 var once sync.Once var resource *Resource func getResource() *Resource { once.Do(func() { // Perform resource initialization, executed only once resource = initResource() }) return resource } // Get the resource in multiple goroutines go func() { res := getResource() // Use the resource }() go func() { res := getResource() // Use the resource }() Resource Cleanup with sync.Once and context.Context Combine sync.Once and context.Context to ensure a resource cleanup operation is executed only once across multiple goroutines, triggered on cancellation or timeout.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 var once sync.Once func cleanup() { // Perform resource cleanup } func doTask(ctx context.Context) { go func() { select { case \u0026lt;-ctx.Done(): once.Do(cleanup) // Execute resource cleanup only once } }() // Asynchronous task logic } Concurrent Safe Maps with sync.Map sync.Map is a concurrent-safe map type in the\nGo standard library, enabling safe read and write operations across multiple goroutines.\n1 2 3 4 5 6 7 8 9 10 11 12 var m sync.Map // Store key-value pairs m.Store(\u0026#34;key\u0026#34;, \u0026#34;value\u0026#34;) // Retrieve values if val, ok := m.Load(\u0026#34;key\u0026#34;); ok { // Use the value } // Delete a key m.Delete(\u0026#34;key\u0026#34;) Managing Goroutines and Cancellation with context.Context context.Context is used to pass context information between goroutines and can be used for cancellation or timeout control. Use context.WithCancel() to create a cancellable context, and context.WithTimeout() to create a context with a timeout.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 ctx, cancel := context.WithCancel(context.Background()) go func() { // Asynchronous task logic if someCondition { cancel() // Cancel the task } }() // Wait for the task to complete or be canceled select { case \u0026lt;-ctx.Done(): // Task canceled or timed out } Setting Deadlines with context.WithDeadline() and context.WithTimeout() context.WithDeadline() and context.WithTimeout() functions create contexts with deadlines to limit the execution time of asynchronous tasks.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func doTask(ctx context.Context) { // Asynchronous task logic select { case \u0026lt;-time.After(5 * time.Second): // Handle timeout case \u0026lt;-ctx.Done(): // Handle context cancellation } } func main() { ctx := context.Background() ctx, cancel := context.WithTimeout(ctx, 3*time.Second) defer cancel() go doTask(ctx) // Continue with other operations } Passing Context Values with context.WithValue() context.WithValue() allows passing key-value pairs within a context, enabling sharing and passing context-related values between goroutines.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type keyContextValue string func doTask(ctx context.Context) { if val := ctx.Value(keyContextValue(\u0026#34;key\u0026#34;)); val != nil { // Use the context value } } func main() { ctx := context.WithValue(context.Background(), keyContextValue(\u0026#34;key\u0026#34;), \u0026#34;value\u0026#34;) go doTask(ctx) // Continue with other operations } Atomic Operations with the atomic Package The atomic package provides functions for atomic operations, ensuring atomicity in read and write operations on shared variables in concurrent environments.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 var counter int64 func increment() { atomic.AddInt64(\u0026amp;counter, 1) } func main() { var wg sync.WaitGroup for i := 0; i \u0026lt; 100; i++ { wg.Add(1) go func() { defer wg.Done() increment() }() } wg.Wait() fmt.Println(\u0026#34;Counter:\u0026#34;, counter) } Summary In this article, we introduce some commonly used keywords. Mastering these keywords will make it easy to deal with common concurrent programming. I believe you have also discovered that the go source code does not provide the commonly used Barrier function in java and c#. Although we can also implement the Barrier function using basic concurrent statements, it is not as convenient as calling the ready-made API interface. There is SingleFlight in golang.org/x and the third-party CyclicBarrier. In the next article, we will introduce these two concurrent primitives.\n","date":"2024-07-22T16:47:32+08:00","permalink":"https://huizhou92.com/p/golang-high-performance-programming-ep6-tips-for-asynchronous-programming/","title":"Golang High-Performance Programming EP6 : Tips For Asynchronous Programming"},{"content":"reflect provides Go with the ability to dynamically obtain the types and values of objects at runtime and create objects dynamically. Reflection can help abstract and simplify code, enhancing development efficiency. The Go standard library and many open-source projects utilize Go\u0026rsquo;s reflection capabilities, such as the json package for serialization and deserialization, and ORM frameworks like gorm/xorm. This article aims to explore reflect and how to improve its performance.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nExample Code\nHow to Simplify Code Using Reflection Let\u0026rsquo;s implement a simple functionality using reflection to see how it can help us simplify the code.\nSuppose we have a configuration class Config, with each field representing a configuration item. We need to read environment variables prefixed with CONFIG_xxxx and initialize Config accordingly.\nList 1: MySQL config\n1 2 3 4 5 6 7 8 9 // https://github.com/go-sql-driver/mysql/blob/v1.8.1/dsn.go#L37 type Config struct { User string `json:\u0026#34;user\u0026#34;` // Username Passwd string `json:\u0026#34;passwd\u0026#34;` // Password (requires User) Net string `json:\u0026#34;net\u0026#34;` // Network (e.g. \u0026#34;tcp\u0026#34;, \u0026#34;tcp6\u0026#34;, \u0026#34;unix\u0026#34;. default: \u0026#34;tcp\u0026#34;) Addr string `json:\u0026#34;addr\u0026#34;` // Address (default: \u0026#34;127.0.0.1:3306\u0026#34; for \u0026#34;tcp\u0026#34; and \u0026#34;/tmp/mysql.sock\u0026#34; for \u0026#34;unix\u0026#34;) DBName string `json:\u0026#34;db_name\u0026#34;` // Database name // 。。。。。 } Footguns It\u0026rsquo;s easy to write code like this:\nList 2: Initializing Config using a for loop\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func InitConfig() *Config { cfg := \u0026amp;Config{} keys := []string{\u0026#34;CONFIG_MYSQL_USER\u0026#34;, \u0026#34;CONFIG_MYSQL_PASSWD\u0026#34;, \u0026#34;CONFIG_MYSQL_NET\u0026#34;, \u0026#34;CONFIG_MYSQL_ADDR\u0026#34;, \u0026#34;CONFIG_MYSQL_DB_NAME\u0026#34;} for _, key := range keys { if env, exist := os.LookupEnv(key); exist { switch key { case \u0026#34;CONFIG_MYSQL_USER\u0026#34;: cfg.User = env case \u0026#34;CONFIG_MYSQL_PASSWORD\u0026#34;: cfg.Passwd = env case \u0026#34;CONFIG_MYSQL_NET\u0026#34;: cfg.Net = env case \u0026#34;CONFIG_MYSQL_ADDR\u0026#34;: cfg.Addr = env case \u0026#34;CONFIG_MYSQL_DB_NAME\u0026#34;: cfg.DBName = env } } } return cfg } However, using hardcoding means that if the Config structure changes, such as modifying the corresponding json fields, deleting, or adding a configuration item, this logic also needs to change. A bigger problem is that it\u0026rsquo;s very error-prone and hard to test.\nIf we switch to using reflection, the implementation code would look like this:\nList 3: Initializing Config using reflect\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func InitConfig2() *Config { config := Config{} typ := reflect.TypeOf(config) value := reflect.Indirect(reflect.ValueOf(\u0026amp;config)) for i := 0; i \u0026lt; typ.NumField(); i++ { f := typ.Field(i) if v, ok := f.Tag.Lookup(\u0026#34;json\u0026#34;); ok { key := fmt.Sprintf(\u0026#34;CONFIG_MYSQL_%s\u0026#34;, strings.ToUpper(v)) if env, exist := os.LookupEnv(key); exist { value.FieldByName(f.Name).Set(reflect.ValueOf(env)) } } } return \u0026amp;config } The implementation logic is quite simple:\nAt runtime, use reflection to obtain the Tag property of each field in Config and concatenate the corresponding environment variable names. Check if the environment variable exists. If it does, assign its value to the field.\nThis way, regardless of adding or deleting fields in Config, the InitConfig function doesn\u0026rsquo;t need modification. Isn\u0026rsquo;t it much simpler than using a for loop? Reflection Performance We\u0026rsquo;ve often heard that the performance of reflection is poor. When comparing JSON parsing libraries, we also verified that the official JSON Unmarshal library performs relatively poorly because it needs to execute more instructions. So, how poor is the performance of reflection exactly? Let\u0026rsquo;s perform a benchmark to find out.\nList 4: Benchmark test for New and Reflect Performance\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 func BenchmarkNew(b *testing.B) { var config *Config for i := 0; i \u0026lt; b.N; i++ { config = new(Config) } _ = config } func BenchmarkReflectNew(b *testing.B) { var config *Config typ := reflect.TypeOf(Config{}) b.ResetTimer() for i := 0; i \u0026lt; b.N; i++ { config, _ = reflect.New(typ).Interface().(*Config) } _ = config } ---- ➜ go_reflect git:(main) ✗ go test --bench . goos: darwin goarch: arm64 pkg: blog-example/go/go_reflect BenchmarkNew-10 47675076 25.40 ns/op BenchmarkReflectNew-10 36163776 32.51 ns/op PASS ok blog-example/go/go_reflect 3.895s If it\u0026rsquo;s just a creation scenario, the performance gap between the two isn\u0026rsquo;t very significant.\nLet\u0026rsquo;s test the field modification scenario. There are two ways to modify fields using reflection:\nFieldByName and Field index mode.\nWe\u0026rsquo;ll test the performance of both modes separately.\nList 5: Testing the performance of modifying fields\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 func BenchmarkFieldSet(b *testing.B) { config := new(Config) b.ResetTimer() for i := 0; i \u0026lt; b.N; i++ { config.Net = \u0026#34;tcp4\u0026#34; config.Addr = \u0026#34;127.0.0.1:3306\u0026#34; config.Passwd = \u0026#34;123456\u0026#34; config.User = \u0026#34;admin\u0026#34; } } func BenchmarkFieldSetFieldByName(b *testing.B) { config := new(Config) value := reflect.ValueOf(config).Elem() b.ResetTimer() for i := 0; i \u0026lt; b.N; i++ { value.FieldByName(\u0026#34;Net\u0026#34;).SetString(\u0026#34;tcp4\u0026#34;) value.FieldByName(\u0026#34;Addr\u0026#34;).SetString(\u0026#34;127.0.0.1:3306\u0026#34;) value.FieldByName(\u0026#34;Passwd\u0026#34;).SetString(\u0026#34;123456\u0026#34;) value.FieldByName(\u0026#34;User\u0026#34;).SetString(\u0026#34;admin\u0026#34;) } } func BenchmarkFieldSetField(b *testing.B) { config := new(Config) value := reflect.Indirect(reflect.ValueOf(config)) b.ResetTimer() for i := 0; i \u0026lt; b.N; i++ { value.Field(0).SetString(\u0026#34;tcp4\u0026#34;) value.Field(1).SetString(\u0026#34;127.0.0.1:3306\u0026#34;) value.Field(2).SetString(\u0026#34;123456\u0026#34;) value.Field(3).SetString(\u0026#34;admin\u0026#34;) } } ---- ➜ go_reflect git:(main) ✗ go test --bench=\u0026#34;BenchmarkFieldSet*\u0026#34; goos: darwin goarch: arm64 pkg: blog-example/go/go_reflect BenchmarkFieldSet-10 1000000000 0.3282 ns/op BenchmarkFieldSetFieldByName-10 6471114 185.3 ns/op BenchmarkFieldSetField-10 100000000 11.88 ns/op PASS ok blog-example/go/go_reflect 3.910s The performance difference is significant. Non-reflective methods compared to the reflect Field index mode have a gap of two orders of magnitude. Compared to the reflect FieldByName mode, the gap reaches three orders of magnitude. One puzzling point is that the FieldByName mode and the Field index mode also have an order of magnitude difference. However, we can find the answer in the source code.\nList 6: reflect/value.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 func (v Value) FieldByName(name string) Value { v.mustBe(Struct) if f, ok := toRType(v.typ()).FieldByName(name); ok { return v.FieldByIndex(f.Index) } return Value{} } // FieldByIndex returns the nested field corresponding to index.// It panics if evaluation requires stepping through a nil // pointer or a field that is not a struct. func (v Value) FieldByIndex(index []int) Value { if len(index) == 1 { return v.Field(index[0]) } v.mustBe(Struct) for i, x := range index { if i \u0026gt; 0 { if v.Kind() == Pointer \u0026amp;\u0026amp; v.typ().Elem ().Kind() == abi.Struct { if v.IsNil() { panic(\u0026#34;reflect: indirection through nil pointer to embedded struct\u0026#34;) } v = v.Elem() } } v = v.Field(x) } return v } List 7: reflect/type.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 func (t *rtype) FieldByName(name string) (StructField, bool) { if t.Kind() != Struct { panic(\u0026#34;reflect: FieldByName of non-struct type \u0026#34; + t.String()) } tt := (*structType)(unsafe.Pointer(t)) return tt.FieldByName(name) } // FieldByName returns the struct field with the given name// and a boolean to indicate if the field was found. func (t *structType) FieldByName(name string) (f StructField, present bool) { // Quick check for top-level name, or struct without embedded fields. hasEmbeds := false if name != \u0026#34;\u0026#34; { for i := range t.Fields { tf := \u0026amp;t.Fields[i] if tf.Name.Name() == name { return t.Field(i), true } if tf.Embedded() { hasEmbeds = true } } } if !hasEmbeds { return } return t.FieldByNameFunc(func(s string) bool { return s == name }) } Internally, fields are stored sequentially, so accessing them by index has an O(1) efficiency, while accessing by Name requires traversing all fields, which has an O(N) efficiency. The more fields (including methods) a struct contains, the greater the efficiency difference between the two methods. However, remembering the order of fields can be error-prone.\nHow to Improve Performance Avoid Using Reflect Whenever Possible Using reflection for assignments is highly inefficient. If there are alternative methods, avoid using reflection, especially in repeatedly called hotspots. For example, in an RPC protocol, where structures need to be serialized and deserialized, avoid using Go\u0026rsquo;s built-in json Marshal and Unmarshal methods because the standard library\u0026rsquo;s JSON serialization and deserialization are implemented using reflection. Using fastjson instead of the standard library can yield a performance improvement of about ten times.\nUse Field Index Mode Whenever Possible From the previous benchmark, the Field index mode is nearly an order of magnitude faster than the FieldByName method. This difference becomes more evident when there are more fields. However, using the Field index mode can be cumbersome because you need to remember the indexes, making it difficult to modify. In this case, you can use a map to cache the names and indexes.\nFor example:\nList 8: Cache FieldByName and Field Index\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func BenchmarkFieldSetFieldByNameCache(b *testing.B) { config := new(Config) typ := reflect.TypeOf(Config{}) value := reflect.ValueOf(config).Elem() cache := make(map[string]int) for i := 0; i \u0026lt; typ.NumField(); i++ { cache[typ.Field(i).Name] = i } b.ResetTimer() for i := 0; i \u0026lt; b.N; i++ { value.Field(cache[\u0026#34;Net\u0026#34;]).SetString(\u0026#34;tcp4\u0026#34;) value.Field(cache[\u0026#34;Addr\u0026#34;]).SetString(\u0026#34;127.0.0.1:3306\u0026#34;) value.Field(cache[\u0026#34;Passwd\u0026#34;]).SetString(\u0026#34;123456\u0026#34;) value.Field(cache[\u0026#34;User\u0026#34;]).SetString(\u0026#34;admin\u0026#34;) } } ---- BenchmarkFieldSetFieldByNameCache-10 32121740 36.85 ns/op This method shows a fourfold improvement over directly using FieldByName, which is quite significant.\nConclusion This article does not delve into the details of reflect. If you want to learn more about reflect, you can refer to the following articles: Learning to Use Go Reflection Go101: Reflection Reflect is used extensively in various foundational libraries. Using reflect appropriately can simplify code, but it comes with performance costs, potentially reducing performance by up to three orders of magnitude in extreme cases. You can optimize performance using Field index + cache. Do you have any other thoughts on reflection? Leave a comment and discuss it with me.\n","date":"2024-07-11T11:03:15+08:00","image":"https://images.hxzhouh.com/blog-images/2024/07/fafe2d9c964dbdd44c76d86834fd745b.png","permalink":"https://huizhou92.com/p/golang-high-performance-programming-ep4-reflect/","title":"Golang High-Performance Programming EP4 : reflect"},{"content":" All examples in this article use a MacBook Pro M1, a 64-bit architecture CPU.\nThis is the third article on high-performance programming in Go, analyzing why memory alignment is needed, the rules of Go memory alignment, and practical examples of memory alignment usage. Finally, it shares two tools to help us identify memory alignment issues during development.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nWhat is Memory Alignment? To a programmer, memory might just be a huge array. We can write an int16, which occupies two bytes, or an int32, which occupies four bytes. For example:\n1 2 3 4 5 type T1 struct { a int8 b int64 c int16 } Those unfamiliar with Go might think the structure is laid out like this, taking up a total of 11 bytes of space.\nFigure 1: Memory layout as understood by some people\nOne after another, very compact and perfect. But in reality, it\u0026rsquo;s not like this. If we print the addresses of T1 variables, we\u0026rsquo;ll find they look something like this, occupying a total of 24 bytes of space.\nFigure 2: Actual memory layout of T1\nList 1: T1 size\n1 2 3 4 5 6 7 8 9 10 func main() { t := T1{} fmt.Println(fmt.Sprintf(\u0026#34;%d %d %d %d\u0026#34;, unsafe.Sizeof(t.a), unsafe.Sizeof(t.b), unsafe.Sizeof(t.c), unsafe.Sizeof(t))) fmt.Println(fmt.Sprintf(\u0026#34;%p %p %p\u0026#34;, \u0026amp;t.a, \u0026amp;t.b, \u0026amp;t.c)) fmt.Println(unsafe.Alignof(t)) } // output // 1 8 2 24 // 0x14000114018 0x14000114020 0x14000114028 // 8 The CPU fetches data from memory based on word size. For example, a 64-bit CPU has a word size of 8 bytes, meaning the CPU accesses memory in 8-byte units, referred to as memory access granularity.\nThis phenomenon can cause several serious problems:\nPerformance degradation due to an extra CPU instruction. What was originally an atomic operation for reading a variable is no longer atomic. Other unexpected situations. Therefore, compilers generally implement memory alignment, sacrificing memory space to ensure:\nPlatform Compatibility: Not all hardware platforms can access arbitrary data at arbitrary addresses. For example, specific hardware platforms only allow fetching specific types of data at specific addresses, otherwise leading to exceptions. Performance: Accessing unaligned memory causes the CPU to perform two memory accesses and spend extra clock cycles handling alignment and computation. Aligned memory can be accessed in a single operation, improving efficiency—a typical space-for-time tradeoff. Memory Alignment in Go The Go spec stipulates Go\u0026rsquo;s alignment rules.\n1 2 3 4 5 6 7 type size in bytes byte, uint8, int8 1 uint16, int16 2 uint32, int32, float32 4 uint64, int64, float64, complex64 8 complex128 16 For a variable x of any type: unsafe.Alignof(x) is at least 1. For a variable x of struct type: unsafe.Alignof(x) is the largest of all the values unsafe.Alignof(x.f) for each field f of x, but at least 1. For a variable x of array type: unsafe.Alignof(x) is the same as the alignment of a variable of the array\u0026rsquo;s element type. In most cases, the Go compiler automatically aligns memory for us, and we don\u0026rsquo;t need to worry about it. However, in one particular case, manual alignment is required.\nFor 64-bit pointer atomic operations on the x86 platform, alignment is mandatory because 64-bit atomic operations on a 32-bit platform require 8-byte alignment, or the program will panic. For example, consider the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main import \u0026#34;sync/atomic\u0026#34; type T3 struct { b int64 c int32 d int64 } func main() { a := T3{} atomic.AddInt64(\u0026amp;a.d, 1) } Running this on the amd64 architecture won\u0026rsquo;t cause an error, but on the i386 architecture, it will panic.\nFigure 3: T3 panic\nThe reason is that T3 is 4-byte aligned on a 32-bit platform and 8-byte aligned on a 64-bit platform. On a 64-bit platform, its memory layout is:\nFigure 4: T3 memory layout on amd64\nBut on the i386 layout:\nFigure 5: T3 memory layout on i386\nThis issue is documented in the atomic package.\nOn 386, the 64-bit functions use instructions unavailable before the Pentium MMX.\nOn non-Linux ARM, the 64-bit functions use instructions unavailable before the ARMv6k core.\nOn ARM, 386, and 32-bit MIPS, it is the caller\u0026rsquo;s responsibility to arrange for 64-bit alignment of 64-bit words accessed atomically via the primitive atomic functions (types Int64 and Uint64 are automatically aligned). The first word in an allocated struct, array, or slice; in a global variable; or in a local variable (because the subject of all atomic operations will escape to the heap) can be relied upon to be 64-bit aligned. To resolve this, we must manually pad T3 to make it \u0026ldquo;look\u0026rdquo; 8-byte aligned:\n1 2 3 4 5 6 type T3 struct { b int64 c int32 _ int32 d int64 } Similar operations can be seen in the Go source code and open-source libraries, such as:\nmgc groupcache Fortunately, we have many tools to help identify and optimize these issues.\nPractical Engineering fieldalignment fieldalignment is an official Go tool that helps us identify potential memory alignment optimizations in code and automatically aligns them. For example, it will automatically convert T1 to be memory-aligned.\n1 2 3 4 5 6 7 8 9 ➜ go_mem_alignment git:(main) ✗ fieldalignment -fix . /Users/hxzhouh/workspace/github/blog-example/go/go_mem_alignment/main.go:8:8: struct of size 24 could be 16 // change type T1 struct { b int64 c int16 a int8 } It can also be used in golangci-lint. fieldalignment is a sub-function of govet, enabled in .golangci.yaml as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # .golangci.yml linters: disable-all: true enable: - govet fast: false linters-settings: govet: # report about shadowed variables check-shadowing: false fast: false # disable: # - fieldalignment # I\u0026#39;m ok to waste some bytes enable: - fieldalignment However, fieldalignment has a frustrating drawback: it removes all blank lines and comments when rearranging struct members. Therefore, you should git commit once, use this tool, then review its changes via git diff and make necessary post-processing adjustments. Thus, I rarely use this tool in production, preferring structlayout.\nstructlayout structlayout displays the layout and size of structs and can output data in svg or json formats. If a struct is complex, this tool can help optimize it.\nVisualize and Optimize Go Struct Layout with structlayout structlayout allows you to display the layout and size of structs, outputting data in SVG or JSON format. If a struct is complex, this tool can be used to optimize it.\nInstallation 1 2 3 4 go install honnef.co/go/tools/cmd/structlayout@latest go install honnef.co/go/tools/cmd/structlayout-pretty@latest go install honnef.co/go/tools/cmd/structlayout-optimize@latest go install github.com/ajstarks/svgo/structlayout-svg@latest Analyze T1 with structlayout 1 structlayout -json ./main.go T1 | structlayout-svg \u0026gt; T1.svg Figure 6: T1 Structure Layout\nWe can clearly see two padding areas: 7 size and 6 size.\nOptimized T2 1 2 3 4 5 type T2 struct { a int8 c int16 b int64 } Figure 7: T2 Structure Layout\nThere are still two padding areas, but only 5 sizes.\nSummary In programming, memory alignment is a crucial technique designed to enhance program performance and compatibility. This article uses Go as an example to explain the basic concepts and necessity of memory alignment in detail, demonstrating the actual layout of different structs in memory through code examples.\nMemory alignment rules in Go are primarily reflected in the order of struct fields. The compiler ensures performance and platform portability through automatic alignment, but in some cases, developers need to manually adjust struct fields to avoid performance issues and potential errors.\nThe empty struct is a helpful tool for memory alignment optimization. For specific operations, refer to my other article: Golang High-Performance Programming EP1: Empty Struct.\nTo help developers detect and optimize memory alignment issues, this article introduces two practical tools:\nfieldalignment: An official Go tool that can automatically optimize struct memory alignment. structlayout: Displays the memory layout of structs, helping developers understand and optimize memory usage more intuitively. By using these tools effectively, developers can reduce memory waste and improve development efficiency while ensuring program performance and stability.\nReferences IBM DeveloperWorks: Data Alignment Go Specification: Size and Alignment Guarantees ","date":"2024-07-07T22:03:43+08:00","image":"https://images.hxzhouh.com/blog-images/2024/07/b3eca51256266ff32f8a27c85544e1c8.jpg","permalink":"https://huizhou92.com/p/golang-high-performance-programming-ep3-memory-alignment/","title":"Golang High-Performance Programming EP3 : Memory Alignment"},{"content":"We all know that Golang has a significant feature: its compilation speed is extremely fast. The speed of compilation was a key consideration when Go was being designed. But have you ever looked at the size of the binary executable file produced after Go compiles your code? Let\u0026rsquo;s take a look at a simple HTTP server example.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { // create a http server and create a handler hello, return hello world http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hello, World\\n\u0026#34;) }) // listen to port 8080 err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) if err != nil { return } } The compiled size is 6.5M.\n1 2 3 ➜ binary-size git:(main) ✗ go build -o server main.go ➜ binary-size git:(main) ✗ ls -lh server -rwxr-xr-x 1 hxzhouh staff 6.5M Jul 2 14:20 server The Go compiler trims the size of the binary file. If you\u0026rsquo;re interested in this, check out my other article How Does the Go Compiler Reduce Binary File Size?.\nNow let\u0026rsquo;s try to optimize the size of the server.\nRemoving Debug Information By default, the Go compiler includes a symbol table and debug information in the compiled program. Typically, you can remove this debug information for a release version to reduce the binary size.\n1 2 3 ➜ binary-size git:(main) ✗ go build -ldflags=\u0026#34;-s -w\u0026#34; -o server main.go ➜ binary-size git:(main) ✗ ls -lh server -rwxr-xr-x 1 hxzhouh staff 4.5M Jul 2 14:30 server -s: Omit the symbol table and debug information. -w: Omit the DWARFv3 debug information. With this option, you cannot use gdb to debug.\nThe size dropped from 6.5M to 4.5M, about a 30% reduction. This is a good first step. Using UPX UPX is an advanced executable file compressor. UPX will typically reduce the file size of programs and DLLs by around 50%-70%, thus reducing disk space, network load times, download times, and other distribution and storage costs.\nOn Mac, you can install UPX via brew.\n1 brew install upx Compressing with UPX Alone UPX has many parameters, with the most important being the compression ratio, ranging from 1-13. 1 represents the lowest compression ratio, and 13 is the highest.\nLet\u0026rsquo;s see how much we can reduce the binary size using UPX alone.\n1 2 ➜ binary-size git:(main) ✗ go build -o server main.go \u0026amp;\u0026amp; upx --brute server \u0026amp;\u0026amp; ls -lh server -rwxr-xr-x 1 hxzhouh staff 3.9M Jul 2 14:38 server The compression ratio is about 60%.\nUPX + Compiler Options Enabling both UPX and -ldflags=\u0026quot;-s -w\u0026quot;:\n1 2 ➜ binary-size git:(main) ✗ go build -ldflags=\u0026#34;-s -w\u0026#34; -o server main.go \u0026amp;\u0026amp; upx --brute server \u0026amp;\u0026amp; ls -lh server -rwxr-xr-x 1 hxzhouh staff 1.4M Jul 2 14:40 server Finally, we get an executable file size of 1.4M compared to the uncompressed 6.5M, saving about 80% of the space. This is quite considerable for large applications.\nHow UPX Works The compressed program works like the original and can run normally without decompression. This compression method is called shell compression, which includes two parts:\nInsert decompression code at the beginning or another suitable place in the program. Compress the rest of the program. When executed, it also includes two parts:\nThe decompression code inserted at the beginning is executed first, decompressing the original program into memory. Then, the decompressed program is executed.\nUPX introduces an extra decompression step when the program is executed, but this time is almost negligible.\nIf you don\u0026rsquo;t have strict requirements on the compiled size, you might choose not to use UPX compression. Generally, server-side standalone background services do not need compressed size. Conclusion This post contains many interesting answers:\nFor example, when implementing the same functionality with C and Go (a small demo), the executable size generated by C is 1/20th of that generated by Go (why?). Using println instead of fmt.Println can avoid importing the fmt package, further reducing the size. Long Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2024-07-02T15:02:46+08:00","image":"https://images.yixiao9206.cn/blog-images/2024/07/5f408427cbeaa5018a5af3e3499fdb82.png","permalink":"https://huizhou92.com/p/golang-high-performance-programming-ep2-reduce-the-size-of-executable-binary-files-with-upx/","title":"Golang High-Performance Programming EP2:  Reduce The Size Of Executable Binary Files With upx"},{"content":"Thanks to Moore\u0026rsquo;s Law, computer performance has greatly improved, along with advancements in databases and various anti-pattern designs advocated by microservices. As a result, we now have fewer opportunities to write complex SQL queries. The industry (yes, even Google) has started advocating against specialized SQL optimization, as the resources saved do not outweigh the cost of employee salaries. However, as engineers, we should strive for technical excellence to become rocket scientists in our field.\nIn this article, I will introduce eight common SQL slow query statements and explain how to optimize their performance. I hope this will be helpful to you.\nLIMIT Statement Pagination is one of the most commonly used scenarios, but it is also prone to problems. For the simple statement below, a typical solution a DBA suggests is adding a composite index to the type, name, and create_time fields. This way, the conditions and sorting can effectively utilize the index, significantly improving performance.\n1 2 3 4 5 6 SELECT * FROM operation WHERE type = \u0026#39;SQLStats\u0026#39; AND name = \u0026#39;SlowLog\u0026#39; ORDER BY create_time LIMIT 1000, 10; Okay, this might solve the problem for over 90% of DBAs. However, when the LIMIT clause becomes \u0026ldquo;LIMIT 1000000, 10\u0026rdquo;, programmers still complain, \u0026ldquo;Why is it slow when I\u0026rsquo;m only fetching 10 records?\u0026rdquo; The database doesn\u0026rsquo;t know where the 1,000,000th record starts, so even with an index, it still needs to calculate from the beginning. In most cases, this performance issue is caused by lazy programming.\nIn scenarios such as frontend data browsing or exporting large data in batches, you can use the previous page\u0026rsquo;s maximum value as a querying parameter. The SQL can be redesigned as follows:\n1 2 3 4 5 6 7 SELECT * FROM operation WHERE type = \u0026#39;SQLStats\u0026#39; AND name = \u0026#39;SlowLog\u0026#39; AND create_time \u0026gt; \u0026#39;2017-03-16 14:00:00\u0026#39; ORDER BY create_time LIMIT 10; With this new design, the query time remains constant and does not change with the increasing data volume.\nImplicit Conversion Another common mistake in SQL statements is when the types of query variables and field definitions do not match. Take the following statement as an example:\n1 2 3 4 5 6 mysql\u0026gt; explain extended SELECT * \u0026gt; FROM my_balance b \u0026gt; WHERE b.bpn = 14000000123 \u0026gt; AND b.isverified IS NULL ; mysql\u0026gt; show warnings; | Warning | 1739 | Cannot use ref access on index \u0026#39;bpn\u0026#39; due to type or collation conversion on field \u0026#39;bpn\u0026#39; In this case, the field bpn is defined as varchar(20), and MySQL\u0026rsquo;s strategy is to convert the string to a number before comparing. This causes the function to be applied to the table field, rendering the index ineffective.\nSuch cases may be caused by parameters automatically filled in by the application framework, rather than the programmer\u0026rsquo;s intention. Nowadays, application frameworks are often complex, and while they provide convenience, they can also create pitfalls.\nJoin Updates and Deletions Although MySQL 5.6 introduced materialization, it only optimizes SELECT statements. For UPDATE or DELETE statements, you need to manually rewrite them using JOIN.\nFor example, consider the following UPDATE statement. MySQL actually performs a loop/nested subquery (DEPENDENT SUBQUERY), and you can imagine the execution time.\n1 2 3 4 5 6 7 8 9 10 11 UPDATE operation o SET status = \u0026#39;applying\u0026#39; WHERE o.id IN (SELECT id FROM (SELECT o.id, o.status FROM operation o WHERE o.group = 123 AND o.status NOT IN ( \u0026#39;done\u0026#39; ) ORDER BY o.parent, o.id LIMIT 1) t); The execution plan is as follows:\n1 2 3 4 5 6 7 +----+--------------------+-------+-------+---------------+---------+---------+-------+------+-----------------------------------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+--------------------+-------+-------+---------------+---------+---------+-------+------+-----------------------------------------------------+ | 1 | PRIMARY | o | index | | PRIMARY | 8 | | 24 | Using where; Using temporary | | 2 | DEPENDENT SUBQUERY | | | | | | | | Impossible WHERE noticed after reading const tables | | 3 | DERIVED | o | ref | idx_2,idx_5 | idx_5 | 8 | const | 1 | Using where; Using filesort | +----+--------------------+-------+-------+---------------+---------+---------+-------+------+-----------------------------------------------------+ After rewriting it as a JOIN, the subquery\u0026rsquo;s select type changes from DEPENDENT SUBQUERY to DERIVED, significantly speeding up the execution time from 7 seconds to 2 milliseconds.\n1 2 3 4 5 6 7 8 9 10 11 UPDATE operation o JOIN (SELECT o.id, o.status FROM operation o WHERE o.group = 123 AND o.status NOT IN ( \u0026#39;done\u0026#39; ) ORDER BY o.parent, o.id LIMIT 1) t ON o.id = t.id SET status = \u0026#39;applying\u0026#39;; The simplified execution plan is as follows:\n1 2 3 4 5 6 +----+-------------+-------+------+---------------+-------+---------+-------+------+-----------------------------------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+------+---------------+-------+---------+-------+------+-----------------------------------------------------+ | 1 | PRIMARY | | | | | | | | Impossible WHERE noticed after reading const tables | | 2 | DERIVED | o | ref | idx_2,idx_5 | idx_5 | 8 | const | 1 | Using where; Using filesort | +----+-------------+-------+------+---------------+-------+---------+-------+------+-----------------------------------------------------+ Mixed Sorting MySQL cannot utilize indexes for mixed sorting. However, in certain scenarios, there are still opportunities to improve performance using special methods.\n1 2 3 4 5 6 SELECT * FROM my_order o INNER JOIN my_appraise a ON a.orderid = o.id ORDER BY a.is_reply ASC, a.appraise_time DESC LIMIT 0, 20; The execution plan shows a full table scan:\n1 2 3 4 5 6 +----+-------------+-------+--------+-------------+---------+---------+---------------+---------+-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra +----+-------------+-------+--------+-------------+---------+---------+---------------+---------+-+ | 1 | SIMPLE | a | ALL | idx_orderid | NULL | NULL | NULL | 1967647 | Using filesort | | 1 | SIMPLE | o | eq_ref | PRIMARY | PRIMARY | 122 | a.orderid | 1 | NULL | +----+-------------+-------+--------+---------+---------+---------+-----------------+---------+-+ Since is_reply only has two states, 0 and 1, we can rewrite it as follows, reducing the execution time from 1.58 seconds to 2 milliseconds:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 SELECT * FROM ((SELECT * FROM my_order o INNER JOIN my_appraise a ON a.orderid = o.id AND is_reply = 0 ORDER BY appraise_time DESC LIMIT 0, 20) UNION ALL (SELECT * FROM my_order o INNER JOIN my_appraise a ON a.orderid = o.id AND is_reply = 1 ORDER BY appraise_time DESC LIMIT 0, 20)) t ORDER BY is_reply ASC, appraisetime DESC LIMIT 20; EXISTS Statement When dealing with EXISTS clauses, MySQL still uses nested subqueries for execution. Take the following SQL statement as an example:\n1 2 3 4 5 6 7 8 9 10 11 SELECT * FROM my_neighbor n LEFT JOIN my_neighbor_apply sra ON n.id = sra.neighbor_id AND sra.user_id = \u0026#39;xxx\u0026#39; WHERE n.topic_status \u0026lt; 4 AND EXISTS(SELECT 1 FROM message_info m WHERE n.id = m.neighbor_id AND m.inuser = \u0026#39;xxx\u0026#39;) AND n.topic_type \u0026lt;\u0026gt; 5; 1 2 3 4 5 6 7 +----+--------------------+-------+------+-----+------------------------------------------+---------+-------+---------+ -----+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra +----+--------------------+-------+------+ -----+------------------------------------------+---------+-------+---------+ -----+ | 1 | PRIMARY | n | ALL | | NULL | NULL | NULL | 1086041 | Using where | | 1 | PRIMARY | sra | ref | | idx_user_id | 123 | const | 1 | Using where | | 2 | DEPENDENT SUBQUERY | m | ref | | idx_message_info | 122 | const | 1 | Using index condition; Using where | +----+--------------------+-------+------+ -----+------------------------------------------+---------+-------+---------+ -----+ By removing the EXISTS clause and changing it to a JOIN, we can avoid nested subqueries and reduce the execution time from 1.93 seconds to 1 millisecond.\n1 2 3 4 5 6 7 8 9 10 SELECT * FROM my_neighbor n INNER JOIN message_info m ON n.id = m.neighbor_id AND m.inuser = \u0026#39;xxx\u0026#39; LEFT JOIN my_neighbor_apply sra ON n.id = sra.neighbor_id AND sra.user_id = \u0026#39;xxx\u0026#39; WHERE n.topic_status \u0026lt; 4 AND n.topic_type \u0026lt;\u0026gt; 5; The new execution plan is as follows:\n1 2 3 4 5 6 7 +----+-------------+-------+--------+ -----+------------------------------------------+---------+ -----+------+ -----+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+--------+ -----+------------------------------------------+---------+ -----+------+ -----+ | 1 | SIMPLE | m | ref | | idx_message_info | 122 | const | 1 | Using index condition | | 1 | SIMPLE | n | eq_ref | | PRIMARY | 122 | ighbor_id | 1 | Using where | | 1 | SIMPLE | sra | ref | | idx_user_id | 123 | const | 1 | Using where | +----+-------------+-------+--------+ -----+------------------------------------------+---------+ -----+------+ -----+ Condition Pushdown There are cases where external query conditions cannot be pushed down to complex views or subqueries:\nAggregated subqueries Subqueries with LIMIT UNION or UNION ALL subqueries Subqueries in output fields Consider the following statement, where the condition affects the aggregated subquery:\n1 2 3 4 5 6 SELECT * FROM (SELECT target, Count(*) FROM operation GROUP BY target) t WHERE target = \u0026#39;rm-xxxx\u0026#39;; 1 2 3 4 5 6 7 +----+-------------+------------+-------+---------------+-------------+---------+-------+------+-------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+------------+-------+---------------+-------------+---------+-------+------+-------------+ | 1 | PRIMARY | n | ALL | NULL | NULL | NULL | NULL | 1086041 | Using where | | 1 | PRIMARY | sra | ref | NULL | idx_user_id | 123 | const | 1 | Using where | | 2 | DEPENDENT SUBQUERY | m | ref | NULL | idx_message_info | 122 | const | 1 | Using index condition; Using where | +----+-------------+------------+-------+---------------+-------------+---------+-------+------+-------------+ By removing the EXISTS clause and changing it to a JOIN, we can avoid nested subqueries and reduce the execution time from 1.93 seconds to 1 millisecond.\n1 2 3 4 5 6 7 8 9 10 SELECT * FROM my_neighbor n INNER JOIN message_info m ON n.id = m.neighbor_id AND m.inuser = \u0026#39;xxx\u0026#39; LEFT JOIN my_neighbor_apply sra ON n.id = sra.neighbor_id AND sra.user_id = \u0026#39;xxx\u0026#39; WHERE n.topic_status \u0026lt; 4 AND n.topic_type \u0026lt;\u0026gt; 5; The new execution plan is as follows:\n1 2 3 4 5 6 7 +----+-------------+-------+--------+ -----+------------------------------------------+---------+ -----+------+ -----+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+--------+ -----+------------------------------------------+---------+ -----+------+ -----+ | 1 | SIMPLE | m | ref | | idx_message_info | 122 | const | 1 | Using index condition | | 1 | SIMPLE | n | eq_ref | | PRIMARY | 122 | ighbor_id | 1 | Using where | | 1 | SIMPLE | sra | ref | | idx_user_id | 123 | const | 1 | Using where | +----+-------------+-------+--------+ -----+------------------------------------------+---------+ -----+------+ -----+ Narrowing the Scope in Advance Let\u0026rsquo;s take a look at the following partially optimized example (main table in the left join acts as a primary query condition):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 SELECT a.*, c.allocated FROM ( SELECT resourceid FROM my_distribute d WHERE isdelete = 0 AND cusmanagercode = \u0026#39;1234567\u0026#39; ORDER BY salecode limit 20) a LEFT JOIN ( SELECT resourcesid， sum(ifnull(allocation, 0) * 12345) allocated FROM my_resources GROUP BY resourcesid) c ON a.resourceid = c.resourcesid; Does this statement still have other issues? It is clear that subquery c is an aggregate query on the entire table, which can cause performance degradation when dealing with a large number of tables.\nIn fact, for subquery c, the left join result set only cares about the data that can be matched with the primary table\u0026rsquo;s resourceid. Therefore, we can rewrite the statement as follows, reducing the execution time from 2 seconds to 2 milliseconds:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 SELECT a.*, c.allocated FROM ( SELECT resourceid FROM my_distribute d WHERE isdelete = 0 AND cusmanagercode = \u0026#39;1234567\u0026#39; ORDER BY salecode limit 20) a LEFT JOIN ( SELECT resourcesid， sum(ifnull(allocation, 0) * 12345) allocated FROM my_resources r, ( SELECT resourceid FROM my_distribute d WHERE isdelete = 0 AND cusmanagercode = \u0026#39;1234567\u0026#39; ORDER BY salecode limit 20) a WHERE r.resourcesid = a.resourcesid GROUP BY resourcesid) c ON a.resourceid = c.resourcesid; However, the subquery a appears multiple times in our SQL statement. This approach incurs additional costs and makes the statement more complex. We can simplify it using the WITH statement:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 WITH a AS ( SELECT resourceid FROM my_distribute d WHERE isdelete = 0 AND cusmanagercode = \u0026#39;1234567\u0026#39; ORDER BY salecode limit 20) SELECT a.*, c.allocated FROM a LEFT JOIN ( SELECT resourcesid， sum(ifnull(allocation, 0) * 12345) allocated FROM my_resources r, a WHERE r.resourcesid = a.resourcesid GROUP BY resourcesid) c ON a.resourceid = c.resourcesid; Conclusion The database compiler generates execution plans that determine how SQL statements are executed. However, compilers can only do their best to serve, and no database compiler is perfect. The scenarios mentioned above also exist in other databases. Understanding the characteristics of the database compiler allows us to work around its limitations and write high-performance SQL statements.\nWhen designing data models and writing SQL statements, bringing algorithmic thinking or awareness is essential. Developing the habit of using the WITH statement when writing complex SQL statements can simplify them and reduce the burden on the database.\nFinally, here is the execution order of SQL statements:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 FROM \u0026lt;left_table\u0026gt; ON \u0026lt;join_condition\u0026gt; \u0026lt;join_type\u0026gt; JOIN \u0026lt;right_table\u0026gt; WHERE \u0026lt;where_condition\u0026gt; GROUP BY \u0026lt;group_by_list\u0026gt; HAVING \u0026lt;having_condition\u0026gt; SELECT DISTINCT \u0026lt;select_list\u0026gt; ORDER BY \u0026lt;order_by_condition\u0026gt; LIMIT \u0026lt;limit_number\u0026gt; Long Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2024-06-29T22:06:00Z","permalink":"https://huizhou92.com/p/8-common-sql-slow-query-statements-and-how-to-optimize-them/","title":"8 Common SQL Slow Query Statements and How to Optimize Them"},{"content":"As an ordinary person, when you browse the web, you may not realize that the web pages sent to you by the server are actually compressed.\nIf you like a programmer, press F12 in the browser, you\u0026rsquo;ll find something like this:\nIt means: In order to save bandwidth and provide speed, I (the server) compressed the content using gzip, and you (the browser) need to decompress it to view it!\nIn HTTP compression, besides gzip, there are also algorithms like compress, deflate, br, etc., which can be dazzling.\nHowever, all these compression algorithms have an ancestor: LZ algorithm.\nLZ comes from the names of two people: Abraham Lempel and Jacob Ziv.\nBoth of them passed away in 2023, living a long life, with Lempel living to be 86 years old and Ziv living to be 91 years old.\nOrigin Data compression can be divided into two types: lossy compression, such as MP3, JPEG, where some unimportant data is deleted during compression, and lossless compression, where binary bits magically disappear, making files significantly smaller, facilitating storage and transmission.\nIn 1948, after Claude Shannon founded information theory, everyone has been working on one thing: how to find the optimal coding to compress a piece of information.\nShannon and Fano were the first to propose the Shannon-Fano coding.\nIt constructs a binary tree from top to bottom by grouping symbols.\nHowever, this method is not the optimal solution and the encoding is not a prefix code, making it prone to ambiguity.\nLater, while teaching information theory at MIT, Fano challenged his students: either take the final exam or improve existing data compression algorithms.\nA graduate student named Huffman didn\u0026rsquo;t like exams, so he chose the latter path.\nHuffman didn\u0026rsquo;t know that even the famous Shannon struggled with this problem. He researched for several months, developed various methods, but none worked.\nJust as he was about to give up and throw his notes into the trash, a wonderful and elegant algorithm crossed his mind: build a binary tree from bottom to top based on the frequency of characters, which is the famous Huffman algorithm.\nHuffman\u0026rsquo;s algorithm is called \u0026ldquo;optimal coding\u0026rdquo; and achieves two goals:\n(1) No character encoding is a prefix of another character encoding.\n(2) The total length of the information encoding is minimized.\nAlthough the Huffman algorithm is excellent, it has a huge limitation: it requires obtaining the probability of each character appearing first, and then compression encoding can be done, which is often impossible in many cases.\nIn the 1970s, with the emergence of the Internet, this problem became more prominent.\nIs it possible to compress data while reading it?\nBreakthrough Ziv and Lempel from the Technion-Israel Institute of Technology jointly challenged this problem.\nThe two were a good team, with Ziv being good at statistics and information theory, while Lempel excelled in Boolean algebra and computer science.\nIn 1977, they both came to Bell Labs for academic sabbaticals.\nAcademic sabbatical, also known as \u0026ldquo;intellectual leave,\u0026rdquo; gives you a long period of\nleave (like six months) after working for a few years, during which you can do whatever you want, and it\u0026rsquo;s paid.\nThe sabbaticals of the big shots are interesting. For example, Ken Thompson, the inventor of Unix, returned to his alma mater, Berkeley, during his sabbatical and spread Unix there, inspiring Bill Joy and others to develop BSD.\nZiv and Lempel were similar. They went to Bell Labs in the United States for academic sabbaticals and co-authored a paper during their \u0026ldquo;sabbatical\u0026rdquo;: \u0026ldquo;A Universal Algorithm for Sequential Data Compression,\u0026rdquo; proposing an algorithm based on a \u0026ldquo;sliding window,\u0026rdquo; which does not directly consider character frequencies but instead finds repeated data blocks (such as strings, byte sequences, etc.) and references the positions where these data blocks appeared previously.\nThis algorithm is LZ77, which is applicable to any type of data, requires no preprocessing (statistical character appearance probabilities), and achieves extremely high compression ratios with just one pass.\nThe following year, they continued their efforts and improved LZ77 to become LZ78, which could perfectly reconstruct data from compressed data and was more efficient than previous algorithms.\nChaos An invaluable treasure like the LZ algorithm remained in the theoretical realm for several years without widespread use.\nIt wasn\u0026rsquo;t until 1984, when Terry Welch of DEC created the LZW algorithm based on LZ, which was used in Unix\u0026rsquo;s compress program.\nWith the widespread dissemination of Unix, the LZ algorithm began to enter the fast lane of rapid development.\nHowever, it also entered an era of chaotic competition.\nIn 1985, Thom Henderson, while downloading files from BBS, found it tedious to download one by one, as dial-up internet was too slow. So he wrote a software called ARC, which could compress multiple files into one, making it much more convenient.\nIn 1986, Phillip Katz discovered ARC, liked it, but felt that the compression speed was too slow. So he rolled up his sleeves, rewrote the key compression and decompression parts in assembly language, and created PKARC, which he started selling.\nWhen Thom Henderson saw his business being snatched away, he sued Phillip Katz, and the reasons were sufficient: the comments and spelling errors in your PKARC are the same as my ARC, you\u0026rsquo;re plagiarizing! Also, while my ARC is open source, the protocol specifies that you can only view it, not modify it!\nIn the end, ARC won the lawsuit, and Phillip Katz paid tens of thousands of dollars in damages.\nGenius Phillip Katz was naturally not satisfied. He studied the LZ77 algorithm and the Huffman algorithm, combined them, and created a new compression algorithm (deflate) and a new file format (zip), as well as the new software PKZIP.\nPKZIP quickly outperformed ARC in both compression ratio and decompression speed, and quickly dominated the DOS era.\nSince the ZIP format was open, the open-source info-zip group also released the open-source, free unzip and zip, implementing the deflate algorithm.\nLater, Jean-loup Gailly and Mark Adler developed the famous gzip (file format + utility) based on deflate, replacing compress on Unix.\ngzip is the HTTP compression format seen at the beginning of the article.\nIn 1991, Nico Mak felt dissatisfied with the command line of PKZIP, so he developed a front-end for Windows 3.1 based on PKZIP (later replaced by the open-source info-zip), allowing people to compress files using a graphical interface. This is the famous WinZip.\nDespite WinZip\u0026rsquo;s success, it was still \u0026ldquo;parasitic\u0026rdquo; on the Windows platform.\nUsers find that WinZip has an exquisite interface and is user-friendly. There is no need to remember those annoying parameters and compression can be completed with a few clicks of the mouse.\nWinZip quickly took over all PCs and became one of the most popular shareware programs in the 1990s.\nWindows intervened and simply integrated Zip functionality into the operating system, ending everything.\nConclusion From LZ77 to LZW, compress, Deflate, gzip\u0026hellip; Lossless compression algorithms have been continuously patched and gradually formed into a huge family. However, no matter how they change, their principles and ideas are not much different from the original LZ algorithm.\nThese algorithms help us compress images, compress text, compress content transmitted over the Internet, and have become an indispensable part of our daily lives.\nIt\u0026rsquo;s no exaggeration to say that the LZ algorithm and its descendants have dominated the world.\nLong Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2024-06-29T22:04:00Z","image":"https://images.yixiao9206.cn/blog-images/2024/04/f99c3cfdce323ee7f49a9fc7cbf2ffc9.png","permalink":"https://huizhou92.com/p/the-magical-algorithms-written-by-two-old-men-dominating-the-world/","title":"The Magical Algorithms Written by Two Old Men, Dominating the World!"},{"content":" During KubeCon EU 2024, CNCF launched its first Cloud-Native AI Whitepaper. This article provides an in-depth analysis of the content of this whitepaper.\nIn March 2024, during KubeCon EU, the Cloud-Native Computing Foundation (CNCF) released its first detailed whitepaper on Cloud-Native Artificial Intelligence (CNAI) 1. This report extensively explores the current state, challenges, and future development directions of integrating cloud-native technologies with artificial intelligence. This article will delve into the core content of this whitepaper.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nWhat is Cloud-Native AI? Cloud-Native AI refers to building and deploying artificial intelligence applications and workloads using cloud-native technology principles. This includes leveraging microservices, containerization, declarative APIs, and continuous integration/continuous deployment (CI/CD) among other cloud-native technologies to enhance AI applications\u0026rsquo; scalability, reusability, and operability.\nThe following diagram illustrates the architecture of Cloud-Native AI, redrawn based on the whitepaper.\nRelationship between Cloud-Native AI and Cloud-Native Technologies Cloud-native technologies provide a flexible, scalable platform that makes the development and operation of AI applications more efficient. Through containerization and microservices architecture, developers can iterate and deploy AI models quickly while ensuring high availability and scalability of the system. Kuuch as resource scheduling, automatic scaling, and service discovery.\nThe whitepaper provides two examples to illustrate the relationship between Cloud-Native AI and cloud-native technologies, namely running AI on cloud-native infrastructure:\nHugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure2 OpenAI Scaling Kubernetes to 7,500 nodes3 Challenges of Cloud-Native AI Despite providing a solid foundation for AI applications, there are still challenges when integrating AI workloads with cloud-native platforms. These challenges include data preparation complexity, model training resource requirements, and maintaining model security and isolation in multi-tenant environments. Additionally, resource management and scheduling in cloud-native environments are crucial for large-scale AI applications and need further optimization to support efficient model training and inference.\nDevelopment Path of Cloud-Native AI The whitepaper proposes several development paths for Cloud-Native AI, including improving resource scheduling algorithms to better support AI workloads, developing new service mesh technologies to enhance the performance and security of AI applications, and promoting innovation and standardization of Cloud-Native AI technology through open-source projects and community collaboration.\nCloud-Native AI Technology Landscape Cloud-Native AI involves various technologies, ranging from containers and microservices to service mesh and serverless computing. Kubernetes plays a central role in deploying and managing AI applications, while service mesh technologies such as Istio and Envoy provide robust traffic management and security features. Additionally, monitoring tools like Prometheus and Grafana are crucial for maintaining the performance and reliability of AI applications.\nBelow is the Cloud-Native AI landscape diagram provided in the whitepaper.\nKubernetes Volcano Armada Kuberay Nvidia NeMo Yunikorn Kueue Flame Distributed Training Kubeflow Training Operator Pytorch DDP TensorFlow Distributed Open MPI DeepSpeed Megatron Horovod Apla … ML Serving Kserve Seldon VLLM TGT Skypilot … CI/CD — Delivery Kubeflow Pipelines Mlflow TFX BentoML MLRun … Data Science Jupyter Kubeflow Notebooks PyTorch TensorFlow Apache Zeppelin Workload Observability Prometheus Influxdb Grafana Weights and Biases (wandb) OpenTelemetry … AutoML Hyperopt Optuna Kubeflow Katib NNI … Governance \u0026amp; Policy Kyverno Kyverno-JSON OPA/Gatekeeper StackRox Minder … Data Architecture ClickHouse Apache Pinot Apache Druid Cassandra ScyllaDB Hadoop HDFS Apache HBase Presto Trino Apache Spark Apache Flink Kafka Pulsar Fluid Memcached Redis Alluxio Apache Superset … Vector Databases Chroma Weaviate Quadrant Pinecone Extensions Redis Postgres SQL ElasticSearch … Model/LLM Observability • Trulens Langfuse Deepchecks OpenLLMetry … Conclusion Finally, the following key points are summarized:\nRole of Open Source Community: The whitepaper indicates the role of the open-source community in advancing Cloud-Native AI, including accelerating innovation and reducing costs through open-source projects and extensive collaboration. Importance of Cloud-Native Technologies: Cloud-Native AI, built according to cloud-native principles, emphasizes the importance of repeatability and scalability. Cloud-native technologies provide an efficient development and operation environment for AI applications, especially in resource scheduling and service scalability. Existing Challenges: Despite bringing many advantages, Cloud-Native AI still faces challenges in data preparation, model training resource requirements, and model security and isolation. Future Development Directions: The whitepaper proposes development paths including optimizing resource scheduling algorithms to support AI workloads, developing new service mesh technologies to enhance performance and security, and promoting technology innovation and standardization through open-source projects and community collaboration. Key Technological Components: Key technologies involved in Cloud-Native AI include containers, microservices, service mesh, and serverless computing, among others. Kubernetes plays a central role in deploying and managing AI applications, while service mesh technologies like Istio and Envoy provide necessary traffic management and security. For more details, please download the Cloud-Native AI whitepaper 4.\nReference Links Long Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. Whitepaper:\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI Scaling Kubernetes to 7,500 nodes:\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCloud-Native AI Whitepaper: \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-06-29T22:03:00Z","image":"https://images.yixiao9206.cn/blog-images/2024/04/3c8f677dae51c4d491a982224b6a3e0d.png","permalink":"https://huizhou92.com/p/a-deep-dive-into-cncfs-cloud-native-ai-whitepaper/","title":"A Deep Dive into CNCF’s Cloud-Native AI Whitepaper"},{"content":"Last month’s hot topic in IT circles was Google laying off many developers from its Python core team and flutter/dart team, purportedly for a city-wide reorganization.\nhttps://news.ycombinator.com/item?id=40171125\nReportedly, those laid off were mostly core members responsible for important Python maintenance.\nAs a gopher, I pondered: will Google abandon Go? And if so, what would become of Go?\nWhat does Google offer to Go? Based on our past understanding clarified by @Lance Taylor and descriptions from various sources, we can estimate what Go has likely received from Google.\nJob Positions: Details regarding job positions of members of the Go core team, including compensation, benefits, and other remuneration. Software and Hardware Resources: Information on Go-related resources such as intellectual property, servers, domain names, and module management mirrors required by the community. Offline Activities: Possibility of reduced or scaled-down Go conferences worldwide in terms of funding and endorsement. Internal Resources of Big Corporations: Gradual loss of exposure to advanced projects and opportunities for Go’s adoption due to the absence of resources within Google. Promotion and Feedback Channels: Slower discovery and response to significant issues and features in Go as Google’s internal demands historically take precedence. Potential Scenarios What might happen if Google dissolves the Go core team and ceases all infrastructure support?\nDissolution of the Go core team, leading members may retire or seek employment elsewhere. If Google decides to cease all investment in Go, maintenance of Go could become more complex as it relies heavily on infrastructure. In such a scenario, Go might transition from Google to an external foundation, resulting in noticeable maintenance fluctuations. If Google chooses to continue investing in Go through other internal teams, the worst-case scenario could involve Google flexing its ownership of intellectual property, possibly leading to Go being rebranded. CNCF might take over Google’s mantle, organizing the future development of Go. Among CNCF projects, the Go language enjoys the widest adoption. Probability of Occurrence Currently, Go belongs to Google Cloud. Considering Go’s current trend focusing on customer success, the likelihood of Google Cloud shutting down Go is low. But who knows? I consulted gemini on this question.\ngenerated by Gemini\nConclusion Drawing from the example of Rust, which transitioned from Mozilla’s core to an independent foundation, Go could potentially thrive even more. A nonprofit organization will probably form around Go (or it may directly join CNCF), with enough support from major companies, at least for a period.\nReferences https://ajmani.net/2024/02/23/go-2019-2022-becoming-a-cloud-team/ https://www.reddit.com/r/golang/comments/1cft7mc/if_google_decided_to_part_with_the_core_go_team/ ","date":"2024-06-29T22:01:00Z","permalink":"https://huizhou92.com/p/if-google-no-longer-supports-golang/","title":"If Google No Longer supports Golang"},{"content":"Go is a statically typed compiled language designed to be concise and efficient. While Go is not a purely object-oriented language, we can still use design patterns to improve code readability and maintainability. Today, I will introduce a common design pattern: the Decorator pattern.\nWhat is the Decorator Pattern? The Decorator pattern is a design pattern that allows us to dynamically add behavior to an object at runtime without altering its implementation. This is achieved by creating a wrapper or decorator containing the original object and providing an enhanced interface to add new behavior.\nIn Go, we can use functions as decorators because Go supports higher-order functions, which means functions can be passed as parameters and returned as values.\nAn Example To better understand the Decorator pattern, let\u0026rsquo;s see how we can implement it in Go through an example.\nFirst, we define a function type Foo and a decorator type FooDecorator:\n1 2 3 type Foo func(string) string type FooDecorator func(Foo) Foo Then, we can create a decorator that takes a function of type Foo and returns a new function of type Foo which adds some behavior before and after calling the original function:\n1 2 3 4 5 6 7 8 func WithLog(decorated Foo) Foo { return func(s string) string { fmt.Println(\u0026#34;Before calling the decorated function\u0026#34;) result := decorated(s) fmt.Println(\u0026#34;After calling the decorated function\u0026#34;) return result } } Now, we can create a Foo function and enhance it using the decorator:\n1 2 3 4 5 6 7 8 9 10 func main() { foo := func(s string) string { fmt.Println(\u0026#34;Foo function called\u0026#34;) return s } foo = WithLog(foo) foo(\u0026#34;Hello, world!\u0026#34;) } In this example, we create a Foo function and use the WithLog decorator to enhance it. When we call the enhanced function, it first prints a message, then calls the original Foo function, and finally prints another message.\nThis is the Decorator pattern in Go. By using decorators, we can dynamically add new behavior without modifying the original function.\nAn HTTP-related Example Next, let\u0026rsquo;s look at an example related to handling HTTP requests. First, we\u0026rsquo;ll start with a simple HTTP server code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strings\u0026#34; ) func WithServerHeader(h http.HandlerFunc) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { log.Println(\u0026#34;---\u0026gt;WithServerHeader()\u0026#34;) w.Header().Set(\u0026#34;Server\u0026#34;, \u0026#34;HelloServer v0.0.1\u0026#34;) h(w, r) } } func hello(w http.ResponseWriter, r *http.Request) { log.Printf(\u0026#34;Received Request %s from %s\\n\u0026#34;, r.URL.Path, r.RemoteAddr) fmt.Fprintf(w, \u0026#34;Hello, World! \u0026#34;+r.URL.Path) } func main() { http.HandleFunc(\u0026#34;/v1/hello\u0026#34;, WithServerHeader(hello)) err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) if err != nil { log.Fatal(\u0026#34;ListenAndServe: \u0026#34;, err) } } In this code, we use the Decorator pattern. The WithServerHeader() function acts as a decorator that takes an http.HandlerFunc and returns a modified version. This example is relatively simple, as we only add a response header using WithServerHeader(). However, we can create many more functions like this, such as writing authentication cookies, checking authentication cookies, and logging.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strings\u0026#34; ) func WithServerHeader(h http.HandlerFunc) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { log.Println(\u0026#34;---\u0026gt;WithServerHeader()\u0026#34;) w.Header().Set(\u0026#34;Server\u0026#34;, \u0026#34;HelloServer v0.0.1\u0026#34;) h(w, r) } } func WithAuthCookie(h http.HandlerFunc) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { log.Println(\u0026#34;---\u0026gt;WithAuthCookie()\u0026#34;) cookie := \u0026amp;http.Cookie{Name: \u0026#34;Auth\u0026#34;, Value: \u0026#34;Pass\u0026#34;, Path: \u0026#34;/\u0026#34;} http.SetCookie(w, cookie) h(w, r) } } func WithBasicAuth(h http.HandlerFunc) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { log.Println(\u0026#34;---\u0026gt;WithBasicAuth()\u0026#34;) cookie, err := r.Cookie(\u0026#34;Auth\u0026#34;) if err != nil || cookie.Value != \u0026#34;Pass\u0026#34; { w.WriteHeader(http.StatusForbidden) return } h(w, r) } } func WithDebugLog(h http.HandlerFunc) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { log.Println(\u0026#34;---\u0026gt;WithDebugLog\u0026#34;) r.ParseForm() log.Println(r.Form) log.Println(\u0026#34;path\u0026#34;, r.URL.Path) log.Println(\u0026#34;scheme\u0026#34;, r.URL.Scheme) log.Println(r.Form[\u0026#34;url_long\u0026#34;]) for k, v := range r.Form { log.Println(\u0026#34;key:\u0026#34;, k) log.Println(\u0026#34;val:\u0026#34;, strings.Join(v, \u0026#34;\u0026#34;)) } h(w, r) } } func hello(w http.ResponseWriter, r *http.Request) { log.Printf(\u0026#34;Received Request %s from %s\\n\u0026#34;, r.URL.Path, r.RemoteAddr) fmt.Fprintf(w, \u0026#34;Hello, World! \u0026#34;+r.URL.Path) } func main() { http.HandleFunc(\u0026#34;/v1/hello\u0026#34;, WithServerHeader(WithAuthCookie(hello))) http.HandleFunc(\u0026#34;/v2/hello\u0026#34;, WithServerHeader(WithBasicAuth(hello))) http.HandleFunc(\u0026#34;/v3/hello\u0026#34;, WithServerHeader(WithBasicAuth(WithDebugLog(hello)))) err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) if err != nil { log.Fatal(\u0026#34;ListenAndServe: \u0026#34;, err) } } Pipeline of Multiple Decorators When using multiple decorators, the code can become less visually appealing as we need to nest functions layer by layer. However, we can refactor the code to make it cleaner. To do this, we first write a utility function that iterates through and calls each decorator:\n1 2 3 4 5 6 7 8 9 type HttpHandlerDecorator func(http.HandlerFunc) http.HandlerFunc func Handler(h http.HandlerFunc, decors ...HttpHandlerDecorator) http.HandlerFunc { for i := range decors { d := decors[len(decors)-1-i] // iterate in reverse h = d(h) } return h } Then, we can use it like this:\n1 2 http.HandleFunc(\u0026#34;/v4/hello\u0026#34;, Handler(hello, WithServerHeader, WithBasicAuth, WithDebugLog)) Conclusion In this article, I demonstrated the Decorator pattern using two examples. However, since Go does not support annotations as a syntactic sugar, using decorators can be cumbersome. Nevertheless, the concept is still important, and we can apply this thinking to write higher-quality code in our daily development.\nRead More Go Program Pattern 01: Functional Options Pattern Go Program Pattern 02: Implementing Class Inheritance and Method Overriding through Composition Go Program Pattern 03: Inversion of Control Go Program Pattern 04: Map-Reduce Go Program Pattern 05: Decorations ","date":"2024-06-29T21:59:00Z","permalink":"https://huizhou92.com/p/go-program-pattern-05-decorations/","title":"Go Program Pattern 05: Decorations"},{"content":"In the previous article, we used defer to recover from panics. In the practical work of a gopher, defer acts like a loyal and reliable teammate, silently helping us with the clean-up work behind the scenes. For example:\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\n1 2 3 4 5 6 7 8 9 10 wg.Add(goroutines) for i := 0; i \u0026lt; goroutines; i++ { go func() { defer wg.Done() for j := 0; j \u0026lt; count/goroutines; j++ { atomic.AddInt64(\u0026amp;sum, 1) } }() } wg.Wait() defer is used to release locks or any other resources.\nIn Go, defer can only be used inside functions and methods.\nThe defer keyword must be followed by a function or method, which are referred to as deferred functions.\ndefer registers these functions into a stack data structure specific to the goroutine in which it is executed. The deferred functions are then scheduled to be executed in a last-in, first-out (LIFO) order before the function containing the defer statement exits.\nRegardless of whether the function reaches the end of its body and returns, explicitly calls return in an error handling branch, or encounters a panic, the functions stored in the deferred function stack will be scheduled for execution. Thus, deferred functions provide a convenient way to perform clean-up tasks for a function in any scenario.\nSeveral Use Cases for defer Capturing panics: Since deferred functions are always executed in any scenario, we can handle exceptions within defer (although it is not recommended to use panic for general errors unless necessary). Resource release: defer allows for graceful resource release, such as file descriptors or locks. Delayed execution: defer can be used to record the execution time of a function, for example: 1 2 3 go func(s time.Time) { fmt.Println(time.Now().Sub(s)) }(time.Now()) Performance Overhead of defer defer makes resource release (like file descriptors or locks) more elegant and less error-prone. However, in performance-sensitive programs, Gophers must be aware of and consider the performance burden introduced by defer.\nIn the following benchmark test, we can observe the performance difference between a version with defer and a version without defer:\n1 2 3 4 5 6 7 8 9 10 11 12 hxzhouh  atomic  ➜ ( main  1)  ♥ 20:16  go test -bench=BenchmarkFooWithDefer 10000000 goos: darwin goarch: arm64 pkg: github.com/hxzhouh/go-example/atomic BenchmarkFooWithDefer-10 189423524 6.353 ns/op PASS ok github.com/hxzhouh/go-example/atomic 3.631s hxzhouh  atomic  ➜ ( main  1)  ♥ 21:05  go test -bench=BenchmarkFooWithoutDefer BenchmarkFooWithoutDefer-10 273232389 4.397 ns/op PASS ok github.com/hxzhouh/go-example/atomic 2.875s In this test, the non-deferred version is approximately 7 times faster than the version with defer in Go 1.12. After optimization in versions 1.13 and 1.14, the performance of defer has significantly improved. On my computer, the non-deferred version still has a performance advantage of about 50%.\nConclusion In most cases, our programs are not highly sensitive to performance. I recommend using defer whenever possible. However, it is important to understand how defer works, as well as a few things to avoid.\n","date":"2024-06-29T21:58:00Z","permalink":"https://huizhou92.com/p/go-defermakes-the-function-simpler-and-more-robust./","title":"Go defer：makes the function simpler and more robust."},{"content":"Go is not a fully object-oriented language; some object-oriented patterns are unsuitable. However, Go has developed its own set of patterns over the years. Today, I would like to introduce a familiar pattern: the Functional Options Pattern.\nWhat is the Functional Options Pattern? Go does not have constructors like other languages. Instead, it typically uses a New function to act as a constructor. However, when a structure has many fields that need to be initialized, there are multiple ways. One preferred way is to use the Functional Options Pattern.\nThe Functional Options Pattern is a pattern for constructing structs in Go. It involves designing a set of expressive and flexible APIs to help configure and initialize the struct.\nThe Go Language Specification by Uber mentions this pattern:\nFunctional options are a pattern in which you declare an opaque Option type that records information in some internal structure. You accept these variable numbers of options and operate on the complete information recorded by the options on the internal structure.\nUse this pattern for optional parameters in constructors and other public APIs where you expect these parameters to be extended, especially when there are already three or more parameters on these functions.\nAn Example To better understand this pattern, let\u0026rsquo;s walk through an example.\nLet\u0026rsquo;s define a Server struct:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main type Server struct { host string port int } func New(host string, port int) *Server { return \u0026amp;Server{ host, port, } } func (s *Server) Start() error { return nil } How do we use it?\n1 2 3 4 5 6 func main() { svr := New(\u0026#34;localhost\u0026#34;, 1234) if err := svr.Start(); err != nil { log.Fatal(err) } } But what if we want to extend the configuration options for the Server? There are generally three approaches:\nDeclare a new constructor function for each different configuration option. Define a new Config struct to store the configuration information. Use the Functional Options Pattern. Approach 1: Declare a New Constructor Function for Each Different Configuration Option This approach involves defining dedicated constructor functions for different options. Let\u0026rsquo;s say we added two fields to the Server struct:\n1 2 3 4 5 6 type Server struct { host string port int timeout time.Duration maxConn int } Typically, host and port are required fields, while timeout and maxConn are optional. We can keep the original constructor function and assign default values to these two fields:\n1 2 3 4 5 6 7 8 func New(host string, port int) *Server { return \u0026amp;Server{ host, port, time.Minute, 100, } } Then, we can provide two additional constructor functions for timeout and maxConn:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func NewWithTimeout(host string, port int, timeout time.Duration) *Server { return \u0026amp;Server{ host, port, timeout, 100, } } func NewWithTimeoutAndMaxConn(host string, port int, timeout time.Duration, maxConn int) *Server { return \u0026amp;Server{ host, port, timeout, maxConn, } } This approach works well for configurations that are unlikely to change frequently. Otherwise, you would need to create new constructor functions every time you need to add a new configuration. This approach is used in the Go standard library, such as the Dial and DialTimeout functions in the net package:\n1 2 func Dial(network, address string) (Conn, error) func DialTimeout(network, address string, timeout time.Duration) (Conn, error) Approach 2: Use a Dedicated Configuration Struct This approach is also common, especially when there are many configuration options. Typically, you create a Config struct that contains all the configuration options for the Server. This approach allows for easy extension without breaking the API of the Server, even when adding more configuration options in the future.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type Server struct { cfg Config } type Config struct { Host string Port int Timeout time.Duration MaxConn int } func New(cfg Config) *Server { return \u0026amp;Server{ cfg, } } When using this approach, you need to construct a Config instance first, which brings us back to the original problem of configuring the Server. If you modify the fields in Config, you may need to define a constructor function for Config if the fields are changed to private.\nApproach 3: Use the Functional Options Pattern A better solution is to use the Functional Options Pattern.\nIn this pattern, we define an Option function type:\n1 type Option func(*Server) The Option type is a function type that takes a *Server parameter. Then, the constructor function for Server accepts a variable number of Option types as parameters:\n1 2 3 4 5 6 7 func New(options ...Option) *Server { svr := \u0026amp;Server{} for _, f := range options { f(svr) } return svr } How do the options work? We need to define a series of related functions that return Option:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 func WithHost(host string) Option { return func(s *Server) { s.host = host } } func WithPort(port int) Option { return func(s *Server) { s.port = port } } func WithTimeout(timeout time.Duration) Option { return func(s *Server) { s.timeout = timeout } } func WithMaxConn(maxConn int) Option { return func(s *Server) { s.maxConn = maxConn } } To use this pattern, the client code would look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package main import ( \u0026#34;log\u0026#34; \u0026#34;server\u0026#34; ) func main() { svr := New( WithHost(\u0026#34;localhost\u0026#34;), WithPort(8080), WithTimeout(time.Minute), WithMaxConn(120), ) if err := svr.Start(); err != nil { log.Fatal(err) } } Adding new options in the future only requires adding corresponding WithXXX functions.\nThis pattern is widely used in third-party libraries, such as github.com/gocolly/colly:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type Collector struct { // ... } func NewCollector(options ...CollectorOption) *Collector // Defines a series of CollectorOptions type CollectorOption struct { // ... } func AllowURLRevisit() CollectorOption func AllowedDomains(domains ...string) CollectorOption ... However, when Uber\u0026rsquo;s Go Programming Style Guide mentions this pattern, it suggests defining an Option interface instead of an Option function type. This Option interface has an unexported method, and the options are recorded in an unexported options struct.\nCan you understand Uber\u0026rsquo;s example?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 type options struct { cache bool logger *zap.Logger } type Option interface { apply(*options) } type cacheOption bool func (c cacheOption) apply(opts *options) { opts.cache = bool(c) } func WithCache(c bool) Option { return cacheOption(c) } type loggerOption struct { Log *zap.Logger } func (l loggerOption) apply(opts *options) { opts.logger = l.Log } func WithLogger(log *zap.Logger) Option { return loggerOption{Log: log} } // Open creates a connection. func Open( addr string, opts ...Option, ) (*Connection, error) { options := options{ cache: defaultCache, logger: zap.NewNop(), } for _, o := range opts { o.apply(\u0026amp;options) } // ... } Summary In real-world projects, consider using the Functional Options Pattern when dealing with many options or options from different sources (e.g., files or environment variables).\nNote that we should not rigidly apply the pattern described above in actual work. For example, in Uber\u0026rsquo;s example, the Open function does not accept only a variable number of Option parameters because the address parameter is required. Therefore, the Functional Options Pattern is more suitable for cases with many configurations and optional parameters.\nReferences: https://golang.cafe/blog/golang-functional-options-pattern.html https://github.com/uber-go/guide/blob/master/style.md#functional-options Read More Go Program Pattern 01: Functional Options Pattern Go Program Pattern 02: Implementing Class Inheritance and Method Overriding through Composition Go Program Pattern 03: Inversion of Control Go Program Pattern 04: Map-Reduce Go Program Pattern 05: Decorations ","date":"2024-06-29T21:58:00Z","permalink":"https://huizhou92.com/p/go-program-pattern-01-functional-options-pattern/","title":"Go Program Pattern 01: Functional Options Pattern"},{"content":"In the previous article, I briefly introduced the composite pattern in Go, which was explained in a simple manner. We understood that Go can achieve polymorphism in object-oriented programming through composition.\nIn this article, let\u0026rsquo;s learn about Inversion of Control (IoC). Inversion of Control is a software design method that involves separating control logic from business logic. Instead of writing control logic within the business logic, which creates a dependency of control logic on business logic, IoC reverses this relationship and makes the business logic dependent on the control logic.\nInversion of Control Let\u0026rsquo;s consider an example where we want to implement a functionality to record the existence of numbers. We can easily implement the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 type IntSet struct { data map[int]struct{} } func NewIntSet() IntSet { return IntSet{make(map[int]struct{})} } func (set *IntSet) Add(x int) { set.data[x] = struct{}{} } func (set *IntSet) Delete(x int) { delete(set.data, x) } func (set *IntSet) Contains(x int) bool { _, ok := set.data[x] return ok } The above code uses a map to store numbers and provides functionalities for adding, deleting, and checking the existence of numbers. Everything seems perfect.\nNow, suppose we want to add an undo feature to this functionality. How can we do that? With a little thought, we can write clear code by wrapping IntSet into UndoableIntSet. Here\u0026rsquo;s the code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 type UndoableIntSet struct { // Poor style IntSet // Embedding (delegation) functions []func() } func NewUndoableIntSet() UndoableIntSet { return UndoableIntSet{NewIntSet(), nil} } func (set *UndoableIntSet) Add(x int) { // Override if !set.Contains(x) { set.data[x] = true set.functions = append(set.functions, func() { set.Delete(x) }) } else { set.functions = append(set.functions, nil) } } func (set *UndoableIntSet) Delete(x int) { // Override if set.Contains(x) { delete(set.data, x) set.functions = append(set.functions, func() { set.Add(x) }) } else { set.functions = append(set.functions, nil) } } func (set *UndoableIntSet) Undo() error { if len(set.functions) == 0 { return errors.New(\u0026#34;No functions to undo\u0026#34;) } // invert the order of calls index := len(set.functions) - 1 if function := set.functions[index]; function != nil { function() } set.functions = set.functions[:index] return nil } This approach is a good choice for extending existing code with new functionalities. It allows for a balance between reusing the existing code and adding new features. However, the main issue with this approach is that the Undo operation is actually a form of control logic, not business logic. The Undo feature cannot be reused because it contains a lot of business logic related to IntSet.\nDependency Inversion Let\u0026rsquo;s explore another implementation approach where we extract the undo feature and make IntSet depend on it:\n1 2 3 4 5 6 7 8 9 10 11 12 type Undo []func() func (undo *Undo) Add(u func()) { *undo = append(*undo, u) } func (undo *Undo) Undo() { if len(*undo) == 0 { return } index := len(*undo) - 1 (*undo)[index]() *undo = (*undo)[:index] } Next, we embed Undo in IntSet:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 type IntSet struct { data map[int]struct{} undo Undo } func NewIntSet() IntSet { return IntSet{make(map[int]struct{}), make(Undo, 0)} } func (set *IntSet) Undo() { set.undo.Undo() } func (set *IntSet) Add(x int) { if set.Contains(x) { return } else { set.undo.Add(func() { set.Delete(x) }) set.data[x] = struct{}{} } } func (set *IntSet) Delete(x int) { if !set.Contains(x) { return } else { set.undo.Add(func() { set.Add(x) }) delete(set.data, x) } } func (set *IntSet) Contains(x int) bool { _, ok := set.data[x] return ok } In our application, we can use it as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 func main() { set := NewIntSet() set.Add(1) set.Add(2) fmt.Println(set.Contains(2)) set.Undo() fmt.Println(set.Contains(2)) set.Delete(1) fmt.Println(set.Contains(1)) set.Undo() fmt.Println(set.Contains(1)) } Output:\n1 2 3 4 5 /Users/hxzhouh/Library/Caches/JetBrains/GoLand2023.3/tmp/GoLand/___go_build_github_com_hxzhouh_go_example_pattern_ioc true false false true This is Inversion of Control, where the control logic Undo no longer depends on the business logic IntSet, but rather the business logic IntSet depends on Undo. Now, the Undo feature can be easily used by other business logics.\nRead More Go Program Pattern 01: Functional Options Pattern Go Program Pattern 02: Implementing Class Inheritance and Method Overriding through Composition Go Program Pattern 03: Inversion of Control Go Program Pattern 04: Map-Reduce Go Program Pattern 05: Decorations ","date":"2024-06-29T21:58:00Z","permalink":"https://huizhou92.com/p/go-program-pattern-03-inversion-of-control/","title":"Go Program Pattern 03: Inversion of Control"},{"content":"Map-Reduce is a programming paradigm used for processing large-scale datasets. It helps simplify the process of parallel computation and improves computational efficiency.\nFirst, let\u0026rsquo;s understand the concepts of Map and Reduce.\nMap: In the Map phase, the input dataset is divided into a series of key-value pairs, and the same operation is applied to each key-value pair. This operation can be a function or a code block used to process each key-value pair and generate intermediate results. Reduce: In the Reduce phase, the intermediate results generated in the Map phase are combined and processed to obtain the final output result. In the Reduce phase, we can aggregate, summarize, or perform other operations on intermediate results with the same key. The core idea of the Map-Reduce programming paradigm is \u0026ldquo;divide and conquer.\u0026rdquo; It allows us to break down complex computational tasks into multiple independent subtasks, process these subtasks in parallel, and then merge the results to obtain the final result.\nBasic Example Here is a simple example demonstrating the workflow of Map-Reduce:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 func MapFunction(arr []string, fn func(string) string) \u0026lt;-chan string { ch := make(chan string) go func() { for _, v := range arr { ch \u0026lt;- fn(v) } close(ch) }() return ch } func ReduceFunction(ch \u0026lt;-chan string, fn func(string, string) string) string { var res string for v := range ch { res = fn(res, v) } return res } func main() { // generate 10 random strings arr := []string{\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;, \u0026#34;f\u0026#34;, \u0026#34;g\u0026#34;, \u0026#34;h\u0026#34;, \u0026#34;i\u0026#34;} // map ch := MapFunction(arr, func(s string) string { return strings.ToUpper(s) }) // reduce res := ReduceFunction(ch, func(s1, s2 string) string { return s1 + s2 }) fmt.Println(res) } go.dev\nIn this example, we define a MapFunction that takes a string array and converts each element to uppercase using a custom function fn, returning a channel. The ReduceFunction takes a channel and a custom function fn to concatenate the results and print them out.\nThe following image provides a metaphor that vividly illustrates the business semantics of Map-Reduce, which is very useful in data processing.\nYou may understand that Map/Reduce is just a control logic, and the real business logic is defined by the data and the function passed to them. Yes, this is a classic programming pattern of separating \u0026ldquo;business logic\u0026rdquo; from \u0026ldquo;control logic.\u0026rdquo; Now let\u0026rsquo;s take a look at a code example with meaningful business logic to reinforce the understanding of separating \u0026ldquo;control logic\u0026rdquo; and \u0026ldquo;business logic.\u0026rdquo;\nBusiness Example Employee Information\nFirst, we have an employee object and some data:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type Employee struct { Name string Age int Vacation int Salary int } var list = []Employee{ {\u0026#34;Hao\u0026#34;, 44, 0, 8000}, {\u0026#34;Bob\u0026#34;, 34, 10, 5000}, {\u0026#34;Alice\u0026#34;, 23, 5, 9000}, {\u0026#34;Jack\u0026#34;, 26, 0, 4000}, {\u0026#34;Tom\u0026#34;, 48, 9, 7500}, {\u0026#34;Marry\u0026#34;, 29, 0, 6000}, {\u0026#34;Mike\u0026#34;, 32, 8, 4000}, } Related Reduce/Filter Functions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 func EmployeeCountIf(list []Employee, fn func(e *Employee) bool) int { count := 0 for i, _ := range list { if fn(\u0026amp;list[i]) { count += 1 } } return count } func EmployeeFilterIn(list []Employee, fn func(e *Employee) bool) []Employee { var newList []Employee for i, _ := range list { if fn(\u0026amp;list[i]) { newList = append(newList, list[i]) } } return newList } func EmployeeSumIf(list []Employee, fn func(e *Employee) int) int { var sum = 0 for i, _ := range list { sum += fn(\u0026amp;list[i]) } return sum } Here\u0026rsquo;s a brief explanation:\nEmployeeCountIf and EmployeeSumIf are used to count the number of employees or calculate the total based on a certain condition. They represent the semantics of Filter + Reduce. EmployeeFilterIn filters the employees based on a certain condition. It represents the semantics of Filter. Now we can have the following code:\n1) Count the number of employees over 40 years old:\n1 2 3 4 5 old := EmployeeCountIf(list, func(e *Employee) bool { return e.Age \u0026gt; 40 }) fmt.Printf(\u0026#34;Old people: %d\\n\u0026#34;, old) //Old people: 2 2) Count the number of employees with a salary greater than 6000:\n1 2 3 4 5 highPay := EmployeeCountIf(list, func(e *Employee) bool { return e.Salary \u0026gt;= 6000 }) fmt.Printf(\u0026#34;High Salary people: %d\\n\u0026#34;, highPay) //High Salary people: 4 3) List employees who have not taken any vacation:\n1 2 3 4 noVacation := EmployeeFilterIn(list, func(e *Employee) bool { return e.Vacation == 0 }) fmt.Printf(\u0026#34;People with no vacation: %v\\n\u0026#34;, noVacation) The Map-Reduce programming paradigm divides the computational task into Map and Reduce phases. Although writing single-machine code may not be faster than a simple for loop and may appear complex, in the era of cloud-native computing, we can leverage parallel computation and shared data access to improve computational efficiency. It is a powerful tool suitable for handling large-scale data and parallel computing scenarios, such as the original Google PageRank algorithm. The main purpose of learning it is to understand its mindset.\nRead More Go Program Pattern 01: Functional Options Pattern Go Program Pattern 02: Implementing Class Inheritance and Method Overriding through Composition Go Program Pattern 03: Inversion of Control Go Program Pattern 04: Map-Reduce Go Program Pattern 05: Decorations ","date":"2024-06-29T21:58:00Z","image":"https://images.yixiao9206.cn/blog-images/2024/05/3c4159b1f5034b6aa79440e6fcd660a4.png","permalink":"https://huizhou92.com/p/go-program-pattern-04-map-reduce/","title":"Go Program Pattern 04: Map-Reduce"},{"content":"1. Experiment: Which Functions Are Included in the Final Executable? This article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nLet\u0026rsquo;s conduct an experiment to determine which functions are included in the final executable! We\u0026rsquo;ll create a demo1 with the following directory structure and code snippets:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // dead-code-elimination/demo1 $ tree -F . . ├── go.mod ├── main.go └── pkga/ └── pkga.go // main.go package main import ( \u0026#34;fmt\u0026#34; \u0026#34;demo/pkga\u0026#34; ) func main() { result := pkga.Foo() fmt.Println(result) } // pkga/pkga.go package pkga import ( \u0026#34;fmt\u0026#34; ) func Foo() string { return \u0026#34;Hello from Foo!\u0026#34; } func Bar() { fmt.Println(\u0026#34;This is Bar.\u0026#34;) } The example is very simple! The main function calls the exported function Foo from the pkga package, which also contains the Bar function (although it is not called by any other function). Now let\u0026rsquo;s compile this module and examine the functions from the pkga package included in the compiled executable file! (This article uses Go version 1.22.0)\n1 2 $ go build $ go tool nm demo | grep demo Surprisingly, we didn\u0026rsquo;t find any symbol information related to pkga in the output of the executable file. This might be due to Go\u0026rsquo;s optimization. Let\u0026rsquo;s disable the optimization of the Go compiler and try again:\n1 2 3 $ go build -gcflags \u0026#39;-l -N\u0026#39; $ go tool nm demo | grep demo 108ca80 T demo/pkga.Foo After disabling inlining optimization, we can see that pkga.Foo appears in the final executable file demo, but the unused Bar function is not included.\nNow let\u0026rsquo;s look at an example with indirect dependencies:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // dead-code-elimination/demo2 $ tree . . ├── go.mod ├── main.go ├── pkga │ └── pkga.go └── pkgb └── pkgb.go // pkga/pkga.go package pkga import ( \u0026#34;demo/pkgb\u0026#34; \u0026#34;fmt\u0026#34; ) func Foo() string { pkgb.Zoo() return \u0026#34;Hello from Foo!\u0026#34; } func Bar() { fmt.Println(\u0026#34;This is Bar.\u0026#34;) } In this example, we call a new function Zoo from the pkgb package within the pkga.Foo function. Let\u0026rsquo;s compile this new example and see which functions are included in the final executable:\n1 2 3 4 $ go build -gcflags=\u0026#39;-l -N\u0026#39; $ go tool nm demo | grep demo 1093b40 T demo/pkga.Foo 1093aa0 T demo/pkgb.Zoo We can observe that only the functions reachable through the program execution path are included in the final executable!\nIn more complex examples, we can use the go build -ldflags='-dumpdep' command to view the call dependency relationship (using demo2 as an example):\n1 2 3 4 5 6 7 8 9 10 $ go build -ldflags=\u0026#39;-dumpdep\u0026#39; -gcflags=\u0026#39;-l -N\u0026#39; \u0026gt; deps.txt 2\u0026gt;\u0026amp;1 $ grep demo deps.txt # demo main.main -\u0026gt; demo/pkga.Foo demo/pkga.Foo -\u0026gt; demo/pkgb.Zoo demo/pkga.Foo -\u0026gt; go:string.\u0026#34;Hello from Foo!\u0026#34; demo/pkgb.Zoo -\u0026gt; math/rand.Int31n demo/pkgb.Zoo -\u0026gt; demo/pkgb..stmp_0 demo/pkgb..stmp_0 -\u0026gt; go:string.\u0026#34;Zoo in pkgb\u0026#34; From this, we can conclude that Go ensures that only the code that is actually used enters the final executable file, even if some code (such as pkga.Bar) and the code that is actually used (such as pkga.Foo) are in the same package. This mechanism also ensures that the final executable file size remains within a manageable range.\nNext, let\u0026rsquo;s explore this mechanism in Go.\n2. Dead Code Elimination Let\u0026rsquo;s review the build process of go build. The following steps outline the go build command:\nRead go.mod and go.sum: If the current directory contains a go.mod file, go build reads it to determine the project\u0026rsquo;s dependencies. It also verifies the integrity of the dependencies based on checksums in the go.sum file. Calculate the package dependency graph: go build analyzes the import statements in the packages being built and their dependencies to construct a dependency graph. This graph represents the relationships between packages, enabling the compiler to determine the build order of packages. Determine the packages to build: Based on the build cache and the dependency graph, go build determines which packages need to be built. It checks the build cache to see if the compiled packages are up to date. If any package or its dependencies have changed since the last build, go build will rebuild those packages. Invoke the compiler (go tool compile): For each package that needs to be built, go build invokes the Go compiler (go tool compile). The compiler converts the Go source code into machine code specific to the target platform and generates object files (.o files). Invoke the linker (go tool link): After compiling all the necessary packages, go build invokes the Go linker (go tool link). The linker merges the object files generated by the compiler into an executable binary file or a package archive file. It resolves symbols and references between packages, performs necessary relocations, and generates the final output. The entire build process can be represented by the following diagram:\nDuring the build process, go build performs various optimizations, such as dead code elimination and inlining, to improve the performance and reduce the size of the generated binary files. Dead code elimination is an important mechanism that ensures the controllable size of the final executable file in Go.\nThe implementation of the dead code detection algorithm can be found in the $GOROOT/src/cmd/link/internal/ld/deadcode.go file. The algorithm operates by traversing the graph and follows these steps:\nStart from the entry point of the system and mark all symbols reachable through relocations. Relocation represents the dependency relationship between two symbols. By traversing the relocation relationships, the algorithm marks all symbols that can be accessed from the entry point. For example, if the function pkga.Foo is called in the main function main.main, there will be a relocation entry for this function in main.main. After marking is complete, the algorithm marks all unmarked symbols as unreachable and dead code. These unmarked symbols represent the code that cannot be accessed by the entry point or any other reachable symbols. However, there is a special syntax element to note, which is types with methods. Whether the methods of a type are included in the final executable depends on different scenarios. In deadcode.go, the function implementation for marking reachable symbols distinguishes three cases of method invocation for reachable types:\nDirect invocation Invocation through reachable interface types Invocation through reflection: reflect.Value.Method (or MethodByName) or reflect.Type.Method (or MethodByName) In the first case, the invoked method is marked as reachable. In the second case, all reachable interface types are decomposed into method signatures. Each encountered method is compared with the interface method signatures, and if there is a match, it is marked as reachable. This method is conservative but simple and correct.\nIn the third case, the algorithm handles methods by looking for functions marked as REFLECTMETHOD by the compiler. The presence of REFLECTMETHOD on a function F means that F uses reflection for method lookup, but the compiler cannot determine the method name during static analysis. Therefore, all functions that call reflect.Value.Method or reflect.Type.Method are marked as REFLECTMETHOD. Functions that call reflect.Value.MethodByName or reflect.Type.MethodByName with non-constant arguments are also considered REFLECTMETHOD. If a REFLECTMETHOD is found, static analysis is abandoned, and all exported methods of reachable types are marked as reachable.\nHere is an example from the reference material:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // dead-code-elimination/demo3/main.go type X struct{} type Y struct{} func (*X) One() { fmt.Println(\u0026#34;hello 1\u0026#34;) } func (*X) Two() { fmt.Println(\u0026#34;hello 2\u0026#34;) } func (*X) Three() { fmt.Println(\u0026#34;hello 3\u0026#34;) } func (*Y) Four() { fmt.Println(\u0026#34;hello 4\u0026#34;) } func (*Y) Five() { fmt.Println(\u0026#34;hello 5\u0026#34;) } func main() { var name string fmt.Scanf(\u0026#34;%s\u0026#34;, \u0026amp;name) reflect.ValueOf(\u0026amp;X{}).MethodByName(name).Call(nil) var y Y y.Five() } In this example, type *X has three methods, and type *Y has two methods. In the main function, we call the methods of an X instance through reflection and directly call a method of a Y instance. Let\u0026rsquo;s see which methods of X and Y are included in the final executable:\n1 2 3 4 5 6 7 8 9 10 $ go build -gcflags=\u0026#39;-l -N\u0026#39; $ go tool nm ./demo | grep main 11d59c0 D go:main.inittasks 10d4500 T main.(*X).One 10d4640 T main.(*X).Three 10d45a0 T main.(*X).Two 10d46e0 T main.(*Y).Five 10d4780 T main.main ... ... We can observe that only the directly called method Five of the reachable type Y is included in the final executable, while all methods of the reachable type X through reflection are present! This aligns with the third case mentioned earlier.\n3. Summary This article introduced the dead code elimination and executable file size reduction mechanisms in the Go language. Through experiments, we verified that only the functions called on the program execution path are included in the final executable, and unused functions are eliminated.\nThe article explained the Go build process, including package dependency graph calculation, compilation, and linking steps, and highlighted dead code elimination as an important optimization strategy. The specific dead code elimination algorithm is implemented through graph traversal, where reachable symbols are marked and unmarked symbols are considered unused. The article also mentioned the handling of type methods.\nWith this dead code elimination mechanism, Go controls the size of the final executable file, achieving executable file size reduction.\nThe source code mentioned in this article can be downloaded here.\n4. References Getting the most out of Dead Code elimination all: binaries too big and growing aarzilli/whydeadcode ","date":"2024-06-29T21:58:00Z","image":"https://images.yixiao9206.cn/blog-images/2024/06/299696bf3a7221f2f9a17261c7e50101.png","permalink":"https://huizhou92.com/p/how-does-the-go-compiler-reduce-binary-file-size/","title":"How Does the Go Compiler Reduce Binary File Size?"},{"content":"In the previous tutorial, I introduced the fact that the Go language, unlike object-oriented programming languages such as Java and PHP, does not support keywords like class to define classes. Instead, it uses the type keyword combined with basic types or structures to define the type system. Additionally, it does not support explicitly defining inheritance relationships between types using the extends keyword.\nStrictly speaking, Go is not an object-oriented programming language, at least not the best choice (Java is the most established one). However, we can simulate object-oriented programming based on some features provided by Go.\nTo implement object-oriented programming, we must implement the three major features of object-oriented programming: encapsulation, inheritance, and polymorphism.\nInheritance Next is inheritance. Although Go does not directly provide syntax for inheritance, we can indirectly achieve similar functionality through composition. Composition means embedding one type into another type to build a new type structure.\nExplicitly defining inheritance relationships has two drawbacks in traditional object-oriented programming: it leads to increasingly complex class hierarchies and affects class extensibility. Many software design patterns advocate using composition instead of inheritance to improve class extensibility.\nLet\u0026rsquo;s take an example. Suppose we want to create a UI component library. We have a Widget structure type with two properties, x and y, representing the length and width of the component.\nIf we want to define a class representing Label, we can do it like this:\n1 2 3 4 type Label struct { Widget text string } Here, Label inherits all the properties of Widget and adds a new property text. Similarly, we can define the Button and ListBox classes:\n1 2 3 4 5 6 7 8 type Button struct { Label } type ListBox struct { Widget text []string index int } Polymorphism First, we define two interfaces, Painter for painting and Clicker for clicking:\n1 2 3 4 5 6 type Painter interface { Paint() } type Clicker interface { Click() } Then, the components implement these interfaces:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func (label Label) Paint() { // display label fmt.Printf(\u0026#34;%p:Label.Paint(%q)\\n\u0026#34;, \u0026amp;label, label.text) } func (button Button) Paint() { // display button fmt.Printf(\u0026#34;Button.Paint(%q)\\n\u0026#34;, button.text) } func (button Button) Click() { // click button fmt.Printf(\u0026#34;Button.Click(%q)\\n\u0026#34;, button.text) } func (listBox ListBox) Paint() { // display listBox fmt.Printf(\u0026#34;ListBox.Paint(%q)\\n\u0026#34;, listBox.text) } Label implements Painter, and Button and ListBox implement both Painter and Clicker.\nAt the application level, we can use these components like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 label := Label{Widget{10, 10}, \u0026#34;State:\u0026#34;} button1 := Button{Label{Widget{10, 70}, \u0026#34;OK\u0026#34;}} button2 := NewButton(50, 70, \u0026#34;Cancel\u0026#34;) listBox := ListBox{Widget{10, 40}, []string{\u0026#34;AL\u0026#34;, \u0026#34;AK\u0026#34;, \u0026#34;AZ\u0026#34;, \u0026#34;AR\u0026#34;}, 0} for _, painter := range []Painter{label, listBox, button1, button2} { painter.Paint() } fmt.Println(\u0026#34;=========================================\u0026#34;) for _, clicker := range []Clicker{listBox, button1, button2} { clicker.Click() } fmt.Println(\u0026#34;=========================================\u0026#34;) for _, widget := range []interface{}{label, listBox, button1, button2} { widget.(Painter).Paint() if clicker, ok := widget.(Clicker); ok { clicker.Click() } } Go language is different from object-oriented programming languages like Java and PHP in that it does not provide keywords specifically for referencing parent class instances (such as super, parent, etc.). In Go language, the design philosophy is simple, without unnecessary keywords. All calls are straightforward.\nSummary Let\u0026rsquo;s summarize briefly. In Go language, the concept of classes in traditional object-oriented programming is intentionally weakened, which is in line with Go\u0026rsquo;s philosophy of simplicity. The \u0026ldquo;classes\u0026rdquo; defined based on structures are just ordinary data types, similar to built-in data types. Built-in data types can also be transformed into \u0026ldquo;classes\u0026rdquo; that can contain custom member methods using the type keyword.\nAll methods associated with a data type collectively form the method set of that type. Like other object-oriented programming languages, methods within the same method set cannot have the same name. Additionally, if they belong to a structure type, their names cannot overlap with any field names in that type.\nReferences How to pass a \u0026lsquo;child\u0026rsquo; struct into a function accepting \u0026lsquo;parent\u0026rsquo; struct? Check if a struct has struct embedding at run time GO编程模式：委托和反转控制 Read More Go Program Pattern 01: Functional Options Pattern Go Program Pattern 02: Implementing Class Inheritance and Method Overriding through Composition Go Program Pattern 03: Inversion of Control Go Program Pattern 04: Map-Reduce Go Program Pattern 05: Decorations ","date":"2024-06-29T21:57:00Z","permalink":"https://huizhou92.com/p/go-program-pattern-02-implementing-class-inheritance-and-method-overriding-through-composition/","title":"Go Program Pattern 02: Implementing Class Inheritance and Method Overriding through Composition"},{"content":"Go language has three types of pointers. In the normal development process, we only encounter the ordinary pointer. However, in the low-level source code of the Go language, there are a lot of operations involving three types of pointer conversion and manipulation. Let’s clarify these points first.\nIn the C language, pointer are crucial. Although pointers make operations highly flexible and efficient, there are many security risks associated with accessing memory through pointer operations, such as accessing memory out of bounds and compromising the atomicity of types in the type system. Here are some examples of incorrect usage:\n1 2 3 4 5 6 7 8 9 10 // Example 1 int arr[2]; *(arr+2) = 1; // Accessing memory address out of bounds ​ // Example 2 int a = 4; int* ap = \u0026amp;a; // Taking the starting address of variable a (4 bytes) *(short*)ap = 2; // Modifying the first 2 bytes of the 4-byte variable a directly through type casting, thus breaking the atomicity of the int variable ​ // The code in Example 2 may occur in certain scenarios, but it has portability issues on machines with different endianness The reason for these security risks in the C language is that it supports pointer operations and pointer type conversions. Therefore, in Go language, the most commonly used ordinary pointers, which have types, have eliminated pointer arithmetic and type conversion operations to ensure type safety. Here’s an example:\n1 2 3 4 var a int32 = 10 var ap *int32 = \u0026amp;a // Ordinary pointer with type ​ ap++ // Illegal, pointer arithmetic is not allowed p := (*int16)(ap) // Illegal, *int32 cannot be directly converted to *int16 This ensures that pointers always point to valid addresses with allocated memory and preserves type independence and atomicity.\nIn addition to ordinary pointers, Go language also retains two other types of pointers that allow bypassing the type system and achieving the same level of memory manipulation as in C language. The other two types of pointers are:\nunsafe.Pointer uintptr\nTo understand these two, we need to establish a concept: a pointer is essentially a number that stores a memory address. The addressing space is 32 bits for a 32-bit machine and 64 bits for a 64-bit machine, so the size of a pointer is equal to the number of bits in the machine. uintptr is straightforward; it is simply a number that stores a memory address. It is equivalent to uint32 a 32-bit machine and uint64 on a 64-bit machine. Since it is a number, it naturally supports arithmetic operations, which allows it to represent any memory location. However, the problem is that data cannot be operated solely based on its memory address; you also need to know its size. In other words, we cannot manipulate data solely based on a uintptr pointer. On the other hand, an ordinary typed pointer not only provides the address but also informs the compiler about the size of the data pointed to. For example, *int32 and *int64 pointers tell the compiler that they operate on 4-byte and 8-byte data, respectively.\nNow that we have explained ordinary pointers and uintptr pointers in Go language, what is this additional unsafe.Pointer compared to C language?\nunsafe.Pointer is a generic pointer that, like uintptr, only keeps the memory address without concerning itself with the type. However, the difference between unsafe.Pointer and uintptr is that the former refers to an object that will be referenced by the garbage collector (GC), so it will not be collected as garbage by the GC. In contrast, the latter only represents the memory address as a number, which means that if a data address is saved by uintptr, it will be mercilessly collected by the garbage collector.\nSummary of the three types of pointers in Go language:\nOrdinary pointer: This does not support pointer arithmetic, saves the address and type information, and the data it points to will not be garbage collected by the GC. unsafe.Pointer: Does not support pointer arithmetic, saves the address but not the type information, and the data it points to will not be garbage collected by the GC. uintptr: Supports address arithmetic, saves the address but not the type information, and the data it points to will be garbage collected by the GC. In practical usage, uintptr cannot be directly converted to an ordinary pointer, and both must be first converted to unsafe.Pointer as an intermediate step before further conversion.\nHere’s a simple example:\n1 2 3 4 5 6 7 8 type Foo struct{ a int32 b int32 } foo := \u0026amp;Foo{} bp := uintptr(unsafe.Pointer(foo)) + 4 // Add 4 to the address of foo to locate foo.b *(*int32)(unsafe.Pointer(bp)) = 1 // Convert to *int32 ordinary pointer and modify the value fmt.Println(foo.b) // foo.b = 1 ","date":"2024-06-29T21:49:00Z","image":"https://images.yixiao9206.cn/blog-images/2024/05/55dd0bd25be20f4740f49df92d204682.png","permalink":"https://huizhou92.com/p/decrypt-go-understand-the-three-pointers-in-the-go/","title":"Decrypt Go:  Understand the Three Pointer's in the Go"},{"content":"Last time we shared the method of debugging Go code using assembly language. Assembly language allows us to easily trace low-level knowledge of the Go runtime and other underlying details. In this article, we will introduce the powerful debugging tool called “go tool”, which, when mastered, can elevate your development skills to the next level.\nThis article focuses on practical techniques for debugging in Golang and the effective usage of related tools, so you no longer need to worry about how to debug Golang code. Golang, as a modern language, comes with built-in debugging capabilities from the very beginning:\nGolang tools are directly integrated into the language tools, supporting memory analysis, CPU analysis, and blocking lock analysis, among others. Delve and GDB are the most commonly used debug tools, allowing you to dive deeper into program debugging. Delve is currently the most user-friendly Golang debugging program, and IDE debugging is essentially calling dlv, such as in Goland. Unit testing is deeply integrated into the language design, making it very convenient to execute unit tests and generate code coverage. Golang tools Golang integrates a variety of useful tools at the language level, which are the essence of the experience accumulated by Robert Griesemer, Rob Pike, Ken Thompson, and other experts. After installing Golang, you can see all the built-in tools by executing go tool.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 hxzhouh@hxzhouhdeMacBook-Pro ~\u0026gt; go tool addr2line asm buildid cgo compile covdata cover dist distpack doc fix link nm objdump pack pprof test2json trace vet Here, I will focus on selecting several commonly used debug tools:\nnm: view the symbol table (equivalent to the system nm command). objdump: disassembly tool, used to analyze binary files (equivalent to the system objdump command). pprof: metric and performance analysis tool. cover: code coverage generation. trace: sampling over a period of time, metric tracking and analysis tool. compile: code assembly. Now, I will demonstrate the usage of these tools with an example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 package main ​ import ( \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; ) ​ var helloCount int ​ func main() { listener, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:50050\u0026#34;) if err != nil { log.Fatal(err) } defer listener.Close() ​ for { conn, err := listener.Accept() if err != nil { log.Fatal(err) } tcpConn := conn.(*net.TCPConn) err = tcpConn.SetNoDelay(true) if err != nil { log.Println(err) } go handleConnection(conn) } } ​ func handleConnection(conn net.Conn) { defer conn.Close() helloCount++ resp := []byte(\u0026#34;Hello count: \u0026#34;) resp = append(resp, []byte(string(rune(helloCount)))...) resp = append(resp, \u0026#39;\\n\u0026#39;) conn.Write(resp) } // go build main.go nm The nm command is used to view the symbol table, which is equivalent to the system nm command and is very useful. When setting breakpoints, if you don\u0026rsquo;t know the function symbol of the breakpoint, you can use this command to find out (this command operates on binary program files).\ncompile Assemble a specific file:\n1 go tool compile -N -l -S main.go You will be able to see the assembly code corresponding to your Go code (please note that this command operates on Go code text), which is cool.\nobjdump The objdump tool is used to disassemble binaries, equivalent to the system objdump (please note that this command parses binary program files).\n1 2 go tool objdump main.o go tool objdump -s DoFunc main.o // Disassembling specific functions Assembly code may not be needed in 90% of scenarios, but if you have experience working with C programs, in certain special situations, inferring application behavior by disassembling a piece of logic may be your only way out. This is because the code running in production usually has optimization enabled, which can cause your code to not match. Furthermore, you cannot attach to processes at will in production environments. Many times, you only have a core file to troubleshoot.\npprof pprof supports four types of analysis:\nCPU: CPU analysis, sampling calls that consume CPU resources, which is generally used to locate and troubleshoot areas of high computational resource consumption in programs. Memory: Memory analysis, which is generally used to troubleshoot memory usage, memory leaks, and other issues. Block: Blocking analysis, which samples the blocking calls in the program. Mutex: Mutex analysis, which samples the competition for mutex locks. For more information about pprof, you can refer to this article.\ntrace Program trace debugging:\n1 go tool trace -http=\u0026#34;:6060\u0026#34; ./ssd_336959_20190704_105540_trace The trace command allows you to trace and collect information over a period of time, then dump it to a file, and finally analyze the dump file using go tool trace and open it in a web format.\nUnit Testing The importance of unit testing is self-evident. In Golang, files ending with _test.go are considered test files. As a modern language, Golang supports unit testing at the language tool level.\nRunning Unit Tests There are two ways to execute unit tests:\nRun go test directly, which is the simplest method. Compile the test files first, then run them. This method provides more flexibility. Running go test 1 2 3 4 5 6 // Run go test directly in your project directory. go test . // Specify the running function. go test -run=TestPutAndGetKeyValue // Print details. go test -v Compilation and Execution Essentially, running Golang unit tests involves compiling *_test.go files into binaries and then running these binaries. When you execute go test, the tool handles these actions for you, but you can also perform them separately.\nCompile the test files to generate the test executable:\n1 2 3 4 // Compile the .test file first. go test -c -coverpkg=. -covermode=atomic -o 1TestSwapInt32_in_sync_atomic.test sync/atomic // Specify running a file. ./1TestSwapInt32_in_sync_atomic.test -test.timeout=10m0s -test.v=true -test.run=TestSwapInt32 This method is usually used in the following scenarios:\nCompile on one machine and run tests on another. Debugging test programs. Code Coverage Analysis Golang’s code coverage is based on unit tests, which serve as the starting point for measuring code coverage of your business code. The operation is simple:\nAdd the -coverprofile parameter when running tests to record code coverage. Use the go tool cover command to analyze and generate coverage reports. 1 2 go test -coverprofile=coverage.out go tool cover -func=coverage.out The output will be similar to the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 hxzhouh@hxzhouhdeMacBook-Pro ~/w/g/s/sync\u0026gt; go tool cover -func=coverage.out heads/go1.21.4? sync/cond.go:47: NewCond 100.0% sync/cond.go:66: Wait 100.0% sync/cond.go:81: Signal 100.0% sync/cond.go:90: Broadcast 100.0% sync/cond.go:98: check 100.0% sync/cond.go:116: Lock 0.0% sync/cond.go:117: Unlock 0.0% sync/map.go:104: newEntry 100.0% sync/map.go:110: loadReadOnly 100.0% sync/map.go:120: Load 100.0% sync/map.go:145: load 100.0% ....... This way, you can see the code coverage for each function.\nProgram Debugging Program debugging mainly relies on two tools:\n1 2 1. dlv 2. gdb Here, I recommend dlv because GDB’s functionality is limited. GDB does not understand Golang’s specific types such as channels, maps, and slices. GDB’s native support for goroutines is limited since it only understands threads. However, GDB has one irreplaceable feature, which is the gcore command.\ndlv Debugging Debugging Binaries 1 dlv exec \u0026lt;path/to/binary\u0026gt; [flags] For example:\n1 dlv exec ./main Debugging binaries with dlv and passing arguments:\n1 dlv exec ./main -- --audit=./d Debugging Processes 1 dlv attach ${pid} [executable] [flags] The process ID is mandatory. For example:\n1 dlv attach 12808 ./main Debugging Core Files Debugging core files with dlv and redirecting standard output to a file:\n1 dlv core \u0026lt;executable\u0026gt; \u0026lt;core\u0026gt; [flags] 1 dlv core ./main core.277282 Common Debugging Syntax System Summary Program Execution\ncall: call a function (note that this will cause the entire program to run). continue: resume execution. next: step over. restart: restart. step: step into a function. step-instruction: step into a specific assembly instruction. stepout: step out of the current function. Breakpoint-related break (alias: b): set a breakpoint. breakpoints (alias: bp): print all breakpoint information. clear: clear a breakpoint. clearall: clear all breakpoints. condition (alias: cond): set a conditional breakpoint. on: set a command to be executed when a breakpoint is hit. trace (alias: t): set a tracepoint, which is also a breakpoint but does not stop the program when hit; it only prints a line of information. This command is useful in certain scenarios where stopping the program affects logic (e.g., business timeouts), and you only want to print a specific variable. Information Printing args: print function arguments. examinemem (alias: x): a powerful command for examining memory, similar to gdb’s x command. locals: print local variables. print (alias: p): print an expression or variable. regs: print register information. set: set variable value. vars: print global variables (package variables). whatis: print type information. Goroutine-related goroutine (alias: gr): print information of a specific goroutine. goroutines (alias: grs): list all goroutines. thread (alias: tr): switch to a specific thread. threads: print information of all threads. Stack-related deferred: execute commands in the context of a defer function. down: move up the stack. frame: jump to a specific stack frame. stack (alias: bt): print stack information. up: move down the stack. Other Commands config: modify configurations. disassemble (alias: disass): disassemble. edit (alias: ed): omitted. exit (alias: quit | q): omitted. funcs: print all function symbols. libraries: print all loaded dynamic libraries. list (alias: ls | l): display source code. source: load commands. sources: print source code. types: print all type information. The above commands cover the complete set of commands supported by dlv, which meet our debugging needs (some commands are only applicable during development and debugging, as it is not possible to single-step debug on production code in most cases).\nApplication Examples\nPrint Global Variables\n1 (dlv) vars This is very useful for inspecting global variables.\nConditional Breakpoints\n1 2 3 4 5 6 # Set a breakpoint first. (dlv) b # Check breakpoint information. (dlv) bp # Customize the condition. (dlv) condition 2 i==2 \u0026amp;\u0026amp; j==7 \u0026amp;\u0026amp; z==32 Inspecting the Stack\n1 2 3 4 # Show all stacks. (dlv) goroutines # Expand all stacks. (dlv) goroutines -t Examining Memory\n1 (dlv) x -fmt hex -len 20 0xc00008af38 The x command is the same as gdb\u0026rsquo;s x command.\ngdb Debugging GDB’s support for Golang debugging is achieved through a Python script called src/runtime/runtime-gdb.py, so its functionality is limited. GDB can only perform basic variable printing and cannot understand some of Golang\u0026rsquo;s specific types such as channels, maps, and slices. GDB cannot directly debug goroutines because it only understands threads. However, GDB has one feature that cannot be replaced, which is the gcore command.\ndlv Debugging Example Debugging a Binary\n1 dlv exec ./main Debugging a Process\n1 dlv attach 12808 ./main Debugging a Core File\n1 dlv core ./main core.277282 gdb Debugging Example 1 gdb ./main Print Global Variables (note the single quotation marks)\n1 (gdb) p \u0026#39;runtime.firstmoduledata\u0026#39; Due to GDB’s limited understanding of Golang’s type system, sometimes it may not be able to print variables, so please pay attention to this.\nPrint Array Length\n1 (gdb) p $len(xxx) Therefore, I usually only use GDB to generate core files.\nTips and Tricks Don’t know how to set breakpoints in functions? Sometimes you don’t know how to set breakpoints in a function. You can use nm to query the function and then set a breakpoint, which will ensure that you hit the breakpoint.\nDon’t know the calling context? Add the following line in your code:\n1 debug.PrintStack() This will print the stack trace at the current code position, allowing you to understand the calling path of the function.\nDon’t know how to enable pprof? There are two ways to enable pprof, corresponding to two packages:\nnet/http/pprof: used in web server scenarios. runtime/pprof: used in non-server applications. These two packages are essentially the same, with net/http/pprof being a web wrapper on top of runtime/pprof.\nUsing net/http/pprof\n1 import _ \u0026#34;net/http/pprof\u0026#34; Using runtime/pprof\nThis method is usually used for performance optimization. When running a program that is not a server application, you want to find bottlenecks, so you typically use this method.\n1 2 3 4 5 6 7 8 // CPU pprof file path f, err := os.Create(\u0026#34;cpufile.pprof\u0026#34;) if err != nil { log.Fatal(err) } // Start CPU pprof pprof.StartCPUProfile(f) defer pprof.StopCPUProfile() Why does the code sometimes execute unexpectedly during single-step debugging? This situation is usually caused by compiler optimization, such as function inlining and removal of redundant logic and parameters from the compiled binary. This can cause unexpected execution during dlv single-step debugging or prevent the printing of certain variables. The solution to this problem is to disable compiler optimization.\n1 go build -gcflags \u0026#34;-N -l\u0026#34; Conclusion This article provides a systematic overview of the techniques and usage of Golang program debugging:\nThe language tool package provides built-in tools that support assembly, disassembly, pprof analysis, symbol table queries, and other practical functions. The language tool package integrates unit testing, and code coverage relies on triggering unit tests. The powerful dlv/gdb tools serve as the main debugging tools, supporting the analysis of binaries, processes, and core files. ","date":"2024-06-29T21:47:00Z","image":"https://cdn-images-1.medium.com/max/800/0*2Qecdc1lDBmffp1b","permalink":"https://huizhou92.com/p/advanced-debugging-tips-for-the-go-language/","title":"Advanced Debugging Tips for the Go Language"},{"content":"gRPC generally avoids defining errors within messages. After all, each gRPC service inherently comes with an error return value, serving as a dedicated channel for error transmission. All error returns in gRPC should either be nil or an error generated by status.Status. This ensures that errors can be directly recognized by the calling Client.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\n1. Basic Usage Simply returning upon encountering a Go error won’t be recognizable by downstream clients. The proper approach is:\nCall the status.New method and pass an appropriate error code to generate a status.Status object. Call the status.Err method to generate an error recognizable by the calling party, then return. 1 2 st := status.New(codes.NotFound, \u0026#34;some description\u0026#34;) err := st.Err() The error code passed in is of type codes.Code. Alternatively, you can use status.Error for a more convenient method that eliminates manual conversion.\n1 err := status.Error(codes.NotFound, \u0026#34;some description\u0026#34;) 2. Advanced Usage The aforementioned errors have a limitation: the error codes defined by code.Code only cover certain scenarios and cannot comprehensively express the error scenarios encountered in business.\ngRPC provides a mechanism to supplement information within errors: the status.WithDetails method.\nClients can directly retrieve the contents by converting the error back to status.Status and using the status.Details method.\nstatus.Details returns a slice, which is a slice of interface{}. However, Go automatically performs type conversion, allowing direct usage through assertion.\nServer-Side Example Generate a status.Status object Populate additional error information 1 2 3 4 5 6 7 8 9 10 11 12 func ErrorWithDetails() error { st := status.Newf(codes.Internal, fmt.Sprintf(\u0026#34;something went wrong: %v\u0026#34;, \u0026#34;api.Getter\u0026#34;)) v := \u0026amp;errdetails.PreconditionFailure_Violation{ //errDetails Type: \u0026#34;test\u0026#34;, Subject: \u0026#34;12\u0026#34;, Description: \u0026#34;32\u0026#34;, } br := \u0026amp;errdetails.PreconditionFailure{} br.Violations = append(br.Violations, v) st, _ = st.WithDetails(br) return st.Err() } Client-Side Example Parse error information after RPC error Retrieve error details directly through assertion 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 resp, err := odinApp.CreatePlan(cli.StaffId.AssetId, gentRatePlanMeta(cli.StaffId)) ​ if status.Code(err) != codes.InvalidArgument { logger.Error(\u0026#34;create plan error:%v\u0026#34;, err) } else { for _, d := range status.Convert(err).Details() { // switch info := d.(type) { case *errdetails.QuotaFailure: logger.Info(\u0026#34;Quota failure: %s\u0026#34;, info) case *errdetails.PreconditionFailure: detail := d.(*errdetails.PreconditionFailure).Violations for _, v1 := range detail { logger.Info(fmt.Sprintf(\u0026#34;details: %+v\u0026#34;, v1)) } case *errdetails.ResourceInfo: logger.Info(\u0026#34;ResourceInfo: %s\u0026#34;, info) case *errdetails.BadRequest: logger.Info(\u0026#34;ResourceInfo: %s\u0026#34;, info) default: logger.Info(\u0026#34;Unexpected type: %s\u0026#34;, info) } } } logger.Infof(\u0026#34;create plan success,resp=%v\u0026#34;, resp) Principles How are these errors passed to the calling Client? They are placed in metadata, then in the HTTP header. Metadata is in the format of key-value pairs. In error transmission, the key is a fixed value: grpc-status-details-bin. The value is encoded by proto and is binary-safe. Most languages have implemented this mechanism.\nNote gRPC imposes restrictions on response headers, with a limit of 8K, so errors should not be too large.\nReference: Protocol Buffers Tutorial errdetails ","date":"2024-06-29T21:38:00Z","image":"https://cdn-images-1.medium.com/max/800/0*XqGwf5DLyzRkMj69","permalink":"https://huizhou92.com/p/go-action-error-handling-in-grpc/","title":"Go Action: Error Handling In gRPC"},{"content":"Our post-analysis has uncovered a critical issue: credential leakage is the most prevalent cause of hacker attacks. Hackers exploit these credentials to gain a foothold on users\u0026rsquo; endpoint devices, such as desktops, laptops, or mobile devices. These endpoint devices, serving as the primary gateway to enterprise networks, are frequently targeted by attackers for illicit purposes. Alarmingly, a Ponemon survey found that 68% of organizations have experienced at least one successful endpoint attack in the past year, underscoring the urgent need for enhanced endpoint security measures.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nOn the other hand, IT teams heavily rely on specialized software for endpoint protection. However, endpoint devices, both online and offline, have become the preferred tools for almost all employees in their work. Therefore, endpoint management is a fundamental component of IT teams\u0026rsquo; enterprise infrastructure strategy. However, the rapid shift to online work methods, regardless of geographical location, has posed urgent and significant challenges for organizations still relying on more traditional approaches.\nWhen establishing cross-endpoint visibility and security controls, both IT and security teams need to understand that each endpoint should bear some, if not full, responsibility for its own security. Unlike traditional network security approaches, the security measures implemented in this case apply to the entire network rather than just individual devices or servers. Therefore, ensuring resilience in all endpoints is crucial for successfully implementing defense strategies.\nThe State of Endpoint Management Organizations should establish a simple form of endpoint management and security management system, such as unified endpoint management, antivirus software, or anti-malware programs, at least across their device clusters. Many organizations go beyond these basic measures and utilize endpoint security encryption technologies, intrusion detection, and behavior-blocking elements to identify and prevent threats or risky behaviors from endpoint users or intruders. However, as the total number of devices in enterprises and the average number of applications installed on these systems rapidly increase, IT and security teams face increasingly complex tasks. Consequently, managers often find that endpoint management consumes excessive time and hinders their focus on other strategic priorities.\nAt the same time, users expect to have a good and consistent endpoint experience regardless of their location. Their main concern is that their technology works effectively if they have continuous access to the reliable resources they need. This means that when users work remotely, IT teams need higher levels of visibility to ensure consistent experiences across different locations.\nDriving Trends for the Future of Endpoint Management To address these challenges, experts in the industry predict that future cross-endpoint management tools will undergo further simplification and modernization. Three distinct trends explain the specific requirements from the new era of remote work:\nIntegration of local endpoint security: More organizations now understand that endpoint security and access security go hand in hand, and establishing resilient zero-trust principles from endpoints to network boundaries is crucial for modern security posture. 51% of organizations have encountered attackers accessing company data by targeting endpoints. Therefore, software-defined boundaries or zero-trust network access (ZTNA) methods no longer rely on contextual factors such as time or geography when granting user access. They also consider device configurations and the security posture of the devices. In the future, most IT and security practitioners will desire a unified endpoint and secure access platform to consolidate visibility and control for addressing risks in cross-endpoint access, applications, and networks.\nResilient, self-healing, and trusted systems: Ensuring resilience in every endpoint is crucial for successfully implementing defense strategies. Self-healing cybersecurity systems enhance security and IT productivity while simplifying the management and protection of today\u0026rsquo;s highly distributed infrastructure. Malicious actors often exploit human errors to disable existing security measures on enterprise devices.\nSelf-healing solutions can monitor critical applications to counter these threats to prevent tampering or degradation. Automation can be used to repair or reinstall problematic or lost applications. However, different self-healing cybersecurity systems are established in different ways.\nSelf-healing can exist at three levels: application, operating system, and firmware. According to Forrester Research, firmware-based tools embedded within devices are the most important, as they ensure the proper functioning of everything on the device (e.g., endpoint agents, VPNs, and various software). This technology remains intact even if administrators reconfigure the endpoint or replace the hard drive. In the future, organizations should prioritize vendors that offer persistent and self-healing capabilities when making final purchasing decisions.\nFocus on consistent user endpoint experience: In addition to IT manageability and core security aspects, organizations must focus on the remote workers themselves, ensuring they have insights and visibility from endpoints to network edges that impact the end-user experience. This includes addressing device issues (e.g., outdated OS systems, disk drive capacity), home office Wi-Fi and network issues, VPN tunnel performance issues, and application-related problems while enabling IT to quickly identify and rectify the root causes. On the other hand, many organizations have shifted to using Digital Experience Monitoring (DEM) tools to help identify technical performance issues and adjust application performance to achieve business objectives. In the future, leading endpoint management tools will incorporate the collection of endpoint user experience telemetry and analysis into their products.\nCurrently, the modernization process of endpoint management strategies is progressing smoothly. Combining the aforementioned trends will provide employees with a better digital experience and help organizations improve operational efficiency while reducing the attack surface.\n","date":"2024-06-29T21:17:49+08:00","permalink":"https://huizhou92.com/p/the-future-of-edr/","title":"The Future of EDR"},{"content":"After writing tons of code and implementing hundreds of interfaces, you finally successfully managed to deploy your application. However, you soon discover that the performance could be better. What a nightmare!\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nThe Need for Performance Analysis Introducing PProf To optimize performance, the first thing to focus on is the toolchain provided by Go itself. In this article, we will explore and utilize the powerful features of Go\u0026rsquo;s performance profiling tool, PProf. It covers the following areas:\nruntime/pprof: Collects runtime data of non-server programs for analysis net/http/pprof: Collects runtime data of HTTP servers for analysis What is the pprof? pprof is a tool used to visualize and analyze performance profiling data. It reads a collection of analysis samples in the profile.proto format and generates reports to visualize and analyze the data (supports both text and graphical reports).\nThe profile.proto file is a Protocol Buffer v3 descriptor file that describes a set of call stacks and symbolization information. It represents a set of sampled call stacks for statistical analysis and is a common format for stack trace configuration files.\nSupported Usage Modes Report generation: Generates reports Interactive terminal use: Supports interactive terminal-based usage Web interface: Provides a web-based interface What Can You Do with pprof? CPU Profiling Collects CPU (including registers) usage of the monitored application at a certain frequency. It helps identify the time the application spends actively consuming CPU cycles. Memory Profiling: Records stack traces when heap allocations occur in the application. It monitors current and historical memory usage and helps detect memory leaks. Block Profiling: Records the locations where goroutines block and wait for synchronization (including timer channels). Mutex Profiling: Reports the competition status of mutexes. A Simple Example Let\u0026rsquo;s start with a simple example that has some performance issues. This will serve as a basic demonstration of program analysis.\nWriting the Demo Files Create a file named demo.go with the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; _ \u0026#34;net/http/pprof\u0026#34; \u0026#34;github.com/EDDYCJY/go-pprof-example/data\u0026#34; ) func main() { go func() { for { log.Println(data.Add(\u0026#34;https://github.com/EDDYCJY\u0026#34;)) } }() http.ListenAndServe(\u0026#34;0.0.0.0:6060\u0026#34;, nil) } Create a file named data/d.go with the following content: 1 2 3 4 5 6 7 8 9 10 11 package data var datas []string func Add(str string) string { data := []byte(str) sData := string(data) datas = append(datas, sData) return sData } When you run this file, your HTTP server will have an additional endpoint /debug/pprof for observing the application\u0026rsquo;s status.\nAnalysis 1. Using the Web Interface To view the current overview, visit http://127.0.0.1:6060/debug/pprof/.\n1 2 3 4 5 6 7 8 9 /debug/pprof/ profiles: 0 block 5 goroutine 3 heap 0 mutex 9 threadcreate full goroutine stack dump This page contains several subpages. Let\u0026rsquo;s dive deeper to see what we can find:\ncpu (CPU Profiling): $HOST/debug/pprof/profile. This performs CPU profiling for 30 seconds by default and generates a profile file for analysis. block (Block Profiling): $HOST/debug/pprof/block. This shows the stack traces causing blocking synchronization. goroutine: $HOST/debug/pprof/goroutine. This displays the stack traces of all currently running goroutines. heap (Memory Profiling): $HOST/debug/pprof/heap. This shows the memory allocation of active objects. mutex (Mutex Profiling): $HOST/debug/pprof/mutex. This displays the stack traces of mutex contention. 2. Using the Interactive Terminal Execute the following command: go tool pprof http://localhost:6060/debug/pprof/profile?seconds=60. 1 2 3 4 5 6 7 $ go tool pprof http://localhost:6060/debug/pprof/profile\\?seconds\\=60 Fetching profile over HTTP from http://localhost:6060/debug/pprof/profile?seconds=60 Saved profile in /Users/eddycjy/pprof/pprof.samples.cpu.007.pb.gz Type: cpu Duration: 1mins, Total samples = 26.55s (44.15%) Entering interactive mode (type \u0026#34;help\u0026#34; for commands, \u0026#34;o\u0026#34; for options) (pprof) After executing this command, wait for 60 seconds (you can adjust the value of seconds). PProf will perform CPU profiling during this time. Once finished, it will enter the interactive command mode, allowing you to view or export the analysis results. For a list of available commands, type pprof help.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 (pprof) top10 Showing nodes accounting for 25.92s, 97.63% of 26.55s total Dropped 85 nodes (cum \u0026lt;= 0.13s) Showing top 10 nodes out of 21 flat flat% sum% cum cum% 23.28s 87.68% 87.68% 23.29s 87.72% syscall.Syscall 0.77s 2.90% 90.58% 0.77s 2.90% runtime.memmove 0.58s 2.18% 92.77% 0.58s 2.18% runtime.freedefer 0.53s 2.00% 94.76% 1.42s 5.35% runtime.scanobject 0.36s 1.36% 96.12% 0.39s 1.47% runtime.heapBitsForObject 0.35s 1.32% 97.44% 0.45s 1.69% runtime.greyobject 0.02s 0.075% 97.51% 24.96s 94.01% main.main.func1 0.01s 0.038% 97.55% 23.91s 90.06% os.(*File).Write 0.01s 0.038% 97.59% 0.19s 0.72% runtime.mallocgc 0.01s 0.038% 97.63% 23.30s 87.76% syscall.Write flat: The time spent in a given function. flat%: The percentage of CPU time spent in a given function. sum%: The cumulative percentage of CPU time spent in a given function and its callees. cum: The total time spent in a function and its callees. cum%: The cumulative percentage of CPU time spent in a given function and its callees. The last column represents the function names. In most cases, these five columns provide insights into the application\u0026rsquo;s runtime behavior, helping you optimize it. 🤔\n2. Execute the following command: go tool pprof http://localhost:6060/debug/pprof/heap.\n1 2 3 4 5 6 7 8 9 $ go tool pprof http://localhost:6060/debug/pprof/heap Fetching profile over HTTP from http://localhost:6060/debug/pprof/heap Saved profile in /Users/eddycjy/pprof/pprof.alloc_objects.alloc_space.inuse_objects.inuse_space.008.pb.gz Type: inuse_space Entering interactive mode (type \u0026#34;help\u0026#34; for commands, \u0026#34;o\u0026#34; for options) (pprof) top Showing nodes accounting for 837.48MB, 100% of 837.48MB total flat flat% sum% cum cum% 837.48MB 100% 100% 837.48MB 100% main.main.func1 -inuse_space: Analyzes the resident memory usage of the application. -alloc_objects: Analyzes the temporary memory allocations of the application. Execute the following command: go tool pprof http://localhost:6060/debug/pprof/block. Execute the following command: go tool pprof http://localhost:6060/debug/pprof/mutex. 3. PProf Visualization Interface This is the exciting part! But before we proceed, we need to write a simple test case to run.\nWriting the Test Case Create a file named data/d_test.go with the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package data import \u0026#34;testing\u0026#34; const url = \u0026#34;https://github.com/EDDYCJY\u0026#34; func TestAdd(t *testing.T) { s := Add(url) if s == \u0026#34;\u0026#34; { t.Errorf(\u0026#34;Test.Add error!\u0026#34;) } } func BenchmarkAdd(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { Add(url) } } Run the test case: 1 2 3 4 5 $ go test -bench=. -cpuprofile=cpu.prof pkg: github.com/EDDYCJY/go-pprof-example/data BenchmarkAdd-4 10000000 187 ns/op PASS ok github.com/EDDYCJY/go-pprof-example/data 2.300s You can also explore -memprofile.\nLaunching the PProf Visualization Interface Method 1: 1 $ go tool pprof -http=:8080 cpu.prof Method 2: 1 2 $ go tool pprof cpu.prof $ (pprof) web If you encounter the message \u0026ldquo;Could not execute dot; may need to install graphviz,\u0026rdquo; it means you need to install graphviz (please consult your favorite search engine).\nViewing the PProf Visualization Interface When you open the PProf visualization interface, you will notice that it is more refined than the official toolchain\u0026rsquo;s PProf. Additionally, it includes a Flame Graph.\nThe Flame Graph is the highlight of this section. It is a dynamic visualization where the call sequence is represented from top to bottom (A -\u0026gt; B -\u0026gt; C -\u0026gt; D). Each block represents a function, and the larger the block, the more CPU time it consumes. It also supports drill-down analysis by clicking on the blocks!\nConclusion In this article, we provided a brief introduction to PProf, the performance profiling tool in Go. PProf is very helpful in locating and analyzing performance issues in specific scenarios.\nWe hope this article has been helpful to you. We encourage you to try it out yourself and delve deeper into the various features and knowledge points it offers.\nThought Questions Congratulations on making it to the end! Here are two simple thought questions to expand your thinking:\nIs flat always greater than cum? Why? In what scenarios would cum be greater than flat? What performance issues can you identify in the demo code provided in this article? How would you address them? Now it\u0026rsquo;s your turn to share your thoughts!\n","date":"2024-06-29T20:51:41+08:00","image":"https://images.yixiao9206.cn/blog-images/2024/05/acdc34e1b74e3daad61599c01f949411.png","permalink":"https://huizhou92.com/p/performance-profiling-of-golangs-biggest-killers-pprof/","title":"Performance Profiling of Golang's Biggest Killers: PProf"},{"content":"Channels Let me get straight to the point and share the channels I rely on to gather information:\nRSS Podcasts Newsletters YouTube Books Research papers This article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nText RSS is my go-to channel for information. I use Reeder 5 to subscribe to RSS feeds locally. As soon as I arrive at the office, the first thing I do is open the software and browse through the newly added articles. If I come across any useful ones, I tag them and save them for later when I have some free time. I try to keep the number of unread articles to a minimum, as an overwhelming backlog might lead to a loss of interest in using Reeder 5.\nStarting in 2023, I began exploring newsletters as a way to stay informed. Initially, I was hesitant to clutter my email inbox (I only have a personal and a work email) with various newsletters. However, I stumbled upon omnivore, an absolute gem of an application. It allows me to subscribe to articles and emails, import books, and make annotations. I can even export these annotations to Obsidian, which perfectly aligns with my workflow. At times, I find myself storing and annotating content from RSS feeds in omnivore.\nNewsletters come in two types. The first type is the weekly kind, where others curate and provide brief comments. I often discover interesting articles in these newsletters and add them to my RSS list. Reading them feels light and enjoyable. The second type consists of meticulously crafted articles by authors, offering in-depth and rich content. Reading such articles requires setting aside dedicated time as they can be quite extensive.\nBoth email and RSS are considered \u0026ldquo;ancient\u0026rdquo; internet protocols. However, the longer I use them (or perhaps as I grow older), the more I appreciate their stability, longevity, and the rich ecosystem they provide.\nBooks and research papers have been a constant in my life since my student days. Every year, I make it a point to read a substantial number of physical books. Newsletters offer a different experience, even compared to e-books. I find it hard to describe the feeling, but I believe that even in this digital age, we should not forget the value of physical books.\nPodcasts Podcasts have become an integral part of my daily routine, particularly during my commute to work. I have developed a fondness for finance and history podcasts. Auditory information consumption is faster for me than visual information, although podcasts don\u0026rsquo;t offer the same information density as text. Instead, they provide a comforting presence during commutes, workouts, or when engaging in mechanical tasks—a voice that often brings a smile to my face.\nVideos More and more people are recognizing YouTube as the ultimate university. Nowadays, I often turn to YouTube for search queries rather than Google, especially when I\u0026rsquo;m seeking practical knowledge. Finding a video tutorial allows me to quickly grasp concepts without the complexity of reading lengthy instruction manuals. YouTube hosts a vast array of knowledge-based videos, with many creators specializing in their respective fields. For instance, the technology videos I frequently watch are always up to date. Whenever a new technology emerges, particularly in the realm of front-end development, YouTube promptly provides videos for trial, evaluation, troubleshooting, and tutorials. If I wish to learn computer fundamentals, there are content creators who explain them in a comprehensive yet accessible manner. Personally, I find that absorbing knowledge from YouTube is faster than reading books. However, I tend to avoid investment-related videos on the platform.\nSummary To summarize, the software tools I rely on include:\nReeder 5: A highly efficient local RSS client. Omnivore: Obsidian: A note-taking software. Apple Podcasts: YouTube I hope that each person can discover their own preferred methods of information acquisition.\n","date":"2024-06-29T20:49:29+08:00","permalink":"https://huizhou92.com/p/information-channels-i-use-2024/","title":"Information Channels  I Use -2024"},{"content":"Today, I will introduce some commonly used commands and tools for viewing Go assembly code and debugging Go programs. These tools can be used in regular situations when engaging with colleagues or in online discussions, allowing you to have an upper hand in critical moments.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nFor example, if a colleague claims that the first piece of code is more efficient than the second one:\n1 2 3 4 5 6 7 8 9 10 package main type Student struct { Class int } func main() { var a = \u0026amp;Student{1} println(a) } 1 2 3 4 5 6 7 8 9 10 package main type Student struct { Class int } func main() { var a = Student{1} var b = \u0026amp;a println(b) } and they explained it in such a way that you couldn\u0026rsquo;t win the argument. What should you do? Just use a single command to generate the assembly code and expose their argument, giving them a reality check.\nGenerating Assembly Code with Go Tool In fact, it\u0026rsquo;s quite simple. There are two commands that can achieve this:\n1 go tool compile -S main.go or:\n1 go build main.go \u0026amp;\u0026amp; go tool objdump ./main The first one is for compilation, which means compiling the source code into an .o object file and outputting the corresponding assembly code.\nThe second one is for disassembling, which means decompiling the executable file into assembly code. Therefore, you need to compile the code into an executable file using the go build command first.\nAlthough these two commands are not identical, they both reveal that the assembly code generated for the two example code snippets is the same. Your colleague\u0026rsquo;s \u0026ldquo;claims\u0026rdquo; falls apart, and you\u0026rsquo;ve got them cornered.\nLocating the Runtime Source Code Go is a language with a runtime. What is the runtime? It\u0026rsquo;s essentially a set of auxiliary programs that the user didn\u0026rsquo;t write. The runtime writes code for us, such as the Go scheduler.\nAll we need to know is that we can create goroutines using the go keyword, and then we can pile up our business logic. As for how goroutines are scheduled, we don\u0026rsquo;t need to worry about it at all because that\u0026rsquo;s the job of the runtime scheduler.\nBut how can we correlate our code with the code inside the runtime?\nThe methods mentioned earlier can achieve this by adding a grep command.\nFor example, if I want to know which function in the runtime corresponds to the go keyword, I can write a test code snippet:\n1 2 3 4 5 6 7 package main func main() { go func() { println(1 + 2) }() } Since the line go func() { }() is on line 4, we can add a condition when using grep:\n1 2 3 4 5 go tool compile -S main.go | grep \u0026#34;main.go:4\u0026#34; // or go build main.go \u0026amp;\u0026amp; go tool objdump ./main | grep \u0026#34;main.go:4\u0026#34; By analyzing the code, we can immediately see that the line go func(){} corresponds to the newproc() function. By further studying the newproc() function, we can gain a better understanding of how goroutines are created.\nDebugging Code with Dlv Some may ask, \u0026ldquo;Are there any other methods or tools available for debugging Go programs and interacting with them?\u0026rdquo; The answer is yes! That\u0026rsquo;s where the dlv debugging tool comes in. It currently provides the best support for debugging Go programs.\nPreviously, I hadn\u0026rsquo;t really explored it in-depth and only knew some very basic commands. However, this time I have learned a few advanced commands, which have significantly enhanced my understanding of Go.\nLet\u0026rsquo;s demonstrate how to use dlv with a specific task.\nWe know that appending elements to a nil slice will not cause any problems. However, if we try to insert new elements into a nil map, it will immediately panic. Why does this happen and where exactly does the panic occur?\nFirst, let\u0026rsquo;s write a sample program that triggers a panic in a map:\n1 2 3 4 5 6 package main func main() { var m map[int]int m[1] = 1 } Next, compile the program and generate an executable file using the go build command:\n1 go build main.go Then, enter the debugging mode using dlv:\n1 dlv exec ./main To set a breakpoint, we can use the b command in three different ways:\nb + address b + line number b + function name Let\u0026rsquo;s set a breakpoint at the line where the map assignment occurs, which is line 5. We\u0026rsquo;ll add a breakpoint at that line:\n1 2 3 (dlv) b main.go:5 Breakpoint 1 set at 0x104203070 for main.main() ./main.go:5 (dlv) Use the c command to directly run until the breakpoint is reached. Then, execute the disass command to see the assembly instructions:\nNext, use the si command to execute a single instruction. Repeat the si command to execute until the mapassign_fast64 function is reached:\nThen, let\u0026rsquo;s set a breakpoint in the map_fast64.go file:\n1 b /opt/homebrew/Cellar/go/1.21.6/libexec/src/runtime/map_fast64.go:93 Now, by using the s command, we can step into the branch where h is checked for nil and then the panic function is executed:\nAt this point, we have found the code that triggers a panic when assigning to a nil map. From here, we can follow the graph and locate the corresponding position in the runtime source code for further exploration.\nIn addition, we can use the bt command to view the call stack:\nUsing the frame 1 command, we can jump to the corresponding position. In this case, 1 corresponds to the main.go:5 line where we set the breakpoint. Isn\u0026rsquo;t it fascinating?\nIn the graph above, we can also clearly see that the user goroutine is called all the way by the goexit function. When the user goroutine completes, it goes back to the goexit function to do some finalization work. However, that\u0026rsquo;s beyond the scope of our discussion.\nAdditionally, dlv can also help us with the second part, \u0026ldquo;Locating the Runtime Source Code.\u0026rdquo;\nSummary Today, I provided systematic methods for viewing runtime source code or assembly code corresponding to user code using commands and tools, which is very practical. To summarize:\ngo tool compile go tool objdump dlv In the future, we will continue analyzing Go source code using these tools, such as for maps and slices. With these tools, our learning process will be more efficient.\n","date":"2024-06-29T20:47:20+08:00","permalink":"https://huizhou92.com/p/how-to-analyze-go-code-in-assembly/","title":"How to analyze Go code in assembly"},{"content":"As a software developer, I\u0026rsquo;m constantly on the lookout for the fastest and most efficient tools to build applications. When it comes to speed and tackling complex tasks, Golang and Node.js stand out as top contenders. Both boast excellent reputations in terms of performance. But the burning question remains—which one is faster, Golang or Node.js? To settle this debate, I embarked on a journey of rigorous benchmark testing, aiming to compare these two technologies in depth. By scrutinizing detailed results, my goal is to ascertain which platform holds the edge in raw speed. The data will reveal whether one platform has a clear advantage in developing high-performance applications.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nIntroduction In recent years, Golang and Node.js have garnered significant attention, each boasting its own strengths and merits. Golang, also known as Go, is a statically typed compiled programming language developed by Google. It has garnered praise for its simplicity, concurrency support through goroutines, and blazing-fast performance. Conversely, Node.js is an event-driven, non-blocking I/O platform built on Chrome\u0026rsquo;s V8 JavaScript engine. It has earned accolades for its asynchronous programming model, extensive package ecosystem through npm, and rapid development capabilities.\nBenchmarking Methodology Before delving into the benchmark test results, it\u0026rsquo;s crucial to establish a standardized methodology to ensure fair and accurate comparisons. In our benchmark tests, we\u0026rsquo;ll focus on common performance metrics such as response time, throughput, and resource utilization. The benchmark testing environment will encompass identical hardware specifications and configurations for Golang and Node.js applications. Additionally, we\u0026rsquo;ll utilize reliable benchmarking tools and frameworks to ensure the reliability and consistency of all experiments.\nResponse Time Comparison One of the pivotal metrics for evaluating web server performance is response time, which measures the duration between sending a request and receiving a response. In our benchmark tests, we\u0026rsquo;ll deploy identical web server applications written in Golang and Node.js, each providing simple HTTP requests and minimizing processing overhead. By simulating varying levels of concurrent client connections and analyzing the corresponding response times, we can assess the performance of Golang and Node.js under real-world workload scenarios.\n1 2 3 4 5 6 7 8 9 // Node.js HTTP server const http = require(\u0026#39;http\u0026#39;); const server = http.createServer((req, res) =\u0026gt; { res.writeHead(200); res.end(\u0026#39;Hello, World!\u0026#39;); }); server.listen(3000); requests per second\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // Golang HTTP server package main import ( \u0026#34;net/http\u0026#34; ) func handler(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\u0026#34;Hello, World!\u0026#34;)) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, handler) http.ListenAndServe(\u0026#34;:3000\u0026#34;, nil) } Throughput Analysis In addition to response time, throughput is another critical performance metric that measures the rate at which a system processes incoming requests. Higher throughput values indicate greater capacity to handle concurrent connections and deliver responses promptly. To compare the throughput of Golang and Node.js applications, we\u0026rsquo;ll ramp up the number of concurrent client requests and monitor each platform\u0026rsquo;s scalability in terms of request processing capability.\nResource Utilization Besides response time and throughput, assessing the resource utilization of Golang and Node.js applications under load is paramount. This includes monitoring CPU usage, memory consumption, and network activity to pinpoint any potential bottlenecks or inefficiencies. By scrutinizing resource utilization metrics, we can glean insights into each platform\u0026rsquo;s overall efficiency and scalability, aiding developers in making informed decisions when selecting the optimal technology stack for their projects.\nCPU Usage: Golang compiles to native machine code, making it highly CPU-efficient. Benchmark tests consistently demonstrate that Golang outperforms Node.js in CPU resource utilization. Node.js operates on a single thread and relies on asynchronous I/O calls, potentially resulting in underutilization of CPU resources and increased overhead. Memory Usage: Due to its static typing and compiled nature, Golang uses less memory than Node.js and necessitates fewer runtime metadata. Node.js relies on dynamic typing and the V8 JavaScript engine, demanding more memory to store type information and heap. Threading Model: Golang leverages lightweight threads (goroutines) for concurrency, simplifying the handling of multithreaded code and parallelism. Node.js adopts a single-threaded, non-blocking I/O model. Concurrency necessitates explicit coding using Worker Threads. Scalability: With lightweight goroutine threads, Golang scales horizontally exceptionally well, capable of accommodating substantial concurrent demands. Node.js excels in vertical scalability on a single machine but may encounter bottlenecks when scaling out due to its single-threaded nature. Concurrency Performance A standout feature of Golang is its native support for concurrency through lightweight goroutines and channels. This enables Golang applications to efficiently handle thousands of concurrent tasks with minimal overhead, making it ideal for building highly concurrent systems such as web servers, microservices, and distributed applications. In contrast, Node.js relies on an event-driven, non-blocking I/O model for concurrency, leveraging asynchronous functions and event loops. We\u0026rsquo;ll compare the concurrency performance of Golang and Node.js through stress testing high-concurrency tasks on both platforms, evaluating their responsiveness and scalability.\nConclusion\nIn summary, the benchmark test results vividly illustrate the performance disparities between Golang and Node.js. While both platforms have their merits, Golang showcases superior performance in terms of raw speed, concurrency, and resource efficiency. Its compiled nature, lightweight goroutines, and efficient runtime render it a compelling choice for high-performance applications necessitating swift response times and scalable concurrency.\nConversely, Node.js offers unparalleled developer productivity, extensive ecosystem support, and seamless integration with JavaScript front-end frameworks. Ultimately, the selection between Golang and Node.js hinges on the specific requirements and priorities of your project—whether it\u0026rsquo;s maximizing performance, leveraging existing JavaScript expertise, or optimizing developer efficiency.\n","date":"2024-06-29T20:33:26+08:00","image":"https://images.yixiao9206.cn/blog-images/2024/03/19f465a70b3b13a1bcf9d608bb5d5f3f.jpeg","permalink":"https://huizhou92.com/p/golang-vs-node/","title":"Golang vs node"},{"content":"Time flies, and in the blink of an eye, it\u0026rsquo;s already the last day of 2023. Looking back on this year, the world has fully recovered from the COVID-19 pandemic, and the Go language is no exception. One of the most noticeable changes is the significant transition of GopherCon conferences and small meetups around the world from being held online or canceled to being held in person. This shift signifies the progress and normalcy returning to the Go community. These events, such as GopherCon, GopherCon UK, GopherCon Europe, GopherCon Australia, and Golab, have received enthusiastic participation and warm welcomes from Gophers worldwide.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nOf particular note is the largest Gopher conference in China, GopherChina 2023. To meet the needs of Gophers in different regions, the GoCN community organized two GopherChina conferences in Beijing and Shanghai in June and November, respectively, for the first time in history.\nEven the \u0026ldquo;father of Go,\u0026rdquo; Rob Pike, who has already taken a back seat, personally participated in these conferences to promote the development of the global Go community and ecosystem. He delivered a keynote speech titled \u0026ldquo;What We Got Right, What We Got Wrong\u0026rdquo; at GopherCon Australia 2023, reflecting on the successes and failures of Go since its inception.\nAs the experts reflect on their lifetime achievements, let\u0026rsquo;s take stock of the year together. In this article, we will discuss the state of Go in 2023, its position in the programming landscape, and the mechanisms and strategies for its future evolution.\n1. Go in 2023 1.1 Stability As in previous years, Go released two major versions in 2023: Go 1.20 in February and Go 1.21 in August. These versions continued the tradition of stability in Go\u0026rsquo;s syntax features. While there were improvements, such as support for type conversion from slice types to array types (or pointers to array types), most of the changes focused on fixing and refining the syntax. For example, the \u0026ldquo;comparable\u0026rdquo; feature relaxed the restrictions on generic arguments, the unsafe package added more \u0026ldquo;syntactic sugar,\u0026rdquo; and there were additions like the min, max, and precise built-in functions, as well as enhanced type inference capabilities.\nThese changes were not surprising to Gophers because they align with Russ Cox\u0026rsquo;s 2022 adage that \u0026ldquo;Go is boring.\u0026rdquo;\nHowever, apart from the relatively modest changes in syntax features, Go continued to seek innovation and adaptation in other areas. Let\u0026rsquo;s examine how Go pursued innovation.\nNote: Pursuing innovation and adaptation may intersect and have some fuzzy boundaries. They can also mutually reinforce each other. Please read the following content without nitpicking. :)\n1.2 Innovation While maintaining stability in syntax features, Go tried optimizing and refining its compiler, toolchain, runtime, and standard library. The goal was to enhance productivity and runtime efficiency, and many of the optimizations and refinements introduced were novel.\nFor example, Go 1.20 introduced the preview version of PGO (profile-guided optimization), which became the default setting in Go 1.21. According to official data, PGO optimization improved performance by 2% to 7%. In the latest Go 1.22rc1, this number has increased from 2% to 14%.\nIn terms of memory management, although the experimental feature \u0026ldquo;arena package\u0026rdquo; introduced in Go 1.20 didn\u0026rsquo;t make it to the stable release in Go 1.21 and is currently in the proposal-hold state, it represents an innovative approach.\nGo is a programming language focused on software engineering, and in 2023, it made several innovations in this field. For example, the \u0026ldquo;new\u0026rdquo; tool, which simplifies Go project creation, gained popularity. It allows developers to clone and create their own Go projects based on go project templates. Another example is the collection of code coverage during application execution, which helps developers gain a deeper understanding of test coverage along the code execution paths. The \u0026ldquo;govulncheck\u0026rdquo; tool is an attempt to address software supply chain security in Go, enriching the means of checking security vulnerabilities in Go projects.\nNote: Regarding software supply chain security, Russ Cox recently gave a dedicated talk titled \u0026ldquo;Open Source Supply Chain Security at Google.\u0026rdquo; Interested readers can learn more from it.\nGo has always been open to new technologies, trends, and ideas from the community. As early as the WASM emerged, Go provided wasm porting support. In Go 1.21, Go also added support for WASI (WebAssembly System Interface), which has not yet formed a final specification.\nThe feedback from the Go community is also a source of innovation for the Go team. A typical example is the inclusion of \u0026ldquo;log/slog\u0026rdquo; in the standard library, which enables native support for structured logging with performance comparable to third-party open-source log packages like \u0026ldquo;zap.\u0026rdquo;\nThe Go community has followed the pace set by the Go team and embarked on the path of innovation. In 2023, one of the most significant events in the IT industry was the emergence of large language models represented by ChatGPT. This event will likely be a significant milestone in human civilization\u0026rsquo;s progress. Using large models has become a hot topic in various fields, and AI has become a buzzword. In the field of traditional machine learning, deep learning, and neural networks, where the ecosystem for Go is not as rich, Go has been exploring and integrating with large models. For example, the \u0026ldquo;ollama\u0026rdquo; open-source project, which enables fast local launch and execution of models like llama2, mistral 7B, codellama, and vicuna, has gained nearly 30k stars in just a few months. Additionally, Google open-sourced the \u0026ldquo;Google AI Go SDK\u0026rdquo; to support integration with various large model projects, providing detailed tutorials on how Gophers can interact with large models through the SDK.\nNote: Google has made the Gemini Pro API free for individual users. This model has the capabilities of GPT 3.5, with 32k context, support for 38 languages, and multimodal capabilities. The only constraint is a limit of 60 requests per minute.\nIn the second Go user survey in 2023, Go developers expressed interest in AI/machine learning tools that improve code quality, reliability, and performance rather than tools for writing code itself. An AI developer assistant who acts as a vigilant and always available expert reviewer may be a more helpful form of AI assistance. The Go team has shown its commitment to this survey, and AI integration may become a regular part of the Go toolchain in the future.\n1.3 Adaptation In August 2023, shortly after the release of Go 1.21, the official Go blog published two articles written by Russ Cox: \u0026ldquo;Backward Compatibility, Go 1.21, and Go 2\u0026rdquo; and \u0026ldquo;Forward Compatibility and Toolchain Management in Go 1.21.\u0026rdquo; These articles clarified the scope and approach of Go\u0026rsquo;s backward compatibility and presented the specific plan for forward compatibility for the first time. These articles laid the theoretical foundation for Go\u0026rsquo;s future \u0026ldquo;adaptation.\u0026rdquo;\nIn terms of backward compatibility, starting from Go 1.21, Russ Cox proposed measures such as expanding and standardizing the use of GODEBUG. The general idea is as follows:\nFor new features/changes within the scope of Go 1 compatibility that may break existing code (e.g., changes to the semantics of panic(nil)), Go will add a new option to the GODEBUG environment variable (e.g., GODEBUG=panicnil=1) to preserve compatibility with the original semantics during compilation. Newly added options in GODEBUG will be retained for at least two years (four Go release versions). Some options that have a significant impact (e.g., http2client and http2server) may be retained even longer or indefinitely. The options set in GODEBUG match the go version specified in go.mod. For example, even if you have to Go 1.21 as your toolchain if the go version in go.mod is 1.20, the new feature semantics controlled by GODEBUG will not take effect and will maintain the behavior of Go 1.20. The new feature semantics will only take effect when you upgrade the go version in go.mod to go 1.21.0. In Go 1.21 and later versions, in addition to using environment variables like GODEBUG=panicnil=1 to restore the original semantics, you can also use the //go: debug directive in the main package. Regarding forward compatibility, Russ Cox proposed a complex and nuanced plan, which is beyond the scope of this translation. Interested readers can refer to my previous article, \u0026ldquo;Understanding Go\u0026rsquo;s Forward Compatibility and Toolchain Rule,\u0026rdquo; for more details.\n1.3.1 Filling Syntax Gaps Among Go\u0026rsquo;s many adaptations, the most significant impact came from \u0026ldquo;fixing\u0026rdquo; existing syntax quirks. These \u0026ldquo;gap-filling\u0026rdquo; efforts may have influenced existing code and, in some cases, even introduced breaking changes. There were voices of opposition within the Go community. However, these changes were already set in motion with the release of Go 1.21. For example, changes to the semantics of panic(nil) and modifications to loop variable semantics, as detailed in the article \u0026ldquo;Some Changes in Go 1.21 Worth Noting.\u0026rdquo;\nFixing existing syntax gaps also drove further innovation. For example, while fixing loop variable semantics, work was underway to expand the range of expressions that can be iterated using the \u0026ldquo;for range\u0026rdquo; construct. In Go 1.22, \u0026ldquo;for the range\u0026rdquo; will support iterating over integer expressions, and experimental support for function iterators was provided.\n1.3.2 Standard Library V2 Demonstrations Go prides itself on being a \u0026ldquo;batteries included\u0026rdquo; language, with a high-quality standard library that Gophers widely embrace. The Go team has been working continuously to enrich the functionality of the standard library. For example, Go 1.22 introduced enhancements to the HTTP.ServeMux, making it comparable to third-party packages like gorilla/mux in terms of handling wildcard routes.\nIn Go 1.22, the standard library introduced its first v2 version package: math/rand/v2. This serves as a demonstration for the future evolution of standard library packages using the vN versioning approach. According to discussions in official Go team issues, packages like sync/v2 and encoding/json/v2 are already on the roadmap.\n2. Go\u0026rsquo;s Position Many people are interested in Go\u0026rsquo;s current status: How widely is it used in major Chinese tech companies? Are smaller companies also adopting it extensively? I have covered these questions in my previous Go language recaps so that I won\u0026rsquo;t repeat them here. No major company would completely rewrite its codebase within a year after extensively adopting a language. The adoption of Go by smaller companies also tends to be inertia-driven.\nLet\u0026rsquo;s start with two unexpected \u0026ldquo;findings\u0026rdquo; this year.\n2.1 Two Unexpected \u0026ldquo;Discoveries\u0026rdquo; In mid-October 2023, Tesla, a world-renowned electric vehicle manufacturer, released a new version of its fleet API and vehicle command SDK. Working in the smart connected vehicle industry, I looked deeper into Tesla\u0026rsquo;s release. To my surprise, Go is now the second most popular programming language in Tesla\u0026rsquo;s open-source projects on GitHub.\nCompared to traditional automakers, Tesla is relatively more open. Openness here has two meanings: openness of vehicle capabilities and transparency of projects. While most domestic automakers have yet to open up their vehicle capabilities, Tesla has been open in both aspects. Although there are not numerous open-source projects, they have tried opening up vehicle APIs and providing targeted open-source projects. In the past, Tesla used Ruby for cloud service-related projects, but since 2022, the use of Go has gradually increased. This includes projects like the Fleet Telemetry reference server implementation for Fleet API and the Tesla vehicle remote control SDK.\nNow, let\u0026rsquo;s take a look at the Apache Software Foundation. As we all know, most Apache projects are primarily written in Java. However, by chance, I came across the GitHub project page of the Apache Software Foundation and found that Go has quietly become the fifth most used language in Apache open-source projects. If we consider only backend languages, Go ranks third, second only to Java and Python.\nMoreover, the Apache Software Foundation has a considerable number of Go projects. You can check this link to see the complete list. Among them are outstanding projects such as \u0026ldquo;answer\u0026rdquo; for building Q\u0026amp;A knowledge systems, \u0026ldquo;dubbo-go\u0026rdquo; as the Go implementation of Apache Dubbo, \u0026ldquo;trafficcontrol\u0026rdquo; for CDN implementations, \u0026ldquo;Camel K\u0026rdquo; for lightweight enterprise application integration framework native to Kubernetes, \u0026ldquo;Apache Arrow\u0026rdquo; Go implementation, and \u0026ldquo;devlake\u0026rdquo; for aggregated data platforms for the development process.\nApache projects have a wide range of applications in enterprise-level systems and platforms. The increased use of Go in Apache projects indicates its growing popularity and acceptance in the enterprise application market.\n2.2 Go Language Rankings Competition and controversy among programming languages are often called \u0026ldquo;programming language wars,\u0026rdquo; and they reflect the collision of different technical communities and paradigms. These heated debates about languages are usually subjective. In the past decade, several widely accepted programming language rankings have emerged, which use relatively objective data to reflect the real-world status of different programming languages in development. However, each ranking has different data sources and models; a single ranking alone cannot provide a complete picture. Currently, there is no authoritative ranking that gives us a comprehensive view. Therefore, to objectively and comprehensively reflect the actual situation of a programming language, we need to refer to multiple rankings.\nNow, let\u0026rsquo;s look at the latest rankings for Go language from famous programming language rankings worldwide (please note that the release dates of these rankings may vary, resulting in some time differences in the data).\n2.2.1 PYPL Programming Language Rankings The PYPL (PopularitY of Programming Language) index is created by analyzing the frequency of language tutorials searched on Google. The more tutorials are searched, the more popular the language is. The raw data comes from Google Trends:\nPYPL Programming Language Rankings, data as of December 2023\n2.2.2 IEEE Spectrum Rankings The IEEE Spectrum rankings are based on surveys of global software engineers and data from job posting websites to calculate the popularity of various programming languages:\nIEEE Spectrum Rankings, data as of August 2023\n2.2.3 RedMonk Programming Language Rankings The RedMonk rankings are based on the number of discussions about languages on GitHub and Stack Overflow, two popular developer communities.\nRedMonk Programming Language Rankings, data as of May 2023\n2.3.4 GitHub Octoverse The GitHub Octoverse rankings provide a visual representation of the actual usage and popularity trends of various programming languages on GitHub over the past year. It measures the activity of programming languages based on the number of open-source projects. In the Top 10 languages list, Go surpassed Ruby for the first time and entered the GitHub Top 10 languages:\nGitHub Octoverse Programming Language Rankings, data as of November 2023\nGitHub Octoverse Programming Language Rankings, data as of November 2023\n2.3.5 Github Language Stats (Githut) Github Language Stats is a personal project that analyzes the usage of programming languages on GitHub based on publicly available data. It provides insights into language usage on GitHub from various perspectives, such as time, pull request count, and star count:\nGithut by PR count, data as of Q3 2023\nGithut by star count, data as of Q3 2023\n2.3.6 TIOBE Index TIOBE Programming Language Index is theoretically the most well-known programming language ranking in the world. It calculates a comprehensive index based on the search query volume related to programming languages on major search engines. However, the fluctuating data of the TIOBE rankings in recent years has made developers have a love-hate relationship with this list. Here is the TIOBE index for December 2023:\nWhen you see Fortran ranking ahead of Go, you realize that this ranking is somewhat unreliable.\nBased on the above six rankings, we can see that Go language remained in a stable development state in 2023, without making significant strides forward or unexpected setbacks.\nThis year, there have been discussions on some online platforms in China about whether Go has cooled down. Based on the actual situation mentioned above, those negative views about Go can be easily refuted. Some may argue that the rising popularity of Rust will have a certain impact on Go, and this cannot be denied. Just like how Go quickly rose and brought some impact to Java in the past, it is a natural process for a programming language to experience such challenges during its evolution. Five years later, Rust may also face challenges from other languages.\nThe future of Go depends on the Go team\u0026rsquo;s grasp of its future direction and the strength of the Go community and ecosystem. In 2023, the Go team also clarified the future evolution mechanism and strategy, so let\u0026rsquo;s take a look.\n3. The Future Evolution of Go 2023 marked the 14th year of Go\u0026rsquo;s open-source existence, and Go has long shed its youthful awkwardness and entered its adolescence. This means that it has become more mature and stable in terms of language features, and its ecosystem has become increasingly rich and complete. As a language in its prime, Go demonstrates a highly engineering-oriented mindset in system design, making it capable of handling complex system construction with ease. The modular support primarily provided by Go modules helps clarify large-scale programs, and its rich concurrency control mechanisms enable it to handle massive requests. At the same time, the Go language ecosystem is flourishing, with various high-quality frameworks emerging, numerous reusable modules available, and a wealth of cloud-native components to choose from. This greatly reduces the workload for developers to build systems from scratch.\nJust like us humans, the mature characteristics of a language in its adolescence cannot completely conceal its uncertainty about future evolution! After the retirement of Ken Thompson and Rob Pike, Russ Cox became the \u0026ldquo;helmsman\u0026rdquo; of the Go ship. Russ Cox\u0026rsquo;s thoughts on programming languages and his judgment of Go\u0026rsquo;s values will directly determine the course of Go\u0026rsquo;s future.\nFortunately, at the GopherCon conference in 2023, we got the answer from Russ Cox: it is \u0026ldquo;decision-making based on shared goals and data-driven insights\u0026rdquo;. Let\u0026rsquo;s take a look at the specific evolution driving mechanisms based on the conclusions presented by Russ Cox in his speech:\nFirst, Go needs to keep evolving, especially with the changes in the computing world. Second, the goal of any change is to make Go better in software engineering, especially in terms of scaling. Third, once we have determined the goals, the next crucial step is to have shared data for decision-making. Fourth, Go toolchain telemetry is an important source of additional real-world Go usage data to supplement our existing surveys and code analysis data. Based on the above, the Go team aims to \u0026ldquo;embrace change\u0026rdquo; but not make haphazard changes like a headless fly. Instead, they rigorously base their decisions on extensive data feedback, including user surveys, user feedback from VSCode plugin usage, research interviews conducted throughout the year, user experience research, and more. They also gather more accurate Go usage data through the optional telemetry feature that will be added to the Go toolchain.\nWith the addition of the optional telemetry feature to the Go toolchain, the Go team will have more precise decision-making based on user data, ensuring that the Go ship continues to sail in the right and bright direction!\n4. Conclusion In 2023, Go language continued to maintain its stability and reliability. Two major versions were released, Go 1.20 and Go 1.21, with relatively few changes in syntax features, focusing on bug fixes and optimizations. However, Go language still maintains a spirit of innovation and change in other aspects.\nThe Go language team is committed to optimizing the compiler, toolchain, runtime, and standard library to enhance productivity and runtime efficiency. They have introduced new features and optimization measures, such as the introduction and improvement of profile-guided optimization (PGO) technology and advancements in memory management. At the same time, Go language has also made innovations in the field of software engineering, including the gonew tool for simplifying project creation, code coverage collection tools, and the govulncheck tool for supply chain security.\nGo language has always maintained an open attitude towards new technologies, trends, and communities. In 2023, Go\u0026rsquo;s support for WASM and WASI was further strengthened. At the same time, the Go community actively responds to and follows the pace of the Go team. Faced with emerging technologies like large language models in the IT industry, the Go community continues to explore and apply them.\nOverall, 2023 was a year of stability and innovation for Go language. Go language maintains its characteristics of simplicity, efficiency, and ease of use, while actively adapting to and adopting new technologies and demands, providing developers with a better development experience and tool support.\nLooking ahead, the Go team has clearly defined an evolution mechanism driven by consensus and user data, ensuring that Go\u0026rsquo;s development direction remains in sync with practical needs. With the addition of the optional toolchain telemetry feature, they can make more accurate and forward-looking decisions based on richer user data.\nI personally still hold my previous judgment: Go will enter or may already be in its golden 5-10 years.\nThis article is a translation of a Chinese blog post written by tony bai. The original article can be found here.\n","date":"2024-06-29T20:30:36+08:00","image":"https://images.yixiao9206.cn/blog-images/2024/02/d14cc890845866e414d614d6648cb171.png","permalink":"https://huizhou92.com/p/go-language-recap-in-2023steady-progress-with-innovation-and-adaptation/","title":"Go Language Recap in 2023:Steady Progress with Innovation and Adaptation"},{"content":"In Go, we use go to create a new goroutine. A goroutine is a lightweight thread managed by the Go runtime.\nWe are all familiar with the states of operating system threads. In The time in computers: how long will it take to switch the context?, we summarized the states of threads and the time it takes for thread scheduling.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nGoroutines are similar; they have their own states, and the runtime controls their states.\nThe data structure of goroutines is defined in runtime2.go. The g.atomicstatus represents a goroutine\u0026rsquo;s state. The source code also defines its range of values.\nApart from several unused states and states related to GC, a goroutine may be in one of the following nine states:\nState Description _Gidle Just allocated and not yet initialized _Grunnable No code execution, no ownership of stack, stored in the run queue _Grunning Can execute code, owns a stack, assigned to kernel thread M and processor P _Gsyscall Executing a system call, owns a stack, not executing user code, assigned to kernel thread M but not in the run queue _Gwaiting Blocked due to runtime, not executing user code and not in the run queue, but may be in the waiting queue of a channel _Gdead Not in use, no code execution, may have allocated stack _Gcopystack Stack is being copied, no code execution, not in the run queue _Gpreempted Blocked due to preemption, not executing user code and not in the run queue, waiting to be woken up _Gscan GC is scanning the stack space, no code execution, can coexist with other states Among these states, the more common ones are _Grunnable, _Grunning, _Gsyscall, _Gwaiting, and _Gpreempted. We will focus on these states here. Goroutine state transitions are a complex process, and there are many methods to trigger goroutine state transitions. We cannot cover all transition routes here but will select some for discussion.\nAlthough goroutine states defined during runtime are numerous and complex, we can aggregate these different states into three categories: Waiting, Runnable, and Running. During runtime, goroutines switch among these three states:\nWaiting: Goroutine is waiting for certain conditions to be met, such as the end of a system call. This includes states like _Gwaiting, _Gsyscall, and _Gpreempted. Runnable: Goroutine is ready to run and can be scheduled for execution. If there are many goroutines in the program, each goroutine may wait for more time. This corresponds to _Grunnable. At this point, the goroutine is in the local queue of P or the global queue. Running: Goroutine is running on a thread, corresponding to _Grunning.\nGrunnable A goroutine enters the Grunnable state under the following circumstances:\nGoroutine Creation In Go, including the main entry main in the user program, all goroutines are created through runtime.newproc -\u0026gt; runtime.newproc1. The former is a wrapper for the latter. The go keyword ultimately translates to a call to runtime.newproc. When runtime.newproc1 completes resource allocation and initialization, the new task\u0026rsquo;s state is set to Grunnable and then added to the current P\u0026rsquo;s local task queue.\n1 2 3 4 5 6 7 8 9 10 11 12 13 func newproc1(fn *funcval, argp unsafe.Pointer, narg int32, callergp *g, callerpc uintptr) { // --snip-- // Get the current P from which to create a new G (newg). _p_ := _g_.m.p.ptr() newg := gfget(_p_) // --snip-- // Set the Goroutine state to Grunnable. casgstatus(newg, _Gdead, _Grunnable) // --snip-- // Add the newly created G to the run queue. runqput(_p_, newg, true) // --snip-- } Wakeup of Blocked Tasks When a blocked task (Gwaiting) is awakened due to certain conditions being met (such as writing data to a channel, which wakes up a task waiting to receive), the state of the waiting task (g1) is transitioned back to Grunnable and added to the task queue by calling runtime.ready. There is a more detailed explanation of goroutine blocking.\n1 2 3 4 5 6 7 8 9 10 func ready(gp *g, traceskip int, next bool) { // --snip-- // Get the current g. _g_ := getg() // Transition the state from Gwaiting to Grunnable. casgstatus(gp, _Gwaiting, _Grunnable) // Add to the run queue. runqput(_g_.m.p.ptr(), gp, next) // --snip-- } Others Another path is transitioning from Grunning and Gsyscall states to Grunnable, which will be discussed later. In short, a task in the Grunnable state must be in a task queue and ready to be scheduled for execution.\nGrunning All tasks in the Grunnable state may be retrieved by the scheduler (P\u0026amp;M) through the findrunnable function. Subsequently, their state is transitioned to Grunning, and finally, runtime.gogo is called to load the context and execute.\n1 2 3 4 5 6 7 8 9 10 11 // One round of scheduler: find a runnable goroutine and execute it. // Never returns. func schedule() { // --snip-- // Pick a runnable g and execute. if gp == nil { gp, inheritTime = findrunnable() // blocks until work is available } // --snip-- execute(gp, inheritTime) } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // Schedules gp to run on the current M. func execute(gp *g, inheritTime bool) { // Switch the current M to the new g. _g_ := getg() _g_.m.curg = gp gp.m = _g_.m // Transition the Grunnable state to Grunning. casgstatus(gp, _Grunnable, _Grunning) // --snip-- // Execute the goroutine. gogo(\u0026amp;gp.sched) } Go adopts a cooperative scheduling scheme. A running task needs to explicitly yield the processor.\nAfter Go 1.2, the runtime also supports a certain degree of task preemption. When the system thread sysmon detects that a task is taking too long to execute or the runtime determines that garbage collection is necessary, the task is marked as \u0026ldquo;preemptible\u0026rdquo;. Upon the next function call of the task, it yields the processor and switches back to the Grunnable state.\nGsyscall To ensure high concurrency performance, the Go runtime first sets its state to Gsyscall before executing OS system calls using the runtime.entersyscall function (if the system call is blocking or takes too long to execute, the current M is detached from P). Upon return from the system call, the thread calls runtime.exitsyscall to attempt to regain P. If successful and the current task has not been preempted, its state transitions back to Grunning for continued execution. Otherwise, it is set to Grunnable and waits to be scheduled for execution again.\n1 2 3 4 5 6 7 8 9 10 11 12 func reentersyscall(pc, sp uintptr) { _g_ := getg() // --snip-- casgstatus(_g_, _Grunning, _Gsyscall) // --snip-- // Detach m and p. pp := _g_.m.p.ptr() pp.m = 0 _g_.m.oldp.set(pp) _g_.m.p = 0 // --snip-- } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func exitsyscall() { _g_ := getg() // --snip-- // If P still exists, attempt to regain P. if exitsyscallfast(oldp) { // --snip-- casgstatus(_g_, _Gsyscall, _Grunning) // --snip-- return } // --snip-- // If P does not exist, Gsyscall -\u0026gt; Grunnable. mcall(exitsyscall0) // --snip-- } Gwaiting When a task\u0026rsquo;s required resource or running condition cannot be met, it needs to call the runtime.park function to enter this state. Subsequently, unless the waiting condition is met, the task will remain in the waiting state and cannot execute. Apart from the example of channels mentioned earlier, Go\u0026rsquo;s timers, network IO operations, atomics, and semaphores can all cause tasks to block.\n1 2 3 4 5 6 // park continuation on g0. func park_m(gp *g) { // --snip-- casgstatus(gp, _Grunning, _Gwaiting) // --snip-- } 1 2 3 4 func gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason waitReason, traceEv byte, traceskip int) { // --snip-- mcall(park_m) } In runtime.park, lock is the lock that the goroutine needs to release when it is blocked (such as in channels), and reason is the reason for blocking, which facilitates debugging with gdb. When all tasks are in the Gwaiting state, it means that the current program has entered a deadlock. In this case, the runtime detects this situation and outputs the backtrace information of all Gwaiting tasks.\nGdead When a task completes execution, it calls runtime.goexit to end. Its state is set to Gdead, and it enters the gFree list of the current P.\nConclusion The state transitions of goroutines are similar to those of thread state transitions, but they appear more complex due to garbage collection reasons. However, if we remove the GC part, the state transitions of goroutines are similar to those of thread state transitions.\nIn the next article, I will summarize and analyze the state transitions of P\n","date":"2024-06-29T20:22:37+08:00","image":"https://images.yixiao9206.cn/blog-images/2024/04/47e76a58f390d2e7740aacfbff0a328b.jpg","permalink":"https://huizhou92.com/p/decryption-gogoroutine-state-switching/","title":"decryption go：goroutine state switching"},{"content":"sync.WaitGroup, I believe all gophers have used it. It is a concurrency primitive in the package sync used for task coordination. It solves the problem of concurrency waiting: when a goroutine A is waiting at a checkpoint for a group of goroutines to complete. Without this synchronization primitive, how can we achieve this functionality? There are many ways; let\u0026rsquo;s first try using channels to implement it.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { channels := make(chan struct{}, 10) for i := 0; i \u0026lt; 10; i++ { go func() { println(\u0026#34;hello,world\u0026#34;) time.Sleep(time.Second * 10) channels \u0026lt;- struct{}{} }() } for i := 0; i \u0026lt; 10; i++ { \u0026lt;-channels fmt.Println(fmt.Sprintf(\u0026#34;done:%d\u0026#34;, i)) } fmt.Println(\u0026#34;all done\u0026#34;) } Although this code can fulfill the functionality, it is cumbersome. We not only need to create a channel but also create a goroutine to wait for this channel. Moreover, if the number of goroutines we need to wait for is uncertain, this approach is not very suitable. Therefore, sync.WaitGroup comes into play.\nNow, let\u0026rsquo;s implement the above functionality in a different way using sync.WaitGroup link.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) func main() { wg := sync.WaitGroup{} wg.Add(10) for i := 0; i \u0026lt; 10; i++ { go func() { defer wg.Done() println(\u0026#34;hello,world\u0026#34;) time.Sleep(time.Second * 10) }() } for i := 0; i \u0026lt; 10; i++ { go func() { wg.Wait() fmt.Println(fmt.Sprintf(\u0026#34;done,%d\u0026#34;, i)) }() } wg.Wait() fmt.Println(\u0026#34;all done\u0026#34;) time.Sleep(time.Second * 1) } In fact, many operating systems and programming languages provide similar concurrency primitives. For example, the barrier in Linux, the barrier in Pthread (POSIX threads), std::barrier in C++, CyclicBarrier and CountDownLatch in Java, and so on. This concurrency primitive is therefore a very fundamental type of concurrency.\nBasic Usage of WaitGroup The basic usage of WaitGroup is very simple, with only three methods:\n1 2 3 func (wg *WaitGroup) Add(delta int) func (wg *WaitGroup) Done() func (wg *WaitGroup) Wait() The Add method is used to increase the number of goroutines to wait for, the Done method is used to decrease the number of goroutines to wait for, and the Wait method is used to wait for all goroutines to complete.\nImplementation of WaitGroup This article is based on go1.21.4\nFirst, let\u0026rsquo;s take a look at the data structure of WaitGroup:\n1 2 3 4 5 type WaitGroup struct { noCopy noCopy state atomic.Uint64 // high 32 bits are counter, low 32 bits are waiter count. sema uint32 } The data structure of WaitGroup is very simple, with only two fields: state and sema. Among them, state is an atomic.Uint64 type used to store the counter and the number of waiting goroutines. sema is a semaphore used for waiting.\nAdd Method Let\u0026rsquo;s take a look at the implementation of the Add method, removing the race detection.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 func (wg *WaitGroup) Add(delta int) { state := wg.state.Add(uint64(delta) \u0026lt;\u0026lt; 32) v := int32(state \u0026gt;\u0026gt; 32) // counter w := uint32(state) // waiter count if v \u0026lt; 0 { panic(\u0026#34;sync: negative WaitGroup counter\u0026#34;) } if w != 0 \u0026amp;\u0026amp; delta \u0026gt; 0 \u0026amp;\u0026amp; v == int32(delta) { panic(\u0026#34;sync: WaitGroup misuse: Add called concurrently with Wait\u0026#34;) } if v \u0026gt; 0 || w == 0 { return } // This goroutine has set counter to 0 when waiters \u0026gt; 0. // Now there can\u0026#39;t be concurrent mutations of state: // - Adds must not happen concurrently with Wait, // - Wait does not increment waiters if it sees counter == 0. // Still do a cheap sanity check to detect WaitGroup misuse. if wg.state.Load() != state { panic(\u0026#34;sync: WaitGroup misuse: Add called concurrently with Wait\u0026#34;) } // Reset waiters count to 0. Wake up all waiters. wg.state.Store(0) for ; w != 0; w-- { runtime_Semrelease(\u0026amp;wg.sema, false, 0) } } The implementation of the Add method is very simple. It left-shifts the delta by 32 bits and adds it to the state. Then, it checks if the counter is less than 0, and if so, it panics. It also checks if the waiter count is not 0 and if the delta is greater than 0 and the counter is equal to delta, it panics. If the counter is greater than 0 or the waiter count is 0, it returns. If the state has been modified by other goroutines, it panics. Finally, it sets the waiter count to 0 and wakes up all waiters.\nDone Method The implementation of the Done method is very simple. It calls the Add method with a parameter of -1.\nWait Method The logic of the Wait method is as follows: it continuously checks the value of the state. If the counter becomes 0, it means that all tasks have been completed, and the caller does not need to wait any longer, so it returns directly. If the counter is greater than 0, it means that there are still tasks that have not been completed, so the caller becomes a waiter, needs to join the waiter queue, and blocks itself.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func (wg *WaitGroup) Wait() { for { state := wg.state.Load() v := int32(state \u0026gt;\u0026gt; 32) w := uint32(state) if v == 0 { return } // Increment waiters count. if wg.state.CompareAndSwap(state, state+1) { runtime_Semacquire(\u0026amp;wg.sema) // block until signalled if wg.state.Load() != 0 { panic(\u0026#34;sync: WaitGroup is reused before previous Wait has returned\u0026#34;) } return } } } Common Errors when Using WaitGroup The high 32 bits of the state are the counter, and the low 32 bits are the waiter count. If the value of the counter exceeds 2^31-1, it will panic. This problem rarely occurs in practical use because this value is too large. Add and Done do not appear in pairs, which means that if you add n times, you should also call Done n times. If you add n times but call Done m times, where m \u0026lt; n, it will panic or cause deadlock. Unexpected timing of Add. If Add is called after Wait, it will panic. Because this will cause the counter to become 0, but there are still goroutines waiting. Reusing WaitGroup. Bugs in WaitGroup usage in real projects In Golang issue 28123, the biggest problem in this code is that line 9 copies the WaitGroup instance w. Although this code can be executed successfully, it does violate the rule of not copying the WaitGroup instance after use. In projects, we can use the vet tool to detect such errors. Docker issue 28161 and issue 27011 are both errors caused by reusing WaitGroup without waiting for the previous Wait to finish before calling Add. Etcd issue 6534 is also a bug in reusing WaitGroup, where Add is called without waiting for the previous Wait to finish. Kubernetes issue 59574 is a bug in which the Wait is forgotten before increasing the count. This bug is considered almost impossible to occur in our usual understanding. Conclusion sync.WaitGroup is a very basic concurrency primitive. Its implementation is very simple, but it has many pitfalls. When using it, you must pay attention to not copying the WaitGroup instance, not reusing the WaitGroup instance, not calling Add after Wait, and ensuring that Add and Done appear in pairs. In real projects, we can use the go vet tool to check for these issues.\n","date":"2024-06-28T19:36:03+08:00","image":"https://images.yixiao9206.cn/blog-images/2024/06/34bc753d78c981f09cfe69fae96ed1f0.png","permalink":"https://huizhou92.com/p/decryption-gowaitgroup/","title":"Decryption Go：WaitGroup"},{"content":"Background In the previous article, we learned that panic can occur in three ways:\nInitiated by developers: by calling the panic() function. Hidden code generated by the compiler: for example, in the case of division by zero. Signals are sent to the process by the kernel, for example, in the case of illegal memory access. This article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nAll three cases can be categorized as calls to the panic() function, indicating that panic in Go is just a special function call and is handled at the language level. Now that we know how panic is triggered, the next step is to understand how panic is handled. When I was first learning Go, I often had some questions in my mind:\nWhat exactly is panic? Is it a struct or a function? Why does panic cause the process to exit? Why does recovery need to be placed inside a defer statement to take effect? Even if recover is placed inside a defer statement, why doesn\u0026rsquo;t the process recover? Why is it possible to panic again after a panic? What are the consequences? Today, let\u0026rsquo;s dive into the code to clarify these questions.\nBased on Go 1.21.4\n_panic Data Structure First, let\u0026rsquo;s look at an example of an actively triggered panic (example). By examining the assembly code, we can find that panic calls the runtime.gopanic function, which contains a crucial data structure called _panic.\nLet\u0026rsquo;s take a look at the _panic data structure:\n1 2 3 4 5 6 7 8 9 type _panic struct { argp unsafe.Pointer // pointer to arguments of deferred call run during panic; cannot move - known to liblink arg any // argument to panic link *_panic // link to earlier panic pc uintptr // where to return to in runtime if this panic is bypassed sp unsafe.Pointer // where to return to in runtime if this panic is bypassed recovered bool // whether this panic is over aborted bool // the panic was aborted goexit bool } Key fields to focus on:\nlink: A pointer to the _panic structure, indicating that _panic can be a unidirectional linked list, similar to the _defer linked list. recovered field: This field determines whether the _panic has been recovered or not. The recover() function actually modifies this field. Let\u0026rsquo;s also take a look at two important fields in g:\n1 2 3 4 5 type g struct { _panic *_panic // panic linked list, this is the innermost one _defer *_defer // defer linked list, this is the innermost one // ... } From this, we can see that both the _defer and _panic linked lists are attached to a goroutine. When can the _panic linked list have multiple elements?\nOnly when the panic() flow calls the panic() function again within a defer function. This is because the panic() function only executes the _defer functions internally!\nThe recover() Function To facilitate explanation, let\u0026rsquo;s start by analyzing what the recover() function does:\n1 2 3 defer func() { recover() }() The recover() function corresponds to the gorecover function implementation in the runtime/panic.go file.\nThe gorecover Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func gorecover(argp uintptr) any { // Must be in a function running as part of a deferred call during the panic. // Must be called from the topmost function of the call // (the function used in the defer statement). // p.argp is the argument pointer of that topmost deferred function call. // Compare against argp reported by caller. // If they match, the caller is the one who can recover. gp := getg() p := gp._panic if p != nil \u0026amp;\u0026amp; !p.goexit \u0026amp;\u0026amp; !p.recovered \u0026amp;\u0026amp; argp == uintptr(p.argp) { p.recovered = true return p.arg } return nil } This function is quite simple:\nRetrieve the current goroutine structure. Retrieve the latest _panic from the _panic linked list of the current goroutine. If it is not nil, proceed with the processing. Set the recovered field of the _panic structure to true and return the arg field. That\u0026rsquo;s all the recover() function does. It simply sets the value of the recovered field and does not involve any magical code jumps. The setting of the recovered field takes effect within the logic of the panic() function.\nThe panic() Function Based on the previous assembly code, we know that panic calls the runtime.gopanic function.\nThe gopanic Function The most important part of the panic mechanism is the gopanic function, which contains all the details about panic. The complexity of understanding panic lies in two points:\nRecursive execution of gopanic when panic is nested. The program counter (pc) and stack pointer (sp) are not manipulated in the usual way, but through direct modification of the instruction register structure, bypassing the logic after gopanic, and even handling recursive gopanic calls. The logic inside gopanic can be divided into two parts: inside the loop and outside the loop.\nInside the Loop The actions inside the loop can be broken down into the following steps:\nTraverse the _defer linked list of the goroutine to retrieve a _defer deferred function. Set the d.started flag and bind the current d._panic (used to check during recursion). Execute the _defer deferred function. Remove the executed _defer function from the linked list. Check if the recovered field of _panic is set to true and take appropriate action. If it is true, reset the pc and sp registers (generally starting from the deferreturn instruction) and enqueue the goroutine in the scheduler to wait for execution. Some Considerations You may notice that the recovered field is only modified in the third step. In fact, you cannot modify the value of _panic.recovered anywhere else.\nQuestion 1: Why does recover need to be placed inside a defer statement to take effect?\nBecause that is the only opportunity!\nLet\u0026rsquo;s consider a few simple examples:\n1 2 3 4 func main() { panic(\u0026#34;test\u0026#34;) recover() } In the above example, recover() is called, so why does it still panic?\nBecause it never reaches the recover() function.\n","date":"2024-06-28T19:32:02+08:00","image":"https://images.yixiao9206.cn/blog-images/2024/02/1301d7769a1515085fa143e709975d83.png","permalink":"https://huizhou92.com/p/decrypt-go-panic-and-recover/","title":"Decrypt Go: Panic and Recover"},{"content":"Cache Consistency Issues In concurrent programming, it\u0026rsquo;s common to encounter issues like this. For instance, concurrency issues arise when two goroutines simultaneously read and write to a variable. Take, for example, the following code: example\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func main() { var a int count := 1000000 wg := sync.WaitGroup{} for i := 0; i \u0026lt; 5; i++ { wg.Add(1) go func(wg *sync.WaitGroup) { for i := 0; i \u0026lt; count; i++ { a++ } wg.Done() }(\u0026amp;wg) } wg.Wait() fmt.Println(a) } I don\u0026rsquo;t know what the result of this code is, but it\u0026rsquo;s highly likely not the desired 5000000. It\u0026rsquo;s very likely to be a number much smaller than 5000000. Why does this happen?\nIn computer architecture, multi-core processors often have multiple levels of cache, with each core having its own cache. These caches store recently used data to speed up access to the data. When a thread writes to a variable, it first loads a copy of the variable from memory into its cache and makes modifications. Then, the thread writes the modified value back to memory. During this process, other threads may modify the value of this variable, but this thread is unaware of it. When other threads modify the value of this variable, the value in the cache of this thread becomes outdated. This is the so-called cache consistency problem, where data consistency issues arise due to asynchronous synchronization of caches across multiple cores.\nHow to Solve Cache Consistency Issues In Intel Developer Manual section 8.2.5, we find the following explanation:\nFor the Intel486 and Pentium processors, the LOCK# signal is always asserted on the bus during a LOCK operation, even if the area of memory being locked is cached in the processor.\nFor the P6 and more recent processor families, if the area of memory being locked during a LOCK operation is cached in the processor that is performing the LOCK operation as write-back memory and is completely contained in a cache line, the processor may not assert the LOCK# signal on the bus. Instead, it will modify the memory location internally and allow its cache coherency mechanism to ensure that the operation is carried out atomically. This operation is called “cache locking.” The cache coherency mechanism automatically prevents two or more processors that have cached the same area of memory from simultaneously modifying data in that area.\nThe I/O instructions, locking instructions, the LOCK prefix, and serializing instructions force stronger ordering on the processor.\nSynchronization mechanisms in multiple-processor systems may depend upon a strong memory-ordering model. Here, a program can use a locking instruction such as the XCHG instruction or the LOCK prefix to ensure that a read-modify-write operation on memory is carried out atomically. Locking operations typically operate like I/O operations in that they wait for all previous instructions to complete and for all buffered writes to drain to memory (see Section 8.1.2, “Bus Locking”).\nFrom the description, we understand that the LOCK prefix and XCHG instruction prefix provide strong consistency guarantees for internal (cached) memory read/write operations. They ensure that instructions after LOCK will only execute after instructions with the LOCK prefix. Moreover, from the manual, we also understand that in modern CPUs, the LOCK operation is not simply locking the communication bus between the CPU and main memory; Intel implements this LOCK operation at the cache level. Therefore, we need not worry about the efficiency of LOCK execution.\nThe Atomic package in Golang mainly ensures three things: atomicity, visibility, and order. Let\u0026rsquo;s look at the Atomic API in Go source code, which mainly includes Swap, CAS, Add, Load, Store, and Pointer categories. The corresponding assembly instructions on IA64 CPUs are as follows:\nSwap: mainly XCHGQ instruction CAS: mainly LOCK CMPXCHGQ instruction Add: mainly LOCK XADDQ instruction Load: mainly MOVQ (Load64) instruction Store: mainly XCHGQ instruction Pointer: mainly treated as a 64-bit int, calling the aforementioned related methods. So, we can use Atomic to solve cache consistency issues, as shown in the following code: example\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func main() { var a int64 count := 1000000 wg := sync.WaitGroup{} for i := 0; i \u0026lt; 5; i++ { wg.Add(1) go func(wg *sync.WaitGroup) { for i := 0; i \u0026lt; count; i++ { atomic.AddInt64(\u0026amp;a, 1) } wg.Done() }(\u0026amp;wg) } wg.Wait() fmt.Println(a) } Now we can get the correct result.\n1 2 5000000 Program exited. Expansion: How to Implement a Lock-Free Queue Using Atomic\nhttps://gist.github.com/hxzhouh/cfa8571f5c8d70422a37fdf9bd395d91\nThrough this example, we can grasp the basic operations of the atomic package. By learning atomic operations, we will delve into Go concurrent programming.\n","date":"2024-06-28T18:34:24+08:00","permalink":"https://huizhou92.com/p/decryption-goatomic-package-addressing-concurrency-issues/","title":"Decryption go：atomic package Addressing concurrency issues"},{"content":"If you haven\u0026rsquo;t read the mutex article, please read my article about mutex first. RWmutex is implemented based on mutex, just like dancing on the shoulders of giants.\nhttps://medium.com/gitconnected/go-source-code-sync-mutex-3082a25ef092\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nIn the process of using mutex, whether it is reading or writing, we use Mutex to ensure that only one goroutine accesses the shared resource. This is a bit \u0026ldquo;wasteful\u0026rdquo; in some cases. For example, in the case of few writes and many reads, even if there is no write operation for a period of time, a large number of concurrent read accesses have to be serialized under the protection of Mutex. At this time, using Mutex has a significant impact on performance. RWmutex is designed to optimize this problem.\nWhat is RWMutex Let\u0026rsquo;s explain the read-write lock RWMutex. The RWMutex in the standard library is a reader/writer mutex. At any given time, it can be held by any number of readers or by a single writer. This is the readers-writers problem, which can generally be divided into three categories based on the priority of read and write operations. The design and implementation of read-write locks are also divided into three categories based on the priority.\nRead-preferring: A read-preferring design can provide high concurrency, but in highly competitive situations, it may lead to write starvation. This is because, if there are a large number of reads, this design will cause the write to only be able to acquire the lock after all the reads have released the lock. Write-preferring: A write-preferring design means that if there is already a writer waiting for the lock, it will prevent new reader requests from acquiring the lock, thus prioritizing the writer. Of course, if some readers have already requested the lock, the new writer will also wait until the existing readers have released the lock before it can acquire it. Therefore, the priority in the write-preferring design is for new requests. This design mainly avoids writer starvation problems. No specified priority: This design is relatively simple and does not distinguish between reader and writer priorities. In some scenarios, this design without specified priorities is more effective. The first type of priority can lead to write starvation, and the second type of priority may lead to read starvation. This access without specified priorities does not distinguish between reading and writing, and everyone has the same priority, solving the starvation problem. The RWMutex design in the Go standard library is a write-preferring solution. A blocking Lock call excludes new reader requests from acquiring the lock.\nImplementation of RWMutex This article is based on go 1.21.4\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type RWMutex struct { w Mutex // held if there are pending writers writerSem uint32 // semaphore for writers to wait for completing readers readerSem uint32 // semaphore for readers to wait for completing writers readerCount atomic.Int32 // number of pending readers readerWait atomic.Int32 // number of departing readers } const rwmutexMaxReaders = 1 \u0026lt;\u0026lt; 30 // func (rw *RWMutex) Lock(){} func (rw *RWMutex) Unlock() {} func (rw *RWMutex) RLock(){} func (rw *RWMutex) RUnlock(){} func (rw *RWMutex) TryLock() bool{} func (rw *RWMutex) TryRLock() bool{} func (rw *RWMutex) RLocker() Locker{} The field readerCount records the current number of readers (and whether there is a writer competing for the lock). readerWait records the number of readers that need to wait for read completion when a writer requests the lock. Implementation of RLock/RUnlock 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func (rw *RWMutex) RLock() { if atomic.AddInt32(\u0026amp;rw.readerCount, 1) \u0026lt; 0 { // When rw.readerCount is negative, it means there is a writer waiting for the lock. Because the writer has a higher priority, the subsequent reader is blocked and put to sleep. runtime_SemacquireMutex(\u0026amp;rw.readerSem, false, 0) } } func (rw *RWMutex) RUnlock() { if r := atomic.AddInt32(\u0026amp;rw.readerCount, -1); r \u0026lt; 0 { rw.rUnlockSlow(r) // There are waiting writers } } func (rw *RWMutex) rUnlockSlow(r int32) { if atomic.AddInt32(\u0026amp;rw.readerWait, -1) == 0 { // The last reader, the writer finally has a chance to acquire the lock runtime_Semrelease(\u0026amp;rw.writerSem, false, 1) } } The second line adds 1 to the reader count. The readerCount field has a dual meaning: when there is no writer competing for or holding the lock, the readerCount is the same as the normal reader count we understand; if there is a writer competing for or holding the lock, the readerCount not only serves as the reader count, but also indicates whether there is a writer competing for or holding the lock (as explained later). In this case, the reader is blocked and waits for the writer to wake up (the fourth line).\nWhen calling RUnlock, we need to subtract 1 from the reader count (the eighth line) because the number of readers has decreased by one. However, the return value of AddInt32 in the eighth line has another meaning. If it is negative, it means that there is a writer competing for the lock. In this case, the rUnlockSlow method is called to check whether all readers have released the read lock. If all readers have released the read lock, the waiting writer can be awakened. When one or more readers hold the lock, the writer competing for the lock waits for these readers to release the lock before it can hold the lock.\nLOCK 1 2 3 4 5 6 7 8 9 10 func (rw *RWMutex) Lock() { // First, solve the problem of other writers competing for the lock rw.w.Lock() // Reverse the readerCount to tell the readers that there is a writer competing for the lock r := atomic.AddInt32(\u0026amp;rw.readerCount, -rwmutexMaxReaders) + rwmutexMaxReaders // If there are readers holding the lock, we need to wait if r != 0 \u0026amp;\u0026amp; atomic.AddInt32(\u0026amp;rw.readerWait, r) != 0 { runtime_SemacquireMutex(\u0026amp;rw.writerSem, false, 0) } } RWMutex is a multiple writer and multiple reader read-write lock, so there may be multiple writers and readers at the same time. In order to avoid writer competition, RWMutex uses a Mutex to ensure writer mutual exclusion. Once a writer obtains the internal mutex, it reverses the readerCount field from the original positive integer readerCount (\u0026gt;=0) to a negative number (readerCount-rwmutexMaxReaders), so that this field has two meanings (both the number of readers and whether there is a writer). Let\u0026rsquo;s look at the following code. In the fifth line, it also records the current number of active readers. The so-called active readers are those readers who hold the read lock and have not released it yet.\nIf readerCount is not 0, it means that there are readers holding the read lock, and RWMutex needs to save the current readerCount to the readerWait field (seventh line). At the same time, this writer enters a blocking waiting state (eighth line). Each time a reader releases the read lock (when calling the RUnlock method), the readerWait field is reduced by 1 until all active readers release the read lock, and then this writer can be awakened.\nUnlock 1 2 3 4 5 6 7 8 9 10 11 func (rw *RWMutex) Unlock() { // Tell the readers that there are no active writers r := atomic.AddInt32(\u0026amp;rw.readerCount, rwmutexMaxReaders) // Wake up the blocked readers for i := 0; i \u0026lt; int(r); i++ { runtime_Semrelease(\u0026amp;rw.readerSem, false, 0) } // Release the internal mutex rw.w.Unlock() } When a writer releases the lock, it reverses the readerCount field again (from a negative number to a positive number), and then wakes up the blocked readers (the fourth to sixth lines). Before returning from Unlock in RWMutex, the internal mutex needs to be released. After releasing it, other writers can continue to compete for the lock.\nTryLock \u0026amp; TryRLock The implementations of TryRLock and TryLock are both simple. They attempt to acquire the read lock or write lock, and if they fail, they return false. If they succeed, they return true. These two methods do not block and wait. Let\u0026rsquo;s look at the source code directly.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 func (rw *RWMutex) TryLock() bool { if !rw.w.TryLock() { // A goroutine holds the write lock return false } // The read lock is occupied if !rw.readerCount.CompareAndSwap(0, -rwmutexMaxReaders) { rw.w.Unlock() return false } return true } // TryRLock attempts to lock rw for reading and reports whether it succeeds. func (rw *RWMutex) TryRLock() bool { for { c := rw.readerCount.Load() // A goroutine holds the write lock if c \u0026lt; 0 { return false } // Attempt to acquire the read lock if rw.readerCount.CompareAndSwap(c, c+1) { return true } } } Controversial Points The controversial points of Mutex also exist in RWMutex, such as non-reentrancy and non-copyability. When writing code, you need to consider them carefully.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // An incorrect usage example of RWmutex func main() { var mu sync.RWMutex // writer, wait a little, and then create a scenario where Lock is called go func() { time.Sleep(200 * time.Millisecond) mu.Lock() fmt.Println(\u0026#34;Lock\u0026#34;) time.Sleep(100 * time.Millisecond) mu.Unlock() fmt.Println(\u0026#34;Unlock\u0026#34;) }() go func() { factorial(\u0026amp;mu, 10) // Calculate the factorial of 10, 10! }() select {} } // Recursive call to calculate factorial func factorial(m *sync.RWMutex, n int) int { if n \u0026lt; 1 { // Factorial exit condition return 0 } fmt.Println(\u0026#34;RLock\u0026#34;) m.RLock() defer func() { fmt.Println(\u0026#34;RUnlock\u0026#34;) m.RUnlock() }() time.Sleep(100 * time.Millisecond) return factorial(m, n-1) * n // Recursive call } Summary RWMutex is implemented based on Mutex. In scenarios with many reads and few writes, we can use RWMutex instead of Mutex to improve performance. At the same time, multiple readers can hold RWMutex through RLock(). When a writer locks RWMutex through Lock(), it first prevents new readers from locking, and then waits for all readers to release the lock before locking. When a writer releases the lock through ULock(), it wakes up the waiting readers. ","date":"2024-06-28T18:32:59+08:00","permalink":"https://huizhou92.com/p/go-source-code-analysis-rwmutex/","title":"Go Source Code Analysis  RWmutex"},{"content":"HTTP (Hypertext Transfer Protocol) has become the most widely used application layer protocol on the Internet. However, it is primarily a network protocol for transferring hypertext and provides no security guarantees. Transmitting data packets in plaintext over the Internet makes eavesdropping and man-in-the-middle attacks possible. Transmitting passwords over HTTP is essentially the same as running naked on the Internet.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nIn 1994, Netscape designed the HTTPS (Hypertext Transfer Protocol Secure) protocol, which uses the Secure Sockets Layer (SSL) to ensure secure data transmission. With the development of the Transport Layer Security (TLS) protocol, we have replaced the deprecated SSL protocol with TLS, although the term \u0026ldquo;SSL certificate\u0026rdquo; is still used.\nHTTPS is an extension of the HTTP protocol that allows us to transmit data over the Internet securely. However, the initial request in an HTTPS connection requires 4.5 times the round-trip time (RTT) compared to HTTP. This article will provide a detailed explanation of the request and response process, analyzing why the HTTPS protocol requires 4.5 RTT to obtain a response from the service provider:\nTCP Protocol: Both communication parties establish a TCP connection through a three-way handshake. TLS Protocol: Both communication parties establish a TLS connection through a four-way handshake. HTTP Protocol: The client sends a request to the server, and the server responds. The analysis is based on specific versions of protocol implementations and common scenarios. With the advancement of network technology, we can reduce the number of required network communications. This article will mention some common optimization solutions in the corresponding sections.\nTCP As an application layer protocol, HTTP relies on a lower-level transport layer protocol to provide basic data transmission functionality. TCP is commonly used as the underlying protocol for HTTP. To prevent the establishment of erroneous historical connections, TCP communication parties perform a three-way handshake to establish a TCP connection6. Let\u0026rsquo;s briefly review the entire process of establishing a TCP connection.\nThe client sends a segment with the SYN flag and the initial sequence number of the data segment (SEQ = M) to the server. Upon receiving the segment, the server sends a segment with the SYN and ACK flags to the client: The server acknowledges the initial sequence number of the client\u0026rsquo;s data segment by returning ACK = M+1. The server notifies the client of the initial sequence number of the server\u0026rsquo;s data segment by sending SEQ = N. The client sends a segment with the ACK flag to the server, confirming the server\u0026rsquo;s initial sequence number, including ACK = N+1. Through the three-way handshake, the TCP connection parties determine the initial sequence number, window size, and maximum segment size of the TCP connection. This allows the communication parties to ensure that the data segments are not duplicated or missed, control the flow through the window size, and avoid IP fragmentation by using the maximum segment size.\nThe original version of the TCP protocol did require a three-way communication to establish a TCP connection. In most current scenarios, the three-way handshake is unavoidable. However, TCP Fast Open (TFO), proposed in 2014, can establish a TCP connection in certain scenarios with just one communication.\nThe TFO strategy uses a TFO Cookie stored on the client to establish a connection with the server quickly. When the client initiates a TCP connection to the server, it includes the TFO option in the SYN message. The server generates a Cookie and sends it to the client. The client caches the Cookie and uses it to establish a TCP connection directly with the server when reconnecting. After verifying the Cookie, the server sends SYN and ACK to the client, initiating data transmission. This reduces the number of communications required.\nTLS The purpose of TLS is to build a secure transmission channel on top of the reliable TCP protocol. However, TLS itself does not provide reliability guarantees, so we still need a reliable transport layer protocol underneath. After establishing a reliable TCP connection between the communication parties, we need to exchange keys through the TLS handshake. Here, we will explain the connection establishment process of TLS 1.2:\nThe client sends a Client Hello message to the server, including the client\u0026rsquo;s supported protocol version, encryption algorithms, compression algorithms, and client-generated random number. Upon receiving the information about the client\u0026rsquo;s supported protocol version and encryption algorithms, the server: Sends a Server Hello message to the client, specifying the chosen protocol version, encryption method, session ID, and server-generated random number. Sends a Certificate message to the client, which includes the server\u0026rsquo;s certificate chain, including information about supported domains, issuers, and expiration dates. Sends a Server Key Exchange message, transmitting the public key and signature information. Optionally sends a CertificateRequest message, requesting the client\u0026rsquo;s certificate for verification. Sends a Server Hello Done message to the client, indicating that all relevant information has been sent. Upon receiving the server\u0026rsquo;s protocol version, encryption method, session ID, and certificate, the client verifies the server\u0026rsquo;s certificate: Sends a Client Key Exchange message to the server, including the pre-master secret, which is a random string encrypted with the server\u0026rsquo;s public key. Sends a Change Cipher Spec message to the server, indicating that subsequent data segments will be encrypted. Sends a Finished message to the server, which includes an encrypted handshake message. Upon receiving the Change Cipher Spec and Finished messages from the client: Sends a Change Cipher Spec message to the client, indicating that subsequent data segments will be encrypted. Sends a Finished message to the client, verifying the client\u0026rsquo;s Finished message and completing the TLS handshake. The key to the TLS handshake uses the random strings generated by both communication parties and the server\u0026rsquo;s public key to generate a negotiated key. This symmetric key allows both parties to encrypt messages, preventing eavesdropping and attacks by intermediaries and ensuring secure communication.\nIn TLS 1.2, establishing a TLS connection takes 2 RTT. However, TLS 1.3 optimizes the protocol, reducing the round-trip time to one, significantly reducing the time required. After 1 RTT, the client can already transmit application-layer data to the server.\nWe won\u0026rsquo;t go into detail about the TLS 1.3 connection establishment process here. In addition to reducing the network overhead in regular handshakes, TLS 1.3 introduces a 0-RTT connection establishment process. 60% of network connections are established when users first visit a website or after a certain interval, while the remaining 40% can be addressed using the 0-RTT strategy of TLS 1.3. However, this strategy, similar to TFO, carries some security risks and should be used with consideration for specific business scenarios.\nHTTP Transmitting data over a well-established TCP and TLS channel is relatively straightforward. The HTTP protocol can directly utilize the reliable and secure channel established at the lower layers to transmit data. The client writes data to the server using the TCP socket interface, and the server responds through the same means after processing the data. Since the entire process involves the client sending a request and the server returning a response, it takes 1 RTT.\nThe data exchange in the HTTP protocol consumes only 1 RTT. When the client and server handle a single HTTP request, we cannot optimize beyond the HTTP protocol itself. However, as the number of requests increases, HTTP/2 allows the reuse of established TCP connections to reduce the additional overhead of TCP and TLS handshakes.\nSummary When a client wants to access a server via HTTPS, the entire process requires 7 handshakes and consumes 9 times the latency. If the RTT is approximately 40ms due to physical distance limitations, the first request requires ~180ms. However, if we want to access a server in the United States with an RTT of approximately 200ms, the HTTPS request will take ~900ms, which is a significant delay. Let\u0026rsquo;s summarize the reasons why the HTTPS protocol requires 9 times the latency to complete communication:\nThe TCP protocol requires a three-way handshake to establish a reliable TCP connection (1.5 RTT). The TLS protocol establishes a TLS connection over TCP through a four-way handshake to ensure communication security (2 RTT). The HTTP protocol sends a request and receives a response over TCP and TLS in one round trip (1 RTT). It is important to note that the calculations of round-trip delay in this article are based on specific scenarios and protocol versions. Network protocols are constantly evolving, and issues that were initially overlooked are often addressed through patch updates. However, in the end, a complete rewrite from the ground up is still necessary.\nHTTP/3 is an example of this. It uses the UDP-based QUIC protocol for handshakes, combining the TCP and TLS handshake processes to reduce the 7 handshakes to 3. It directly establishes a reliable and secure transmission channel, reducing the time required from ~900ms to ~500ms. We will cover HTTP/3-related content in future articles. Finally, let\u0026rsquo;s consider some open-ended questions for further exploration. Interested readers can carefully consider the following questions:\nWhat are the similarities and differences between the QUIC protocol and the TCP protocol as transport layer protocols? How is it possible to establish a client-server connection using 0-RTT? Further Reading Here are some excellent articles that explain TCP three-way handshake, TLS connection, and TCP Fast Open in detail:\nWalls, Colin (2005). Embedded Software: The Works. Newnes. p. 344. ISBN 0-7506-7954-9. Archived from the original on 2019-02-09. Retrieved 2018-10-20. What is an SSL Certificate? Wikipedia: HTTPS Wikipedia: TCP Fast Open RFC793 Transmission Control Protocol, September 1981 RFC793 RFC5246 The Transport Layer Security (TLS) Protocol. Version 1.2. Aug 2008 RFC5246 Cheng, Y., Chu, J., Radhakrishnan, S., and A. Jain, “TCP Fast Open”, RFC 7413, DOI 10.17487/RFC7413, December 2014,. ↩︎ The First Few Milliseconds of an TLS 1.2 Connection TLS 1.3 Handshake: Taking a Closer Look Introducing Zero Round Trip Time Resumption (0-RTT) ","date":"2024-06-28T18:24:40+08:00","permalink":"https://huizhou92.com/p/why-does-https-need-7-handshakes-and-9-times-delay/","title":"Why does HTTPS need 7 handshakes and 9 times delay?"},{"content":"In existing logging libraries, including the slog logging library introduced in go 1.21.0, typically support log file rotation and segmentation. However, these functionalities are not built-in directly but require manual configuration to enable.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nThis article will explore several popular logging libraries, such as Logrus, zap, and the official Slog. I will analyze the key design elements of these libraries and discuss how they support the configuration of log rotation and segmentation functionalities.\nAre you ready? Grab a cup of your favorite coffee or tea, and let\u0026rsquo;s dive into the exploration.\nAnalysis of logrus, zap, and slog Designs When comparing and analyzing the designs of the logrus, zap, and slog logging libraries, a notable commonality is that they all include the crucial attribute io.Writer. This attribute plays a central role in the design of logging frameworks as it determines the destination of log output.\nlogrus Logging Library logrus is a feature-rich Go logging library that provides structured logging and log level control.\nWhen using logrus, you can create a Logger instance by calling the logrus.New() function. Through this instance, you can perform various operations such as customizing the log output location and printing logs. Take a look at the following code:\n1 2 3 4 5 6 7 8 logger := logrus.New() logger.Out = os.Stdout // stdout // or redirects to file //out, err := os.OpenFile(\u0026#34;file.log\u0026#34;, os.O_CREATE|os.O_WRONLY, 0666) //if err != nil { // panic(err) //} //logger.Out = out The definition of the Logger struct is as follows:\n1 2 3 4 5 6 type Logger struct { Out io.Writer Hooks LevelHooks Formatter Formatter // other fields... } The key attribute Out, of type io.Writer, is used to specify the output destination of the logs, whether it\u0026rsquo;s standard output, a file, or any other custom output medium.\nzap Logging Library zap is a high-performance logging library that provides structured logging, multi-level log control, and flexible configuration options.\nSimilar to logrus, zap also allows configuring the log output location through settings. However, the implementation approach is slightly different. In zap, log output is achieved through the configuration of zapcore.Core. When creating an instance of zapcore.Core, you need to specify a zapcore.WriteSyncer interface implementation as a parameter, which directly determines the log\u0026rsquo;s output destination. To create a zapcore.WriteSyncer instance, the zapcore.AddSync() function is commonly used, which takes a parameter of type io.Writer.\nHere\u0026rsquo;s a basic example of creating a log instance using zap:\n1 2 3 4 5 6 7 8 9 10 writer := zapcore.AddSync(os.Stdout) // use stdout core := zapcore.NewCore( zapcore.NewJSONEncoder(zap.NewProductionEncoderConfig()), writer, zap.InfoLevel, ) logger := zap.New(core) defer logger.Sync() // Flush any buffered log entries // use logger The key lies in the zapcore.AddSync() function, which takes a parameter of type io.Writer. This parameter is used to specify the log\u0026rsquo;s output destination, whether it\u0026rsquo;s standard output, a file, or any other custom output medium.\nslog Logging Library slog is an official logging library introduced in go 1.21.0, providing structured logging. If you want to learn more about the slog logging library,\nSimilar to logrus and zap, slog also allows users to set the log output destination by specifying an io.Writer parameter when creating an implementation of slog.Handler.\n1 2 textLogger := slog.New(slog.NewTextHandler(os.Stdout, nil)) jsonLogger := slog.New(slog.NewJSONHandler(os.Stdout, nil)) In both of these functions, the first parameter of slog.NewTextHandler and slog.NewJSONHandler is of type io.Writer.\nSummary of the Analysis In the analysis of the three mainstream logging libraries, logrus, zap, and slog, we can identify a crucial commonality: they all rely on the io.Writer interface when handling log output. These logging libraries use the io.Writer interface as a key parameter type to set the log\u0026rsquo;s output destination.\nMechanism and Practice of Log Rotation and Segmentation Functionality Mechanism of Implementation After analyzing the designs of the logrus, zap, and slog logging libraries, we have discovered their commonalities. Now, let\u0026rsquo;s delve into the mechanism of log rotation and segmentation functionality.\nTo implement log file rotation and segmentation, we typically rely on third-party libraries such as lumberjack. There are other similar libraries available, but we won\u0026rsquo;t list them all here.\nlumberjack is a library specifically designed for log rotation and segmentation. Its role can be likened to a pluggable component. By configuring this component and integrating it into the chosen logging library, we can achieve log file rotation and segmentation functionality.\nThe code for initializing the lumberjack component is as follows:\n1 2 3 4 5 6 7 8 log := \u0026amp;lumberjack.Logger{ Filename: \u0026#34;/path/file.log\u0026#34;, // location of the log file MaxSize: 10, // Maximum file size (in MB) MaxBackups: 3, // Maximum number of old files to keep MaxAge: 28, // Maximum number of days to retain old files Compress: true, // whether to compress/archive old files LocalTime: true, // create timestamps using local time } In this example, we create a lumberjack.Logger instance and set the following parameters:\nFilename: Specifies the storage path of the log file. MaxSize: Triggers log rotation when the log file reaches a certain size in MB. MaxBackups: Specifies the maximum number of old log files to keep. MaxAge: Sets the maximum number of days to retain old log files. Compress: Determines whether to compress/archive old log files (e.g., convert them to .gz). LocalTime: Creates timestamps using local time. It\u0026rsquo;s important to note that the Logger struct of lumberjack implements the io.Writer interface. This means that all the core logic related to log file rotation and segmentation is encapsulated in the Write method. This implementation also allows the Logger struct to be integrated into any logging library that supports the io.Writer parameter.\nNow that we understand these details, you should have a good understanding of how to implement log rotation and segmentation functionality. Since the Logger struct of lumberjack implements the io.Writer interface, you can pass it to third-party libraries for seamless integration and configuration.\nPractice Implementation with logrus Logging Library 1 2 3 4 5 6 7 8 9 10 log := \u0026amp;lumberjack.Logger{ Filename: \u0026#34;/path/file.log\u0026#34;, // location of log file MaxSize: 10, // Maximum file size in MB MaxBackups: 3, // Maximum number of old files to keep MaxAge: 28, // Maximum number of days to retain old files Compress: true, // whether to compress/archive old files LocalTime: true, // create timestamps using local time } logger := logrus.New() logger.Out = log Implementation with zap Logging Library 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 log := \u0026amp;lumberjack.Logger{ Filename: \u0026#34;/path/file.log\u0026#34;, // location of log file MaxSize: 10, // Maximum file size in MB MaxBackups: 3, // Maximum number of old files to keep MaxAge: 28, // Maximum number of days to retain old files Compress: true, // whether to compress/archive old files LocalTime: true, // create timestamps using local time } writer := zapcore.AddSync(log) core := zapcore.NewCore( zapcore.NewJSONEncoder(zap.NewProductionEncoderConfig()), writer, zap.InfoLevel, ) logger := zap.New(core) defer logger.Sync() // Flush any buffered log entries Implementation with slog Logging Library 1 2 3 4 5 6 7 8 9 10 log := \u0026amp;lumberjack.Logger{ Filename: \u0026#34;/path/file.log\u0026#34;, // location of log file MaxSize: 10, // Maximum file size (in MB) MaxBackups: 3, // Maximum number of old files retained MaxAge: 28, // Maximum number of days to keep old files Compress: true, // Whether to compress/archive old files LocalTime: true, // Create a timestamp with local time } textLogger := slog.New(slog.NewTextHandler(log, nil)) jsonLogger := slog.New(slog.NewJSONHandler(log, nil)) In this article, we have analyzed the design elements of the three popular logging libraries, logrus, zap, and slog. Although they have differences in the details of creating log instances, they all rely on the io.Writer interface parameter to handle log output. By mastering how to configure the io.Writer parameter and combining it with the use of the lumberjack library, we can achieve log file rotation and segmentation functionality.\nEven if new logging libraries are introduced in the future, we can quickly integrate log file rotation and segmentation functionality using similar methods.\n","date":"2024-06-28T18:21:48+08:00","permalink":"https://huizhou92.com/p/an-analysis-of-mainstream-go-logging-librarieslearning-how-to-integrate-log-rotation-and-segmentation-functionality-from-a-design-perspective/","title":"An Analysis of Mainstream Go Logging Libraries：Learning How to Integrate Log Rotation and Segmentation Functionality from a Design Perspective"},{"content":"TCP protocol is a network protocol that we encounter almost every day. The majority of network connections are established based on the TCP protocol. People who have studied computer networks or have some understanding of the TCP protocol know that establishing a connection using TCP requires a three-way handshake.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nIf we briefly explain the process of establishing a TCP connection, many people who have prepared for interviews would be familiar with it. However, when it comes to delving into the question of “Why does TCP require a three-way handshake to establish a connection?” most people would not be able to answer this question or might provide incorrect answers. This article will discuss why we need a three-way handshake to establish a TCP connection instead of four or two.\nOverview Before analyzing the question at hand, let’s first address a common misconception that has misled many people regarding the TCP connection process. For a long time, the author of this article also believed that it provided a good explanation for why a TCP connection requires a three-way handshake:\n\u0026ndash;\u0026gt; Can you hear me?\n\u0026lt;\u0026ndash; I can hear you. Can you hear me?\n\u0026ndash;\u0026gt; I can hear you too.\nUsing analogies to explain a problem often leads to a situation where “nine out of ten analogies are wrong.” If someone uses an analogy to answer your “why” question, you need to carefully consider the flaws in their analogy. Analogies can only provide a partial similarity, and we can never find an absolutely correct analogy. Analogies are only useful when we want to present the characteristics of something in a simple and understandable way. In the rest of the article, we will explain why this analogy is flawed, and readers can read the remaining content with this question in mind.\nWhen many people try to answer or think about this question, they tend to focus on the “three” in the three-way handshake, which is indeed important. However, if we reexamine the question, do we really understand what a “connection” is? Only when we know the definition of a “connection” can we attempt to answer why TCP requires a three-way handshake.\nThe reliability and flow control mechanisms described above require that TCPs initialize and maintain certain status information for each data stream. The combination of this information, including sockets, sequence numbers, and window sizes, is called a connection.\nThe RFC 793 — Transmission Control Protocol document clearly defines what a connection is in TCP. In summary, a connection is the information used to ensure reliability and flow control mechanisms, including sockets, sequence numbers, and window sizes.\nTherefore, establishing a TCP connection means that the two parties involved in communication need to reach a consensus on the three types of information mentioned above. A pair of sockets in a connection is composed of an Internet address identifier and a port. The window size is mainly used for flow control, and the sequence number is used to track the sequence of data packets sent by the initiating party, allowing the receiving party to confirm the successful receipt of a particular data packet based on the sequence number.\nAt this point, we have transformed the original question into “Why do we need a three-way handshake to initialize sockets, window sizes, and initial sequence numbers?” Next, we will analyze and seek explanations for this refined question.\nDesign This article will mainly discuss why we need a three-way handshake to initialize sockets, window sizes, initial sequence numbers, and establish a TCP connection from the following aspects:\nA three-way handshake is required to prevent the initialization of duplicate historical connections. A three-way handshake is required to initialize the initial sequence numbers of both communicating parties. Discuss the possibility of establishing a connection with a different number of handshakes. Among these arguments, the first one is the primary reason why TCP chooses to use a three-way handshake. The other reasons are secondary in comparison. We discuss them here to provide a more comprehensive perspective and understand this interesting design decision from multiple angles.\nHistorical Connections The RFC 793 — Transmission Control Protocol clearly points out the primary reason why TCP uses a three-way handshake: to prevent confusion caused by the initiation of old duplicate connections.\nThe principle reason for the three-way handshake is to prevent old duplicate connection initiations from causing confusion.\nImagine this scenario: if the number of communications between the two parties is only two, once the sender sends a connection establishment request, it cannot retract this request. In a complex or poor network condition, if the sender continuously sends multiple connection establishment requests and TCP establishes a connection with only two communications, the receiver can only choose to accept or reject the sender’s request. The receiver is not sure whether this request is an expired connection due to network congestion.\nTherefore, TCP chooses to use a three-way handshake to establish a connection and introduces the RST control message. When the receiver receives the request, it sends the sender\u0026rsquo;s SEQ+1 as part of the ACK control message. At this point, the sender can determine whether the current connection is a historical connection:\nIf the current connection is a historical connection, meaning the SEQ has expired or timed out, the sender will directly send an RST control message to terminate this connection. If the current connection is not a historical connection, the sender will send an ACK control message, and the two parties will successfully establish a connection. By using a three-way handshake and the RST control message, the ultimate control over whether to establish a connection is given to the sender. Only the sender has enough context to determine if the current connection is erroneous or expired. This is also the primary reason why TCP uses a three-way handshake to establish a connection.\nInitial Sequence Numbers Another important reason for using a three-way handshake is that both communicating parties need to obtain an initial sequence number for sending information. As a reliable transport layer protocol, TCP needs to build a reliable transport layer in an unstable network environment. The uncertainty of the network can lead to issues such as packet loss and out-of-order delivery. Common problems may include:\nData packets being repeatedly sent by the sender, resulting in duplicate data. Data packets being lost during transmission due to routing or other network nodes. Data packets arriving at the receiver may not be in the order they were sent. To address these potential issues, the TCP protocol requires the sender to include a “sequence number” field in the data packet. With the sequence number corresponding to each data packet, we can:\nThe receiver can deduplicate repeated data packets based on the sequence number. The sender will resend the corresponding data packet until it is acknowledged. The receiver can reorder the data packets based on their sequence numbers. Sequence numbers play a crucial role in TCP connections, and the initial sequence number, as part of a TCP connection, needs to be initialized during the three-way handshake. Since both parties in a TCP connection need to obtain the initial sequence number, they need to send a SYN control message to each other, carrying their expected initial sequence number SEQ. Upon receiving the SYN message, the receiver will confirm it using the ACK control message and SEQ+1.\nAs shown in the above diagram, the two TCPs, A and B, send SYN and ACK control messages to each other. After both parties obtain their expected initial sequence numbers, they can start communication. Due to the design of the TCP message header, we can combine the two middle communications into one. TCP B can send both the ACK and SYN control messages to TCP A simultaneously, reducing the four communications to three.\nA three-way handshake is necessary because sequence numbers are not tied to a global clock in the network, and TCPs may have different mechanisms for picking the ISN’s. The receiver of the first SYN has no way of knowing whether the segment was an old delayed one or not unless it remembers the last sequence number used on the connection (which is not always possible), and so it must ask the sender to verify this SYN. The three-way handshake and the advantages of a clock-driven scheme are discussed in [3].\nFurthermore, as a distributed system, the network does not have a global clock for counting. TCP can initialize sequence numbers using different mechanisms. As the receiver of a TCP connection, we cannot determine if the initial sequence number received from the other party is expired. Therefore, we need the other party to make this determination. It is not practical for the receiver to save and verify the sequence numbers, which reinforces the point we made in the previous section — avoiding the initialization of historical wrong connections.\nNumber of Communications When discussing the number of communications required to establish a TCP connection, we often focus on why it takes three communications instead of two or four. Discussing using more communications to establish a connection is often meaningless because we can always “exchange the same information using more communications.” Therefore, it is technically possible to establish a connection using four, five, or even more communications.\nThe issue of increasing the number of communications in a TCP connection often does not require discussion. What we pursue is actually completing the information exchange with the fewest number of communications (the theoretical minimum). This is why we repeatedly emphasize in the previous sections that using a “two-way handshake” cannot establish a TCP connection, and using a three-way handshake is the minimum number of communications required to establish a connection.\nConclusion In this article, we discussed why TCP requires a three-way handshake to establish a connection. Before analyzing this question in detail, we first reconsidered what a TCP connection is. The RFC 793 — Transmission Control Protocol — IETF Tools provides a clear definition of a TCP connection — the data used for ensuring reliability and flow control mechanisms, including sockets, sequence numbers, and window sizes.\nThe three-way handshake in TCP can effectively prevent the initiation of erroneous historical connections and reduce unnecessary resource consumption for both communicating parties. The three-way handshake helps both parties obtain the initial sequence numbers, ensuring that data packets are transmitted without duplication or loss and maintaining their order. At this point, it is clear why “two-way handshake” and “four-way handshake” are not used:\n“Two-way handshake”: It cannot prevent the initialization of erroneous historical connections and wastes resources for the receiver. “Four-way handshake”: The design of the TCP protocol allows us to simultaneously transmit both the ACK and SYN control messages, reducing the number of communications. Therefore, there is no need to use more communications to transmit the same information. Returning to the question raised at the beginning of the article, why is using an analogy to explain TCP’s three-way handshake incorrect? This is mainly because the analogy does not clearly explain the core issue — avoiding the initialization of historical duplicate connections.\nReference RFC 793 — Transmission Control Protocol — IETF Tools Why do we need a 3-way handshake? Why not just 2-way? # TCP 3-Way Handshake Process ","date":"2024-06-28T16:33:50+08:00","permalink":"https://huizhou92.com/p/why-tcp-requires-three-handshakes-to-establish-a-connection/","title":"Why TCP requires three handshakes to establish a connection"},{"content":"Last time, I shared several open-source software written in Go, which received a lot of love from readers. In this article, I\u0026rsquo;m sharing some open-source software written in Rust, believing they will be helpful in your life. These could serve as good learning materials if you\u0026rsquo;re also learning Rust.\nThis article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nAs usual, all installations and tests are based on my Mac M1.\nPlease note that some software doesn\u0026rsquo;t provide unified installation packages and requires compilation from source code. Therefore, you\u0026rsquo;ll need some programming experience to install the Rust programming environment and cargo build tool. You can refer to this page for guidance: Rust Cargo Installation Guide\nczkawka Stars: 16.8k | Forks: 554\nA multifunctional file-cleaning tool written in Rust. This project aims to find and clean duplicate files, empty folders, and similar image files, among others. It\u0026rsquo;s free, open-source, and ad-free, featuring speed, cross-platform compatibility, and multilingual support. With this tool, you can easily clean up unnecessary files on your computer, freeing up storage space.\nThe author of this software also shares insights on Medium here.\nInstallation 1 2 3 4 5 6 7 8 brew install rustup rustup-init brew install gtk4 adwaita-icon-theme librsvg libheif pkg-config git clone https://github.com/qarmin/czkawka.git cd czkawka export LIBRARY_PATH=$LIBRARY_PATH:/opt/homebrew/Cellar/libheif/1.17.6/lib/ source \u0026#34;$HOME/.cargo/env\u0026#34; cargo run --release --bin czkawka_gui --features \u0026#34;heif,libraw\u0026#34; cmd-wrapped Stars: 828 | Forks: 22\nA command-line history analysis tool written in Rust. This command-line tool reads your command-line operation history and generates detailed analysis reports, including information about command line activity periods over the past year, commonly used commands, etc., supporting Zsh, Bash, Fish, etc.\nInstallation 1 cargo install cmd-wrapped Usage:\n1 cmd-wrapped 2024 -s zsh The output looks like this:\nlettura Stars: 1.1k | Forks: 50\nA minimalist open-source RSS reader. This is a desktop RSS reader developed based on Tauri, free, ad-free, and with a clean interface, suitable for macOS and Windows operating systems.\nThis software provides installation packages for you to try out.\nprivaxy Stars: 2.2k | Forks: 87\nA Rust-based tracking and ad-blocking tool. This project is based on the MITM (Man-In-The-Middle) attack principle, establishing bidirectional tunnels between two ends to block advertising requests by filtering URL addresses, thus intercepting ads. It consumes less memory, runs faster, supports automatic updating of filter lists, displays statistics, and customizes filter lists.\nThis software provides installation packages for you to try out.\nlsd Stars: 12.1k | Forks: 395\nThe next-generation ls command. This project is a tool written in Rust to view directory listings similar to the ls command, with added features such as colors and icons, making it more visually appealing.\nInstallation 1 cargo install lsd tailspin Stars: 4.8k | Forks: 65\nThis project is a Rust-based command-line log viewer that is ready to use without configuration. It highlights numbers, dates, IPs, URLs, etc., making important information stand out.\nInstallation 1 brew install tailspin I\u0026rsquo;ve already replaced less with this tool, which looks much better.\nveloren Stars: 5.2k | Forks: 350\nIt\u0026rsquo;s a pixel-style RPG game written in Rust. It draws inspiration from games like \u0026ldquo;The Legend of Zelda: Breath of the Wild,\u0026rdquo; \u0026ldquo;Dwarf Fortress,\u0026rdquo; and \u0026ldquo;Minecraft.\u0026rdquo; Although the graphics of this game are low-resolution, it offers a vast open world where players can craft items, synthesize objects, battle, level up, tame pets, explore dungeons and caves, glide in the air, and trade with NPCs.\nSummary There are many interesting projects on GitHub. If you want to learn a language, the best way is to find interesting projects on GitHub and contribute to them. Practice is more important than theory.\n","date":"2024-06-28T16:30:11+08:00","permalink":"https://huizhou92.com/p/7-amazing-rust-project-on-github/","title":"7 Amazing Rust Project on GitHub"},{"content":"unsafe.Pointer In the previous article, we compared the differences between three types of pointers in Go. You can check that out before diving into this one. Now, let\u0026rsquo;s summarize the distinctions between the three pointer types.\nOrdinary pointers: They do not support pointer arithmetic, retain both address and type information, and the data they point to won\u0026rsquo;t be garbage collected. unsafe.Pointer: They do not support pointer arithmetic, retain the address but not the type information, and the data they point to won\u0026rsquo;t be garbage collected. uintptr: They support address arithmetic, retain the address but not the type information, and the data they point to will be garbage collected. This article was first published in the Medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nCompared to C, Go is designed as a strongly-typed statically-typed language for ease of writing, high efficiency, and reduced complexity. Strongly-typed means once defined, a type cannot be changed, while statically-typed means type checking is done before runtime.\nFor safety reasons, Go doesn\u0026rsquo;t allow conversion between two pointer types. To address this, Go introduces unsafe.Pointer. unsafe.Pointer is a special kind of pointer that can hold addresses of any type, akin to the void* pointer in C, being versatile.\nExample In the official documentation, we find four rules regarding unsafe.Pointer:\nAny pointer value of any type can be converted to a Pointer. A Pointer can be converted to a pointer value of any type. A uintptr can be converted to a Pointer. A Pointer can be converted to a uintptr. With unsafe.Pointer, we can achieve conversions between different types.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;reflect\u0026#34; \u0026#34;unsafe\u0026#34; ) //go:nosplit func convert(p *int) *int32 { return (*int32)(unsafe.Pointer(p)) } func main() { var i int = 1 q := convert(\u0026amp;i) fmt.Println(reflect.TypeOf(*q)) } Output:\n1 2 ➜ unsafe git:(main) ✗ go run main.go int32 We\u0026rsquo;ve successfully converted variables of different types using unsafe.Pointer.\nFor the third and fourth rules, let\u0026rsquo;s test them with a more complex example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;unsafe\u0026#34; ) type User struct { name string age int32 } func main() { user := \u0026amp;User{ name: \u0026#34;Tom\u0026#34;, age: 18, } fmt.Println(*user) pName := (*string)(unsafe.Pointer(user)) *pName = \u0026#34;Bob\u0026#34; age := (*int32)(unsafe.Pointer(uintptr(unsafe.Pointer(user)) + unsafe.Offsetof(user.age))) *age = 20 fmt.Println(*user) } Output:\n1 2 3 ➜ unsafe git:(main) ✗ go run main.go {Tom 18} {Bob 20} We\u0026rsquo;ve manipulated the values of different fields by offsetting in memory. Firstly, when modifying the name field of user, since name is the first field, no offset is required. We obtain the pointer to user, then use unsafe.Pointer to convert it to *string for the assignment. Secondly, when modifying the age field of user, we need an offset because age isn\u0026rsquo;t the first field. We convert the pointer address of user to uintptr, then use unsafe.Offsetof(u.age) to get the offset needed, perform address arithmetic (+) to offset. Now, the address points to the age field of user, and to assign it, we need to convert uintptr to *int32. Thus, by converting uintptr to unsafe.Pointer and then to *int32, we can proceed with the operation. Ultimately, we\u0026rsquo;ve achieved pointer operations akin to C in Go.\nConclusion unsafe is indeed unsafe, hence it should be used sparingly, especially in memory manipulation. This circumvents Go\u0026rsquo;s built-in safety mechanisms and improper operations could lead to memory corruption, which can be very challenging to debug.\nunsafe.Pointer is mainly used during Go\u0026rsquo;s compilation phase. Because it allows bypassing the type system to directly access memory, it offers higher efficiency. If we decompile the convert function, we\u0026rsquo;d find that it merely changes the MOVQ instruction to MOVL, without introducing any additional instructions.\n","date":"2024-06-27T16:42:06+08:00","permalink":"https://huizhou92.com/p/decryption-gounsafe.pointer/","title":"Decryption go：unsafe.Pointer"},{"content":"Why is Panic Worth Thinking About? When learning Go, many questions often arise. Sometimes, what seems to be understood is actually not. What exactly is panic? It seems obvious, but it can be difficult to explain. I\u0026rsquo;m going to use two articles to understand the concept of panic thoroughly:\nPosture: Understanding the origins of panic. It doesn\u0026rsquo;t just come out of nowhere. There are three main postures to consider. Principles: Fully comprehending the internal workings of panic and understanding its deeper principles. This article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nThe Three Postures of Panic When does panic occur? Let\u0026rsquo;s start with the \u0026ldquo;form.\u0026rdquo; From a developer\u0026rsquo;s perspective, panic can be categorized into active and passive.\nActive posture:\nDevelopers actively call the panic() function. Passive posture:\nThe compiler triggers hidden code. The kernel sends a signal to the process. Hidden Code by the Compiler Go is simple yet powerful, and the compiler plays a crucial role. It handles many tasks on behalf of programmers, such as logic supplementation and memory escape analysis. This includes the throwing of panics!\nLet\u0026rsquo;s take a classic example: dividing by zero in integer arithmetic triggers a panic. How does this happen?\nConsider the following minimal code snippet:\n1 2 3 4 func divzero(a, b int) int { c := a / b return c } This function has a risk of division by zero. When b is equal to 0, the program triggers a panic and exits, as shown below:\n1 root@ubuntu:~/code/gopher/src/panic# ./test_zero panic: runtime error: integer divide by zero goroutine 1 [running]: main.zero(0x64, 0x0, 0x0) /root/code/gopher/src/panic/test_zero.go:6 +0x52 Now, the question is: How does the program trigger a panic?\nCode reveals all secrets.\nLooking at the code, it seems simple—just one line: c := a / b, right?\nWell, it\u0026rsquo;s actually assembly code. The hidden logic added by the compiler is not visible in the source code.\nBy using the dlv debugger to set a breakpoint in the divzero function and executing disassemble, we can uncover the secret. Here is a snippet of the assembly code with annotations:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 (dlv) disassemble TEXT main.zero(SB) /root/code/gopher/src/panic/test_zero.go // Check if b is equal to 0 test_zero.go:6 0x4aa3c1 4885c9 test rcx, rcx // If it\u0026#39;s not equal to 0, jump to 0x4aa3c8 and execute the instruction; otherwise, continue execution test_zero.go:6 0x4aa3c4 7502 jnz 0x4aa3c8 // If execution reaches this point, it means b is 0, so jump to 0x4aa3ed, which is call $runtime.panicdivide =\u0026gt; test_zero.go:6 0x4aa3c6 eb25 jmp 0x4aa3ed test_zero.go:6 0x4aa3c8 4883f9ff cmp rcx, -0x1 test_zero.go:6 0x4aa3cc 7407 jz 0x4aa3d5 test_zero.go:6 0x4aa3ce 4899 cqo test_zero.go:6 0x4aa3d0 48f7f9 idiv rcx // ... test_zero.go:7 0x4aa3ec c3 ret // See the magical function? test_zero.go:6 0x4aa3ed e8ee27f8ff call $runtime.panicdivide Do you see the hidden function?\nThe compiler secretly adds an if/else logic and even includes the code for runtime.panicdivide.\nIf b is equal to 0, it jumps to the function runtime.panicdivide.\nTake a look at the panicdivide function. It\u0026rsquo;s a simplified wrapper: 1 2 3 4 5 // runtime/panic.go func panicdivide() { panicCheck2(\u0026#34;integer divide by zero\u0026#34;) panic(divideError) } As you can see, it calls the panic() function.\nThis is how a panic is triggered when dividing by zero. It\u0026rsquo;s not something that magically appears out of nowhere; rather, it\u0026rsquo;s the additional logic added by the compiler to ensure that a panic is triggered when the divisor is 0.\nTriggered by Process Signal The most typical example is illegal memory access, such as accessing a nil pointer, which triggers a panic. How does this happen?\nConsider this minimal example:\n1 2 3 4 func nilptr(b *int) int { c := *b return c } When calling nilptr(nil), it will cause the process to exit with an exception:\n1 root@ubuntu:~/code/gopher/src/panic# ./test_nil panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x4aa3bc] goroutine 1 [running]: main.nilptr(0x0, 0x0) /root/code/gopher/src/panic/test_nil.go:6 +0x1c Now, the question is: How does this panic occur?\nWhen the Go process starts, it registers the default signal handler, sigtramp. When the CPU encounters a 0 address, it triggers a page fault exception, indicating an illegal address. The kernel sends a SIGSEGV signal to the process. When this signal is received, the sigtramp function handles it, ultimately calling the panic() function:\n1 2 3 4 5 6 7 sigtramp (pure assembly code) -\u0026gt; sigtrampgo (signal_unix.go) -\u0026gt; sighandler (signal_sighandler.go) -\u0026gt; preparePanic (signal_amd64x.go) -\u0026gt; sigpanic (signal_unix.go) -\u0026gt; panicmem -\u0026gt; panic In the sigpanic function, the panicmem function is called, which then calls the panic() function. This leads to Go\u0026rsquo;s panic handling.\nSimilar to panicdivide, panicmem is a minimal wrapper for panic():\n1 2 3 4 func panicmem() { panicCheck2(\u0026#34;invalid memory address or nil pointer dereference\u0026#34;) panic(memoryError) } This method triggers the panic function by using a software interrupt via a signal. It allows the Go registered signal handler to be invoked, enabling panic handling at the language level.\nYou may wonder when the logic for signal handling is registered.\nDuring process initialization, when creating M0 (thread), the system call sigaction is used to register the signal handling function as sigtramp. The call stack looks like this:\n1 2 3 mstartm0 (proc.go) -\u0026gt; initsig (signal_unix.go:113) -\u0026gt; setsig (os_linux.go) As a result, when a software interrupt is triggered, the Go signal handling function is called, allowing panic handling at the language level.\nActive Panic by Developers The third way is when developers actively call panic themselves:\n1 2 3 func main() { panic(\u0026#34;panic test\u0026#34;) } This is a simple function call—very straightforward.\nDiscussing the Essence of Panic Now that we have explored the postures of panic, all three methods ultimately rely on the panic() function. So, one thing is clear: panic is a language-level handling mechanism.\nBy default, after a panic occurs, if Go doesn\u0026rsquo;t handle it in any specific way, the default behavior is to print the reason for the panic, print the stack trace, and exit the program.\nNow, let\u0026rsquo;s go back to the fundamental question: What exactly is panic?\nI won\u0026rsquo;t delve into the concept, but I\u0026rsquo;ll describe a few simple facts:\nThe panic() function internally creates a crucial data structure called _panic and associates it with the goroutine. The panic() function executes the _defer function chain and handles the state of _panic accordingly. What does it mean to handle _panic?\nIt means looping through the _defer function chain on the goroutine. If all the _defer functions are executed and the state of _panic hasn\u0026rsquo;t been recovered, there is no other option but to exit the process and print the stack trace.\nIf a friend on the _defer chain recovers the state of _panic, marking it as recovered, the process ends there. The normal code execution continues after that, following the deferreturn logic.\nSo, what is panic?\nIt\u0026rsquo;s just a special function call. That\u0026rsquo;s all it is.\nHow special is it? I will explore its deep principles in the next article. In the meantime, consider a few questions:\nWhat exactly is panic? Is it a structure or a function? Why does panic cause a Go process to exit? Why does recover need to be placed within a defer to work? Why does the process still not recover even when recover is placed within a defer? Why is it possible to panic again after a panic? What are the consequences? Summary Panic can occur in three ways: it can be actively triggered by developers, assisted by the compiler\u0026rsquo;s logic, or triggered by a software interrupt signal. Regardless of the posture, all methods ultimately rely on the panic() function. Panic is merely a language-level handling mechanism. By default, after a panic occurs, if not handled, the program prints the panic cause, the stack trace, and exits the process. ","date":"2024-06-27T15:45:59+08:00","image":"https://images.yixiao9206.cn/blog-images/2024/06/06b79c29b3273fa4c8fe8e96e0e0a603.png","permalink":"https://huizhou92.com/p/decrypt-go-three-reasons-for-panic/","title":"Decrypt Go: Three reasons for panic"},{"content":"Recently, I stumbled upon an intriguing website showcasing numerous web cameras operated by the Japanese government. Surprisingly, these cameras offer live feeds by refreshing a single image every 0.1 seconds. Initially, I thought this was a sign of governmental inefficiency or outdated IT infrastructure in Japan. However, upon further reflection, I realized that the underlying reasons may not be so simple.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nBoth Europe and the United States have long-standing systems, some of which have been running for decades. Similarly, these Japanese government web cameras were set up over 20 years ago, during a time when 56k dial-up and ADSL were the standard. Given the limited bandwidth of that era, using a 0.1-second image refresh to create a live feed was quite practical. The fact that this system has operated smoothly for over two decades speaks volumes about its stability.\nTechnological Progress and Practical Compatibility In the span of just 30 years, we’ve witnessed massive transformations from the personal computer era to the mobile internet age. Consider the 1994 release of the original “The Need for Speed”:\nNow compare it to the latest “Need For Speed: Unbound,” which features near-photorealistic graphics. This is a testament to the triumph of technological advancement.\nAs a tech professional, I initially assumed that government public service webcams should at least offer 1080p high-bitrate video or even VR capabilities. But is this really necessary?\nThe Human Touch in Tech Choices Only a small fraction of the global population uses the latest smartphones. Even if Apple sells 100 million iPhones a year, the total number in use might be around 500 million. At least 60% of people use devices that can’t handle high-definition video due to display and network limitations. The Japanese government’s seemingly clunky method of refreshing images every 0.1 seconds actually boasts the broadest compatibility, supporting devices from Windows 98 to the latest MacBook Pro. This approach ensures the widest reach.\nThe Value and Longevity of Older Devices Over the past 30 years, we’ve accumulated many infrastructure and personal devices. This is a valuable asset. While new devices are more advanced, considering the compatibility of older devices when developing new products can extend their lifespan and avoid unnecessary waste.\nThe Practical Benefits of Low-Tech Solutions Consider a train station in Niigata, Japan, which displays its departure times via a web camera focused on the station\u0026rsquo;s schedule board. This approach might seem “low-tech,” but it has numerous practical advantages:\nLow Cost: No need for complex systems or apps; maintenance is simple and cost-effective. Wide Accessibility: Even basic 2G internet and older phones can display the schedule, ensuring information is widely accessible. Advanced Isn’t Always Better When choosing technology for projects, practical considerations often outweigh pursuing the latest advancements. In many cases, the most successful projects are those that use the minimum viable product (MVP) approach to meet essential needs:\nUser Needs: Understand the target user’s requirements and device capabilities to select the most appropriate technology. Cost-Effectiveness: Evaluate the cost-benefit ratio and choose the most economically sensible technology. Sustainability: Consider the long-term sustainability and maintenance costs to ensure stable, ongoing operations. Conclusion While technological advancements offer exciting possibilities, selecting the most suitable—not necessarily the most advanced—technology often yields the best results. This approach best uses existing resources and demonstrates a commitment to practicality and sustainability.\n","date":"2024-06-23T09:54:05+08:00","permalink":"https://huizhou92.com/p/innovation-and-tradition-using-images-instead-of-video-streams-for-live-feeds/","title":"Innovation and Tradition: Using Images Instead of Video Streams for Live Feeds"},{"content":"What\u0026rsquo;s Consistent Hashing First, let me quote a definition from Wikipedia:\nIn computer science, consistent hashing is a special kind of hashing technique such that when a hash table is resized, only 𝑛/𝑚 keys need to be remapped on average where 𝑛 is the number of keys and 𝑚 is the number of slots. In contrast, in most traditional hash tables, a change in the number of array slots causes nearly all keys to be remapped because the mapping between the keys and the slots is defined by a modular operation.\nConsistent hashing evenly distributes cache keys across shards, even if some of the shards crash or become unavailable.\nIn distributed systems, consistency hash is everywhere. CDN, KV, load balancing, and other places have their shadows. Consistency hash is one of the cornerstone algorithms of distributed systems. It has the following advantages.\nBalanced load: Consistency hash algorithm can evenly distribute data on nodes. Scalability: In the consistency hash algorithm, only part of the data needs to be re-mapped when the number of nodes increases or decreases. It is easier for the system to expand horizontally, and the number of nodes can be increased to meet greater load requirements; Reduce data migration: Compared with the traditional hash algorithm, the consistent hash algorithm has less data that needs to be re-mapped when nodes increase or decrease, which can greatly reduce the cost of data migration and reduce the instability and delay of the system; This article aims to learn the consistency hash algorithm and its simple implementation.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nPrinciple of Consistent Hashing Algorithm Basic Consistent Hashing Algorithm The most basic consistent hashing algorithm is to distribute nodes directly on a ring, thus dividing the value range. After the key goes through hash(x), it falls into different value ranges and is processed by the corresponding node. The most common size of the value range space is: 2^32 - 1. Nodes fall into this space to divide the value ranges belonging to different nodes. As shown in the figure.\nNode\nThe hash range stored by Node A is [0,2^12).\nThe hash range stored by Node B is [2^12,2^28).\nThe hash range stored by Node C is [2^28,0).\nThe basic consistent hashing algorithm mentioned above has obvious drawbacks:\nThe random distribution of nodes makes it difficult to distribute the hash value domain evenly. As can be seen from above, the data stored by the three nodes is uneven. After dynamically adding nodes, it is difficult to continue ensuring uniformity even if the original distribution was uniform. One serious drawback caused by adding or removing nodes is: When a node becomes abnormal, all its load will be transferred to an adjacent node. When a new node joins, it can only share the load with one adjacent node. Virtual Nodes Rob Pike said: \u0026ldquo;In computer science, there are no problems that cannot be solved with another layer of indirection.\u0026rdquo; Consistent hashing follows this principle as well.\nIf the three nodes are imbalanced, we can virtualize them into N virtual nodes: A[a1,a2\u0026hellip;.a1024]. Then, we map them onto a hash ring in this way.\nEach virtual node has a corresponding hash range. It is responsible for a segment of keys and then reads and writes data based on the virtual node\u0026rsquo;s name to find the corresponding physical node.\nThe above three problems are perfectly solved with the introduction of virtual nodes.\nAs long as we have enough virtual nodes, the data of each node can be balanced (⚠️: this comes at a cost in engineering). If a node goes down, its data will be evenly distributed among all nodes in the cluster. Similarly, newly added nodes can also handle the load of all nodes.\nGo language implementation Complete code\nFirst, define a hash_ring and use crc32.ChecksumIEEE as the default hash function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 type VirtualNode struct { Hash uint32 Node *Node } type Node struct { // Physics Node ID string Addr string } type HashRing struct { Nodes map[string]*Node VirtualNodes []VirtualNode. mu sync.Mutex hash HashFunc } func NewHashRing(hash HashFunc) *HashRing { if hash == nil { hash = crc32.ChecksumIEEE } return \u0026amp;HashRing{ Nodes: make(map[string]*Node), VirtualNodes: make([]VirtualNode, 0), hash: hash, } } Let\u0026rsquo;s take a look at how to add a physics node:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func (hr *HashRing) AddNode(node *Node) { hr.mu.Lock() defer hr.mu.Unlock() hr.Nodes[node.ID] = node for i := 0; i \u0026lt; VirtualNodesPerNode; i++ { virtualNodeID := fmt.Sprintf(\u0026#34;%s-%d\u0026#34;, node.ID, i) hash := hr.hash([]byte(virtualNodeID)) hr.VirtualNodes = append(hr.VirtualNodes, VirtualNode{Hash: hash, Node: node}) } sort.Slice(hr.VirtualNodes, func(i, j int) bool { return hr.VirtualNodes[i].Hash \u0026lt; hr.VirtualNodes[j].Hash }) } For each added node, the corresponding number of virtual nodes must be created, and it is necessary to ensure that the virtual nodes are ordered (so that they can be searched).\nSimilarly, when removing, virtual nodes also need to be deleted.\n1 2 3 4 5 6 7 8 9 10 11 12 13 func (hr *HashRing) RemoveNode(nodeID string) { hr.mu.Lock() defer hr.mu.Unlock() delete(hr.Nodes, nodeID) virtualNodes := make([]VirtualNode, 0) for _, vn := range hr.VirtualNodes { if vn.Node.ID != nodeID { virtualNodes = append(virtualNodes, vn) } } hr.VirtualNodes = virtualNodes } When querying, we first locate the corresponding virtual node, and then find the corresponding physical node based on the virtual node.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func (hr *HashRing) GetNode(key string) *Node { hr.mu.Lock() defer hr.mu.Unlock() if len(hr.VirtualNodes) == 0 { return nil } hash := hr.hash([]byte(key)) idx := sort.Search(len(hr.VirtualNodes), func(i int) bool { return hr.VirtualNodes[i].Hash \u0026gt;= hash }) if idx == len(hr.VirtualNodes) { idx = 0 } return hr.VirtualNodes[idx].Node } Finally, let\u0026rsquo;s take a look at how the business uses this hash_ring\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 type KVSystem struct { hashRing *HashRing kvStores map[string]*kvstorage.KVStore } func NewKVSystem(nodes int) *KVSystem { hashRing := NewHashRing(crc32.ChecksumIEEE) for i := 0; i \u0026lt; nodes; i++ { // init node node := \u0026amp;Node{ ID: fmt.Sprintf(\u0026#34;Node%d\u0026#34;, i), Addr: fmt.Sprintf(\u0026#34;192.168.1.%d\u0026#34;, i+1), } hashRing.AddNode(node) } kvStores := make(map[string]*kvstorage.KVStore) //init storage for id := range hashRing.Nodes { kvStores[id] = kvstorage.NewKVStore() } return \u0026amp;KVSystem{ hashRing: hashRing, kvStores: kvStores, } } func (kv *KVSystem) Get(key string) (string, bool) { //get value node := kv.hashRing.GetNode(key) return kv.kvStores[node.ID].Get(key) } func (kv *KVSystem) Set(key string, value string) { // set value node := kv.hashRing.GetNode(key) kv.kvStores[node.ID].Set(key, value) } func (kv *KVSystem) Delete(key string) { node := kv.hashRing.GetNode(key) kv.kvStores[node.ID].Delete(key) } // DeleteNode requires reallocating the data stored on the node. func (kv *KVSystem) DeleteNode(nodeID string) { allData := kv.kvStores[nodeID].GetAll() kv.hashRing.RemoveNode(nodeID) delete(kv.kvStores, nodeID) for key, value := range allData { kv.Set(key, value) } } func (kv *KVSystem) AddNode() { node := \u0026amp;Node{ ID: fmt.Sprintf(\u0026#34;Node%d\u0026#34;, len(kv.hashRing.Nodes)), Addr: fmt.Sprintf(\u0026#34;192.168.1.%d\u0026#34;, len(kv.hashRing.Nodes)+1), } kv.hashRing.AddNode(node) kv.kvStores[node.ID] = kvstorage.NewKVStore() } In this way, we have achieved the simplest key-value storage based on consistent hashing. Isn\u0026rsquo;t it very simple? But it supports the operation of our entire network world.\nLong Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2024-06-20T10:40:37+08:00","image":"https://images.yixiao9206.cn/blog-images/2024/06/f3b53488c2407a75d85406be58526010.png","permalink":"https://huizhou92.com/p/distributed-cornerstone-algorithm-consistent-hash/","title":"Distributed cornerstone algorithm: consistent hash"},{"content":"In Go, a normal struct typically occupies a block of memory. However, there\u0026rsquo;s a special case: if it\u0026rsquo;s an empty struct, its size is zero. How is this possible, and what is the use of an empty struct?\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type Test struct { A int B string } func main() { fmt.Println(unsafe.Sizeof(new(Test))) fmt.Println(unsafe.Sizeof(struct{}{})) } /* 8 0 */ The Secret of the Empty Struct Special Variable: Zerobase An empty struct is a struct with no memory size. This statement is correct, but to be more precise, it actually has a special starting point: the zerobase variable. This is a uintptr global variable that occupies 8 bytes. Whenever countless struct {} variables are defined, the compiler assigns the address of this zerobase variable. In other words, in Go, any memory allocation with a size of 0 uses the same address, \u0026amp;zerobase.\nExample\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package main import \u0026#34;fmt\u0026#34; type emptyStruct struct {} func main() { a := struct{}{} b := struct{}{} c := emptyStruct{} fmt.Printf(\u0026#34;%p\\n\u0026#34;, \u0026amp;a) fmt.Printf(\u0026#34;%p\\n\u0026#34;, \u0026amp;b) fmt.Printf(\u0026#34;%p\\n\u0026#34;, \u0026amp;c) } // 0x58e360 // 0x58e360 // 0x58e360 The memory addresses of variables of an empty struct are all the same. This is because the compiler assigns \u0026amp;zerobase during compilation when encountering this special type of memory allocation. This logic is in the mallocgc function:\n1 2 3 4 5 6 7 //go:linkname mallocgc func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer { ... if size == 0 { return unsafe.Pointer(\u0026amp;zerobase) } ... This is the secret of the Empty struct. With this special variable, we can accomplish many functionalities.\nEmpty Struct and Memory Alignment Typically, if an empty struct is part of a larger struct, it doesn\u0026rsquo;t occupy memory. However, there\u0026rsquo;s a special case when the empty struct is the last field; it triggers memory alignment.\nExample\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 type A struct { x int y string z struct{} } type B struct { x int z struct{} y string } func main() { println(unsafe.Alignof(A{})) println(unsafe.Alignof(B{})) println(unsafe.Sizeof(A{})) println(unsafe.Sizeof(B{})) } /** 8 8 32 24 **/ When a pointer to a field is present, the returned address may be outside the struct, potentially leading to memory leaks if the memory is not freed when the struct is released. Therefore, when an empty struct is the last field of another struct, additional memory is allocated for safety. If the empty struct is at the beginning or middle, its address is the same as the next variable.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 type A struct { x int y string z struct{} } type B struct { x int z struct{} y string } func main() { a := A{} b := B{} fmt.Printf(\u0026#34;%p\\n\u0026#34;, \u0026amp;a.y) fmt.Printf(\u0026#34;%p\\n\u0026#34;, \u0026amp;a.z) fmt.Printf(\u0026#34;%p\\n\u0026#34;, \u0026amp;b.y) fmt.Printf(\u0026#34;%p\\n\u0026#34;, \u0026amp;b.z) } /** 0x1400012c008 0x1400012c018 0x1400012e008 0x1400012e008 **/ Use Cases of the Empty Struct The core reason for the existence of the empty struct struct{} is to save memory. When you need a struct but don\u0026rsquo;t care about its contents, consider using an empty struct. Go\u0026rsquo;s core composite structures such as map, chan, and slice can all use struct{}.\nmap \u0026amp; struct{} 1 2 3 4 5 6 // Create map m := make(map[int]struct{}) // Assign value m[1] = struct{}{} // Check if key exists _, ok := m[1] chan \u0026amp; struct{} A classic scenario combines channel and struct{}, where struct{} is often used as a signal without caring about its content. As analyzed in previous articles, the essential data structure of a channel is a management structure plus a ring buffer. The ring buffer is zero-allocated if struct{} is used as an element.\nThe only use of chan and struct{} together is for signal transmission since the empty struct itself cannot carry any value. Generally, it\u0026rsquo;s used with no buffer channels.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // Create a signal channel waitc := make(chan struct{}) // ... goroutine 1: // Send signal: push element waitc \u0026lt;- struct{}{} // Send signal: close close(waitc) goroutine 2: select { // Receive signal and perform corresponding actions case \u0026lt;-waitc: } In this scenario, is struct{} absolutely necessary? Not really, and the memory saved is negligible. The key point is that the element value of chan is not cared about, hence struct{} is used.\nSummary An empty struct is still a struct, just with a size of 0. All empty structs share the same address: the address of zerobase. We can leverage the empty struct\u0026rsquo;s non-memory-occupying feature to optimize code, such as using maps to implement sets and channels. References The empty struct, Dave Cheney Go 最细节篇— struct{} 空结构体究竟是啥？ ","date":"2024-06-17T23:05:42+08:00","image":"https://images.yixiao9206.cn/blog-images/2024/06/c78d096394f1791445b7c10f88dd0378.jpg","permalink":"https://huizhou92.com/p/decrypt-go-empty-struct/","title":"Decrypt Go:  empty struct"},{"content":"Compare the performance, advantages, and disadvantages of fastjson, gjson, and jsonparser\nThis article delves into the analysis of how the standard library in Go parses JSON and then explores popular JSON parsing libraries, their characteristics, and how they can better assist us in development in different scenarios.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me on the medium. Thank you very much.\nI didn\u0026rsquo;t plan to look into the JSON library\u0026rsquo;s performance issue. However, recently, I did a pprof on my project and found from the flame graph below that more than half of the performance consumption in business logic processing is during JSON parsing. Therefore, this article came about.\nThis article delves into the analysis of how the standard library in Go parses JSON and then explores popular JSON parsing libraries, as well as their characteristics and how they can better assist us in development in different scenarios.\nMainly introduce the analysis of the following libraries (2024-06-13):\nlib Star JSON Unmarshal valyala/fastjson 2.2 k tidwall/gjson 13.8 k buger/jsonparser 5.4 k JSON Unmarshal 1 func Unmarshal(data []byte, v interface{}) \u0026ldquo;The official JSON parsing library requires two parameters: the object to be serialized and the type of this object. Before actually performing JSON parsing, reflect.ValueOf is called to obtain the reflection object of parameter v. Then, the method for parsing is determined based on the non-empty characters at the beginning of the incoming data object.\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func (d *decodeState) value(v reflect.Value) error { switch d.opcode { default: panic(phasePanicMsg) // array case scanBeginArray: ... // struct or map case scanBeginObject: ... // Literals, including int, string, float, etc. case scanBeginLiteral: ... } return nil } If the parsed object starts with [, it indicates that this is an array object and will enter the scanBeginArray branch; if it starts with {, it indicates that the parsed object is a struct or map, and then enters the scanBeginObject branch, and so on.\nSub Summary Looking at Unmarshal\u0026rsquo;s source code, it can be seen that a large amount of reflection is used to obtain field values. If the JSON is nested, recursive reflection is needed to obtain values. Thus, the performance can be imagined to be very poor.\nHowever, if performance is not highly valued, using it directly is actually a very good choice. It has complete functionality, and the official team has been continuously iterating and optimizing it. Maybe its performance will also make a qualitative leap in future versions. It should be the only one that can directly convert JSON objects into Go structs.\nfastjson The characteristic of this library is fast, just like its name suggests. Its introduction page says so:\nFast. As usual, up to 15x faster than the standard encoding/json.\nIts usage is also very simple, as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func main() { var p fastjson.Parser v, _ := p.Parse(`{ \u0026#34;str\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;int\u0026#34;: 123, \u0026#34;float\u0026#34;: 1.23, \u0026#34;bool\u0026#34;: true, \u0026#34;arr\u0026#34;: [1, \u0026#34;foo\u0026#34;, {}] }`) fmt.Printf(\u0026#34;foo=%s\\n\u0026#34;, v.GetStringBytes(\u0026#34;str\u0026#34;)) fmt.Printf(\u0026#34;int=%d\\n\u0026#34;, v.GetInt(\u0026#34;int\u0026#34;)) fmt.Printf(\u0026#34;float=%f\\n\u0026#34;, v.GetFloat64(\u0026#34;float\u0026#34;)) fmt.Printf(\u0026#34;bool=%v\\n\u0026#34;, v.GetBool(\u0026#34;bool\u0026#34;)) fmt.Printf(\u0026#34;arr.1=%s\\n\u0026#34;, v.GetStringBytes(\u0026#34;arr\u0026#34;, \u0026#34;1\u0026#34;)) } // Output: // foo=bar // int=123 // float=1.230000 // bool=true // arr.1=foo To use fastjson, first, give the JSON string to the Parser parser for parsing, and then retrieve it through the object returned by the Parse method. If it is a nested object, you can directly pass in the corresponding parent-child key when passing parameters to the Get method.\nAnalysis The design of fastjson differs from the standard library Unmarshal in that it divides JSON parsing into two parts: Parse and Get.\nParse is responsible for parsing the JSON string into a structure and returning it. Data is then retrieved from the returned structure. The Parse process is lock-free, so if you want to call Parse concurrently, you need to use ParserPool.\nfastjson processes JSON by traversing it from top to bottom, storing the parsed data in a Value structure:\n1 type Value struct { o Object a []*Value s string t Type } This structure is very simple:\no Object: Indicates that the parsed structure is an object. a []*Value: Indicates that the parsed structure is an array. s string: If the parsed structure is neither an object nor an array, other types of values are stored in this field as a string. t Type: Represents the type of this structure, which can be TypeObject, TypeArray, TypeString, TypeNumber, etc. 1 type Object struct { kvs []kv keysUnescaped bool } type kv struct { k string v *Value } This structure stores the recursive structure of objects. After parsing the JSON string in the example above, the resulting structure looks like this:\nCode In terms of implementation, the absence of reflection code makes the entire parsing process very clean. Let\u0026rsquo;s directly look at the main part of the parsing:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 func parseValue(s string, c *cache, depth int) (*Value, string, error) { if len(s) == 0 { return nil, s, fmt.Errorf(\u0026#34;cannot parse empty string\u0026#34;) } depth++ // The maximum depth of the json string cannot exceed MaxDepth if depth \u0026gt; MaxDepth { return nil, s, fmt.Errorf(\u0026#34;too big depth for the nested JSON; it exceeds %d\u0026#34;, MaxDepth) } // parse object if s[0] == \u0026#39;{\u0026#39; { v, tail, err := parseObject(s[1:], c, depth) if err != nil { return nil, tail, fmt.Errorf(\u0026#34;cannot parse object: %s\u0026#34;, err) } return v, tail, nil } // parse array if s[0] == \u0026#39;[\u0026#39; { ... } // parse string if s[0] == \u0026#39;\u0026#34;\u0026#39; { ... } ... return v, tail, nil } parseValue will determine the type to be parsed based on the first non-empty character of the string. Here, an object type is used for parsing:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func parseObject(s string, c *cache, depth int) (*Value, string, error) { ... o := c.getValue() o.t = TypeObject o.o.reset() for { var err error // 获取Ojbect结构体中的 kv 对象 kv := o.o.getKV() ... // 解析 key 值 kv.k, s, err = parseRawKey(s[1:]) ... // 递归解析 value 值 kv.v, s, err = parseValue(s, c, depth) ... // 遇到 ，号继续往下解析 if s[0] == \u0026#39;,\u0026#39; { s = s[1:] continue } // 解析完毕 if s[0] == \u0026#39;}\u0026#39; { return o, s[1:], nil } return nil, s, fmt.Errorf(\u0026#34;missing \u0026#39;,\u0026#39; after object value\u0026#34;) } } The parseObject function is also very simple. It will get the key value in the loop, and then call the parseValue function recursively to parse the value from top to bottom, parsing JSON objects one by one until encountering } at last.\nSub Summary Through the above analysis, it can be seen that fastjson is much simpler in implementation and has higher performance than the standard library. After using Parse to parse the JSON tree, it can be reused multiple times, avoiding the need for repeated parsing and improving performance.\nHowever, its functionality is very rudimentary and lacks common operations such as JSON to struct or JSON to map conversion. If you only want to simply retrieve values from JSON, then using this library is very convenient. But if you want to convert JSON values into a structure, you will need to manually set each value yourself.\nGJSON In my test, although the performance of GJSON is not as extreme as fastjson, its functionality is very complete and its performance is also quite OK. Next, let me briefly introduce the functionality of GJSON.\nThe usage of GJSON is similar to fastjson, it is also very simple. Just pass in the JSON string and the value that needs to be obtained as parameters.\n1 2 json := `{\u0026#34;name\u0026#34;:{\u0026#34;first\u0026#34;:\u0026#34;li\u0026#34;,\u0026#34;last\u0026#34;:\u0026#34;dj\u0026#34;},\u0026#34;age\u0026#34;:18}` lastName := gjson.Get(json, \u0026#34;name.last\u0026#34;) In addition to this function, simple fuzzy matching can also be performed. It supports wildcard characters * and ? in the key. * matches any number of characters, while ? matches a single character, as follows:\n1 2 3 4 5 6 7 json := `{ \u0026#34;name\u0026#34;:{\u0026#34;first\u0026#34;:\u0026#34;Tom\u0026#34;, \u0026#34;last\u0026#34;: \u0026#34;Anderson\u0026#34;}, \u0026#34;age\u0026#34;: 37, \u0026#34;children\u0026#34;: [\u0026#34;Sara\u0026#34;, \u0026#34;Alex\u0026#34;, \u0026#34;Jack\u0026#34;] }` fmt.Println(\u0026#34;third child*:\u0026#34;, gjson.Get(json, \u0026#34;child*.2\u0026#34;)) fmt.Println(\u0026#34;first c?ild:\u0026#34;, gjson.Get(json, \u0026#34;c?ildren.0\u0026#34;)) child*.2: First, child* matches children, .2 reads the third element; c?ildren.0: c?ildren matches children, .0 reads the first element; In addition to fuzzy matching, it also supports modifier operations.\n1 2 3 4 5 6 json := `{ \u0026#34;name\u0026#34;:{\u0026#34;first\u0026#34;:\u0026#34;Tom\u0026#34;, \u0026#34;last\u0026#34;: \u0026#34;Anderson\u0026#34;}, \u0026#34;age\u0026#34;: 37, \u0026#34;children\u0026#34;: [\u0026#34;Sara\u0026#34;, \u0026#34;Alex\u0026#34;, \u0026#34;Jack\u0026#34;] }` fmt.Println(\u0026#34;third child*:\u0026#34;, gjson.Get(json, \u0026#34;children|@reverse\u0026#34;)) children|@reverse 先读取数组children，然后使用修饰符@reverse翻转之后返回，输出。\n1 nestedJSON := `{\u0026#34;nested\u0026#34;: [\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, [\u0026#34;three\u0026#34;, \u0026#34;four\u0026#34;]]}` fmt.Println(gjson.Get(nestedJSON, \u0026#34;nested|@flatten\u0026#34;)) @flatten flattens the inner array of array nested to the outer array and returns:\n1 [\u0026#34;one,\u0026#34; \u0026#34;two,\u0026#34; \u0026#34;three,\u0026#34; \u0026#34;four\u0026#34;] There are some other interesting features, you can check the official documentation.\nAnalysis The Get method parameter of GJSON is composed of two parts, one is a JSON string, and the other is called Path, which represents the matching path of the JSON value to be obtained.\nIn GJSON, because it needs to meet many definitions of parsing scenarios, the parsing is divided into two parts. You need to parse the Path before traversing the JSON string.\nIf you encounter a value that can be matched during the parsing process, it will be returned directly, and there is no need to continue to traverse down. If multiple values are matched, the whole JSON string will be traversed all the time. If you encounter a Path that cannot be matched in the JSON string, you also need to traverse the complete JSON string.\nIn the process of parsing, the content of parsing will not be saved in a structure like fastjson, which can be used repeatedly. So when you call GetMany to return multiple values, you actually need to traverse the JSON string many times, so the efficiency will be relatively low.\nIt\u0026rsquo;s important to be aware that when using the @flatten function to parse JSON, it won\u0026rsquo;t be validated. This means that even if the input string is not a valid JSON, it will still be parsed. Therefore, it\u0026rsquo;s essential for users to double-check that the input is indeed a valid JSON to avoid any potential issues.\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 func Get(json, path string) Result { // 解析 path if len(path) \u0026gt; 1 { ... } var i int var c = \u0026amp;parseContext{json: json} if len(path) \u0026gt;= 2 \u0026amp;\u0026amp; path[0] == \u0026#39;.\u0026#39; \u0026amp;\u0026amp; path[1] == \u0026#39;.\u0026#39; { c.lines = true parseArray(c, 0, path[2:]) } else { // Parse according to different objects, and loop here until \u0026#39;{\u0026#39; or \u0026#39;[\u0026#39; is found for ; i \u0026lt; len(c.json); i++ { if c.json[i] == \u0026#39;{\u0026#39; { i++ parseObject(c, i, path) break } if c.json[i] == \u0026#39;[\u0026#39; { i++ parseArray(c, i, path) break } } } if c.piped { res := c.value.Get(c.pipe) res.Index = 0 return res } fillIndex(json, c) return c.value } In the Get method, you can see a long code string used to parse various paths. Then, a for loop continuously traverses JSON until it finds \u0026lsquo;{\u0026rsquo; or \u0026lsquo;[\u0026rsquo; before performing the corresponding logic processing.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 func parseObject(c *parseContext, i int, path string) (int, bool) { var pmatch, kesc, vesc, ok, hit bool var key, val string rp := parseObjectPath(path) if !rp.more \u0026amp;\u0026amp; rp.piped { c.pipe = rp.pipe c.piped = true } // Nest two for loops to find the key value for i \u0026lt; len(c.json) { for ; i \u0026lt; len(c.json); i++ { if c.json[i] == \u0026#39;\u0026#34;\u0026#39; { i++ var s = i for ; i \u0026lt; len(c.json); i++ { if c.json[i] \u0026gt; \u0026#39;\\\\\u0026#39; { continue } // Find the key value and jump to parse_key_string_done if c.json[i] == \u0026#39;\u0026#34;\u0026#39; { i, key, kesc, ok = i+1, c.json[s:i], false, true goto parse_key_string_done } ... } key, kesc, ok = c.json[s:], false, false // break parse_key_string_done: break } if c.json[i] == \u0026#39;}\u0026#39; { return i + 1, false } } if !ok { return i, false } // Check whether it is a fuzzy match if rp.wild { if kesc { pmatch = match.Match(unescape(key), rp.part) } else { pmatch = match.Match(key, rp.part) } } else { if kesc { pmatch = rp.part == unescape(key) } else { pmatch = rp.part == key } } // parse value hit = pmatch \u0026amp;\u0026amp; !rp.more for ; i \u0026lt; len(c.json); i++ { switch c.json[i] { default: continue case \u0026#39;\u0026#34;\u0026#39;: i++ i, val, vesc, ok = parseString(c.json, i) if !ok { return i, false } if hit { if vesc { c.value.Str = unescape(val[1 : len(val)-1]) } else { c.value.Str = val[1 : len(val)-1] } c.value.Raw = val c.value.Type = String return i, true } case \u0026#39;{\u0026#39;: if pmatch \u0026amp;\u0026amp; !hit { i, hit = parseObject(c, i+1, rp.path) if hit { return i, true } } else { i, val = parseSquash(c.json, i) if hit { c.value.Raw = val c.value.Type = JSON return i, true } } ... break } } return i, false } In reviewing the parseObject code, the intention was not to teach JSON parsing or string traversal but to illustrate a bad-case scenario. The nested for loops and consecutive if statements can be overwhelming and may remind you of a colleague\u0026rsquo;s code you\u0026rsquo;ve encountered at work.\nSub Summary Advantages:\nPerformance: jsonparser performs relatively well compared to the standard library. Flexibility: It offers various retrieval methods and customizable return values, making it very convenient. Disadvantages:\nNo JSON Validation: It does not check for the correctness of the JSON input. Code Smell: The code structure is cumbersome and hard to read, which can make maintenance challenging. Note When parsing JSON to retrieve values, the GetMany function will traverse the JSON string multiple times based on the specified keys. Converting the JSON to a map can reduce the number of traversals.\nConclusion While jsonparser has notable performance and flexibility, its lack of JSON validation and complex, hard-to-read code structure present significant drawbacks. If you need to parse JSON and retrieve values frequently, consider the trade-offs between performance and code maintainability.\njsonparser Analysis jsonparser also processes an input JSON byte slice and allows for quickly locating and returning values by passing multiple keys.\nSimilar to GJSON, jsonparser does not cache the parsed JSON string in a data structure as fastjson does. However, when multiple values need to be parsed, the EachKey function can be used to parse multiple values in a single pass through the JSON string.\nIf a matching value is found, jsonparser returns immediately without further traversal. For multiple matches, it traverses the entire JSON string. If a path does not match any value in the JSON string, it still traverses the entire string.\njsonparser reduces the use of recursion by employing loops during JSON traversal, thus decreasing the call stack depth and enhancing performance.\nIn terms of functionality, ArrayEach, ObjectEach, and EachKey functions allow for passing a custom function to meet specific needs, greatly enhancing the utility of jsonparser.\nThe code for jsonparser is straightforward and clear, making it easy to analyze. Those interested can examine it themselves.\nSub Summary The high performance of jsonparser compared to the standard library can be attributed to:\nUsing for loops to minimize recursion. Avoid the use of reflection, unlike the standard library. Exiting immediately upon finding the corresponding key value without further recursion. Operating on the passed-in JSON string without allocating new space, thus reducing memory allocations. Additionally, the API design is highly practical. Functions like ArrayEach, ObjectEach, and EachKey allow for passing custom functions, solving many issues in actual business development.\nHowever, jsonparser has a significant drawback: it does not validate JSON. If the input is not valid JSON, jsonparser will not detect it.\nPerformance Comparison Parsing Small JSON Strings Parsing a simple JSON string of approximately 190 bytes\nLibrary Operation Time per Iteration Memory Usage Memory Allocations Performance Standard Library Parse to map 724 ns/op 976 B/op 51 allocs/op Slow Parse to struct 297 ns/op 256 B/op 5 allocs/op Average fastjson get 68.2 ns/op 0 B/op 0 allocs/op Fastest parse 35.1 ns/op 0 B/op 0 allocs/op Fastest GJSON Convert to map 255 ns/op 1009 B/op 11 allocs/op Average get 232 ns/op 448 B/op 1 allocs/op Average jsonparser get 106 ns/op 232 B/op 3 allocs/op Fast Parsing Medium JSON Strings Parsing a JSON string of moderate complexity, approximately 2.3KB\nLibrary Operation Time per Iteration Memory Usage Memory Allocations Performance Standard Library Parse to map 4263 ns/op 10212 B/op 208 allocs/op Slow Parse to struct 4789 ns/op 9206 B/op 259 allocs/op Slow fastjson get 285 ns/op 0 B/op 0 allocs/op Fastest parse 302 ns/op 0 B/op 0 allocs/op Fastest GJSON Convert to map 2571 ns/op 8539 B/op 83 allocs/op Average get 1489 ns/op 448 B/op 1 allocs/op Average jsonparser get 878 ns/op 2728 B/op 5 allocs/op Fast Parsing Large JSON Strings Parsing a JSON string of high complexity, approximately 2.2MB\nLibrary Operation Time per Iteration Memory Usage Memory Allocations Performance Standard Library Parse to map 2292959 ns/op 5214009 B/op 95402 allocs/op Slow Parse to struct 1165490 ns/op 2023 B/op 76 allocs/op Average fastjson get 368056 ns/op 0 B/op 0 allocs/op Fast parse 371397 ns/op 0 B/op 0 allocs/op Fast GJSON Convert to map 1901727 ns/op 4788894 B/op 54372 allocs/op Average get 1322167 ns/op 448 B/op 1 allocs/op Average jsonparser get 233090 ns/op 1788865 B/op 376 allocs/op Fastest Summary During this comparison, I analyzed several high-performance JSON parsing libraries. It was evident that these libraries share several common characteristics:\nThey avoid using reflection. They parse JSON by traversing the bytes of the JSON string sequentially. They minimize memory allocation by directly parsing the input JSON string. They sacrifice some compatibility for performance. Despite these trade-offs, each library offers unique features. The fastjson API is the simplest to use; GJSON offers fuzzy searching capabilities and high customizability; jsonparser supports inserting callback functions during high-performance parsing, providing a degree of convenience.\nFor my use case, which involves simply parsing certain fields from HTTP response JSON strings with predetermined fields and occasional custom operations, jsonparser is the most suitable tool.\nTherefore, if performance concerns you, consider selecting a JSON parser based on your business requirements.\nReference https://github.com/buger/jsonparser\nhttps://github.com/tidwall/gjson\nhttps://github.com/valyala/fastjson\nhttps://github.com/json-iterator/go\nhttps://github.com/mailru/easyjson\nhttps://github.com/Jeffail/gabs\nhttps://github.com/bitly/go-simplejson\n","date":"2024-06-13T17:10:25+08:00","permalink":"https://huizhou92.com/p/analyze-various-high-performance-json-parsing-libraries-in-go./","title":"Analyze various high-performance JSON parsing libraries in Go."},{"content":"Last week, Go 1 .23 entered the freeze period, meaning no new features will be added, and any already added features are unlikely to be removed. This is a great opportunity to preview the upcoming changes. In this article, Let’s learn about the new iter package.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nIn Go 1.22, the range over func experimental feature was introduced, but it needed to be enabled by the parameter GOEXPERIMENT=rangefunc. In Go 1.23, this kind of iteration can be directly implemented with code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func Backward(s []string) func(yield func(string) bool) { return func(yield func(string) bool) { for i := len(s) - 1; i \u0026gt;= 0; i-- { yield(strings.ToUpper(s[i])) } } } ​ func ToUpperByIter() { sl := []string{\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;, \u0026#34;golang\u0026#34;} for v := range Backward(sl) { // do business } } yield is a conventional name for callable functions passed into iterators.\nNow, let’s consider how we would write the code to achieve the same functionality without using the iter package:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func Convert[S any, D any](src []S, mapFn func(s S) D) []D { r := make([]D, 0, len(src)) for _, i := range src { r = append(r, mapFn(i)) } return r } func ToUpByString() { sl := []string{\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;, \u0026#34;golang\u0026#34;} s0 := Convert(sl, func(v string) string { return strings.ToUpper(v) }) for _, v := range s0 { // do business } } Performance Comparison Let’s compare the performance of the two methods:\n1 2 3 4 5 6 7 8 9 10 11 12 13 ➜ huizhou92 git:(master) ✗ go test -bench . -count=3 goos: darwin goarch: arm64 pkg: huizhou92 cpu: Apple M1 Pro BenchmarkToUpByString-10 8568332 128.7 ns/op BenchmarkToUpByString-10 9310351 128.6 ns/op BenchmarkToUpByString-10 9344986 128.5 ns/op BenchmarkToUpByIter-10 12440120 96.22 ns/op BenchmarkToUpByIter-10 12436645 96.25 ns/op BenchmarkToUpByIter-10 12371175 96.64 ns/op PASS ok huizhou92 8.162s The result is clear: ToUpperByIter performs better because it doesn\u0026rsquo;t reallocate a new slice, making it more efficient than the previous method.\nThe goal of iter The iter package aims to provide a unified and efficient iteration method. It offers a standard iteration interface for custom container classes (especially after the introduction of generics) and can replace some existing APIs that return slices. By using iterators and leveraging compiler optimization, performance can be improved. Additionally, it provides a standard iteration mechanism suitable for functional programming styles.\nThe Use of iter iter supports two types of iterators:\n1 2 3 4 5 6 7 8 9 // Seq is an iterator over sequences of individual values. // When called as seq(yield), seq calls yield(v) for each value v in the sequence, // stopping early if yield returns false. type Seq[V any] func(yield func(V) bool) // Seq2 is an iterator over sequences of pairs of values, most commonly key-value pairs. // When called as seq(yield), seq calls yield(k, v) for each pair (k, v) in the sequence, // stopping early if yield returns false. type Seq2[K, V any] func(yield func(K, V) bool) The map package has already been used iter to add methods such as All and Keys. Here is a reference to its implementation:\n​\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 //https://go.googlesource.com/go/blob/c83b1a7013784098c2061ae7be832b2ab7241424/src/maps/iter.go#L12 // All returns an iterator over key-value pairs from m. // The iteration order is not specified and is not guaranteed // to be the same from one call to the next. func All[Map ~map[K]V, K comparable, V any](m Map) iter.Seq2[K, V] { return func(yield func(K, V) bool) { for k, v := range m { if !yield(k, v) { return } } } } // Keys returns an iterator over keys in m. // The iteration order is not specified and is not guaranteed // to be the same from one call to the next. func Keys[Map ~map[K]V, K comparable](m Map) iter.Seq[K] { return func(yield func(K) bool) { for k := range m { if !yield(k) { return } } } } Community Opinions Photo by Ana Flávia on Unsplash\n“In my opinion, yield is a complicated enough concept to cause a lot of bad, incomprehensible code to appear. This suggestion provides only a syntax sugar for writing something that is already more than possible in the language. I believe this goes against the rule of _One problem - one solution_. Please, let Go stay boring.\u0026quot; Source\nThis is a common objection within the community. yield is not easy to understand, and we can implement iterators in many ways.\nConclusion I support the addition of iter.\nThe iter package offers numerous possibilities for developers aiming to streamline their code and adopt more functional programming practices. However, its reception is mixed due to concerns about performance, complexity, and the learning curve.\nAs with any new tool, the key is to balance its use where it offers clear benefits while remaining mindful of the potential drawbacks. The Go community will undoubtedly continue to explore and debate the best ways to harness iter\u0026rsquo;s power without compromising the language\u0026rsquo;s foundational principles.\nReference 61405 56413 iterators_in_go_123 ","date":"2024-06-08T19:41:11+08:00","image":"https://images.yixiao9206.cn/blog-images/2024/06/91f640dd323a289717b83dd1c1a3c38c.png","permalink":"https://huizhou92.com/p/go-1.23-new-iter-package/","title":"Go 1.23: new Iter package"},{"content":"Last week, Go 1 .23 entered the freeze period, meaning no new features will be added, and any already added features are unlikely to be removed. This is a great opportunity to preview the upcoming changes.\nIn this article, we will introduce the new package unique\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nAccording to wikipedia, interning is to reuse objects with equal values on demand. The technology reduces the action of creating new objects. This creation mode is often used for numbers and strings in different programming languages, which can avoid the overhead of unnecessary repeated allocation of objects.\nUnique referred to go4.org/intern, moved it to the official library, and made corresponding changes. Issue #62483\nAs described by the official documentation, the unique package provides a lightweight (only eight bytes) implementation of comparing two variables for equality. For example, in the following code:\nThe performance improvement is still obvious\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ➜ unique git:(master) ✗ /Users/hxzhouh/workspace/googlesource/go/bin/go test -bench=\u0026#39;BenchmarkMake1\u0026#39; -count=5 goos: darwin goarch: arm64 pkg: unique cpu: Apple M1 Pro BenchmarkMake1-10 122033748 9.692 ns/op BenchmarkMake1-10 123878858 9.688 ns/op BenchmarkMake1-10 123927121 9.706 ns/op BenchmarkMake1-10 123849468 9.759 ns/op BenchmarkMake1-10 123306187 9.673 ns/op PASS ok unique 11.055s ➜ unique git:(master) ✗ /Users/hxzhouh/workspace/googlesource/go/bin/go test -bench=\u0026#39;BenchmarkMake2\u0026#39; -count=5 goos: darwin goarch: arm64 pkg: unique cpu: Apple M1 Pro BenchmarkMake2-10 1000000000 0.3118 ns/op BenchmarkMake2-10 1000000000 0.3114 ns/op BenchmarkMake2-10 1000000000 0.3119 ns/op BenchmarkMake2-10 1000000000 0.3136 ns/op BenchmarkMake2-10 1000000000 0.3115 ns/op PASS ok unique 1.875s However, you shouldn\u0026rsquo;t treat it as a global variable to use, store shared data, the unique underlying implementation is actually a map, and the cost of querying is also very high.\nexample\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ➜ huizhou92_test git:(master) ✗ /Users/hxzhouh/workspace/googlesource/go/bin/go test --bench=BenchmarkBusinessUnique --count=5 goos: darwin goarch: arm64 pkg: huizhou92_test cpu: Apple M1 Pro BenchmarkBusinessUnique-10 3114 373867 ns/op BenchmarkBusinessUnique-10 3280 390818 ns/op BenchmarkBusinessUnique-10 2941 376503 ns/op BenchmarkBusinessUnique-10 3291 389665 ns/op BenchmarkBusinessUnique-10 2954 398610 ns/op PASS ok huizhou92_test 6.320s ➜ huizhou92_test git:(master) ✗ /Users/hxzhouh/workspace/googlesource/go/bin/go test --bench=BenchmarkBusinessString --count=5 goos: darwin goarch: arm64 pkg: huizhou92_test cpu: Apple M1 Pro BenchmarkBusinessString-10 526721706 2.185 ns/op BenchmarkBusinessString-10 548612287 2.183 ns/op BenchmarkBusinessString-10 549425077 2.188 ns/op BenchmarkBusinessString-10 549012100 2.182 ns/op BenchmarkBusinessString-10 548929644 2.183 ns/op PASS ok huizhou92_test 7.237s Because of this, the discussion about unique is still ongoing, possibly because it is not used very often? Anyway, the fact that this new package has entered the standard library is a reality. net/netip has already been refactored using unique to compare detailed information about IP addresses.\n","date":"2024-06-04T09:53:22+08:00","image":"https://images.yixiao9206.cn/blog-images/2024/06/0a8a9a271b2db6d5922f8e58e589b187.png","permalink":"https://huizhou92.com/p/golang-1.23-new-unique-package/","title":"Golang 1.23:  new unique package"},{"content":"In the previous article, we learned how to capture gRPC traffic using Wireshark. In this article, we will delve deeper into some details of gRPC. I am using the official example of gRPC grpc-go/examples/helloworld.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nPacket Capturing Details gRPC combines HTTP/2 and Protocol Buffers, making packet capturing more convenient when you understand these aspects. For instance, when capturing packets with Wireshark, manually set unrecognized HTTP/2 protocols as HTTP/2 (otherwise, it will be parsed as binary over TCP). Also, note that when viewing HTTP/2, do not expect JSON data display (protobuf uses its own compression algorithm).\nOverview of a gRPC Call [Image - gRPC Call Overview]\nFrom the above image, you can see that in the helloworld gRPC scenario, multiple HTTP2 requests occur when the client calls the server. This can generally be categorized as: Magic-\u0026gt;Settings-\u0026gt;Headers-\u0026gt;Data-\u0026gt;Settings-\u0026gt;Window-update, Ping-\u0026gt;Ping-\u0026gt;Headers, Data, Headers-\u0026gt;Window_update, ping-\u0026gt;Ping.\nSettings From the gRPC call overview image, you can observe two instances of Settings configurations with identical data. The specific data is as follows:\nThe screenshot of gRPC request Headers reveals some details:\nThe Header data of HTTP/2 is compressed. Headers include method (POST), scheme (http), path (/helloword.Greeter/SayHello), content-type (application/grpc), and more. Data The Headers section primarily consists of data related to the client\u0026rsquo;s request to the server, while the Data section involves the server sending data to the client.\nAnother Settings Configuration The second Settings configuration is almost identical to the first, except for the ACK flag being set to True.\nWindow_update, Ping In an HTTP/2 request, two stream blocks can be included.\nPing (Pong) When the client sends a ping request to the server, the server responds with a pong. The ping shown above is initiated by the client, while the subsequent ping is initiated by the server.\nThe data here represents what the server sends to the client (including header and body), as shown in the following screenshot.\nWindow_update, Ping It\u0026rsquo;s evident that not only can the client initiate a ping to the server, but the server can also ping the client in return.\nAnother Ping (Pong) After the server initiates a ping operation, the client responds with a pong.\nConclusion By capturing a gRPC helloworld scenario, this article traced the potential data flow process of a gRPC call. Combining theoretical knowledge of gRPC with the packet capturing content in this article should provide a better understanding of gRPC (focusing on HTTP/2 technology).\nReferences grpc / grpc.io HTTP/2: Official documentation for HTTP/2 Protocol Buffers HTTP/2 and How it Works @Carson ","date":"2024-05-29T16:31:16+08:00","permalink":"https://huizhou92.com/p/exploring-grpc-details-with-wireshark/","title":"Exploring gRPC details with WireShark"},{"content":"As we all know, HTTPS can solve the security issues in HTTP plaintext transmission, especially the problem of man-in-the-middle attacks. Its original full name is HTTP over SSL (or HTTP Security). SSL stands for Secure Sockets Layer, which was later replaced by TLS (Transport Layer Security). Today, let\u0026rsquo;s summarize the key points of HTTPS.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nHTTPS Versions People generally refer to the SSL and TLS protocols as the SSL/TLS protocol, but when people mention SSL in daily conversations, they usually mean the TLS protocol.\nTLS protocol has versions 1.1, 1.2, and 1.3. Among them, 1.2 used to be the mainstream, but now it is recommended to use the improved TLS 1.3, which upgrades the Handshake and Record protocols to make communication more secure and efficient.\nIn terms of security, TLS 1.3 removes some encryption algorithms that were considered insecure in TLS 1.2, such as RC4, DES, 3DES, AES-CBC, and MD5, which reduces the risk of security vulnerabilities.\nIn terms of performance, TLS 1.3 reduces the number of round trips (RTT) during the handshake process, thereby speeding up the connection establishment. In the best case scenario, TLS 1.3 only requires one round trip to complete the handshake, and it also supports 0-RTT extension, while TLS 1.2 requires two or more.\nOf course, as a well-designed Internet protocol, TLS 1.3 also considers maximizing forward compatibility through the extension protocol of the hello handshake message, which is not elaborated here.\nCore Process of HTTPS Depending on the differences in different versions, the detailed processes may vary slightly. Without pursuing rigorous details, the working process of HTTPS is as follows.\nThis diagram by bytebytego is very expressive, showing the key interactions and core encryption processes. The most crucial steps are how to establish a TCP connection, how to negotiate symmetric encryption keys through asymmetric encryption, and finally communicate through symmetric encryption.\nHTTPS, more precisely TLS, is well-designed. The key components are the Record Layer and several protocols. The former is the data transport channel, and various sub-protocols run on it. The Record is the basic unit of data transmission in TLS, similar to TCP segments and IP packets, which is the meaning of the following diagram.\nThe most important protocol in the Protocol is the Handshake protocol. After capturing the Client Hello, it will be more clearly reflected in Wireshark.\nHTTPS SNI Extension In the early days of the Internet, single-server machines were not that powerful, and the accompanying HTTPS, such as SSL v2, also had design flaws. At that time, there was an assumption that a single-server with an IP would only host one domain service. Therefore, after DNS resolution, it was very certain to directly connect to the IP and use the specific certificate for a particular domain. However, with the explosion of cloud computing, virtual hosting, and the scarcity of IP addresses in IPv4, it is inevitable that a server will host multiple domain scenes. This poses a problem for servers in not knowing which domain\u0026rsquo;s SSL certificate the client wants to access, which led to the emergence of HTTPS SNI.\nSNI (Server Name Indication) is an extension of the TLS protocol, which allows the client to send the target hostname information to the server during the handshake process. This way, the server can host multiple domains\u0026rsquo; HTTPS services on the same IP address and provide the correct certificate for each domain.\nThis problem seems simple, but in the early stages of the widespread adoption of HTTPS and the move of various Internet service providers to full-site HTTPS, many CDN vendors did not support SNI. Of course, today in 2024, both software ecosystems like Nginx and various vendors already support it.\nSNI information is transmitted through the TLS handshake protocol. The packet capture diagram is roughly as follows.\nIn practice, you can use the -servername option in the openssl s_client subcommand to specify SNI:\n1 openssl s_client -connect example.com:443 -servername example.com If you use the OpenSSL Library, you can also use functions like SSL_set_tlsext_host_name and BIO_set_conn_hostname to set SNI in the code.\nHTTPS Certificate Mechanism HTTPS achieves a series of encryption, decryption, signing, verification, and other functions through the public key system\u0026rsquo;s asymmetric, symmetric, and hash algorithms, basically realizing the four security properties: confidentiality, integrity, authentication, and non-repudiation. It also provides solutions for typical man-in-the-middle attacks (MITM).\nTo solve the trust issue of public keys, the certificate and trust chain mechanism are introduced. A certificate is issued by a third-party Certificate Authority (CA). It is essentially a file, usually stored with extensions like .crt, .cer, or .pem. This file is encoded according to certain standards, such as X.509, and contains information such as the public key, certificate holder information, issuing authority information, validity period, and digital signature.\nThere are some well-known CA organizations, such as DigiCert, VeriSign, Entrust, Let\u0026rsquo;s Encrypt, etc. The certificates they issue are divided into DV, OV, and EV, corresponding to different levels of trust. However, CA itself also has trust issues. The trust of small CAs relies on the signature and authentication of large CAs, but when it reaches the end of the chain, it can only use \u0026ldquo;self-signed certificates\u0026rdquo; or \u0026ldquo;root certificates\u0026rdquo;.\nMost operating systems and browsers have built-in root certificates for major CAs, and during HTTPS communication, the certificate chain is verified layer by layer until the root certificate.\nHTTPS Software Ecosystem Although the HTTPS or TLS ecosystem is rich, OpenSSL dominates the field. It supports almost all publicly available encryption algorithms and protocols and has become the de facto standard. Many applications use it as the underlying library to implement TLS functionality, such as the famous Apache, Nginx, etc.\nOpenSSL originated from SSLeay and has branched out into many branches, such as Google\u0026rsquo;s BoringSSL and OpenBSD\u0026rsquo;s LibreSSL. OpenSSL\u0026rsquo;s content is also extremely comprehensive, and learning can be prioritized using the openssl command. For specific details, you can refer to ChatGPT.\nHTTPS Acceleration Solutions HTTPS is great, but great things come at a cost. Therefore, various optimizations for full-site HTTPS deployment can basically be written as a separate article. Here are some brief points.\nFirst is optimizing RTT, which is particularly important in IO-intensive Internet scenarios. It mainly involves protocol upgrades, such as upgrading to HTTP/3 and TLS 1.3, which optimize RTT through different principles. Second is optimizing single-step performance, such as adding TLS acceleration cards, setting up dedicated TLS clusters or modules, and paying attention to terms like TLS session resumption.\nI have written an article before, sharing why HTTPS is so slow. If you are interested, you can read it here: Why does HTTPS need 7 handshakes and 9 times delay?\nReferences What\u0026rsquo;s the difference between HTTP and HTTPS?\nhow-does-https-work\nLong Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2024-05-27T18:37:27+08:00","image":"https://images.yixiao9206.cn/blog-images/2024/05/9113c36ee94b362ffe79a997b75c8efe.png","permalink":"https://huizhou92.com/p/understanding-https-key-points-and-processes-explained-in-detail/","title":"Understanding HTTPS: Key Points and Processes Explained in Detail"},{"content":"Last week, Go 1.23 entered the freeze period, meaning no new features will be added, and any already added features are unlikely to be removed. This is a great opportunity to preview the upcoming changes.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nDiscussion on Google Groups\nToday, we will discuss the changes to the //go:linkname directive in Go 1.23.\nRelated issue: GitHub Issue #67401\nTL;DR: The //go:linkname directive is not officially recommended and does not guarantee any forward or backward compatibility. It is wise to avoid using it whenever possible.\nWith that in mind, let’s dive into the new changes and see how they relate to us.\nWhat Does the linkname Directive Do? In simple terms, the linkname directive is used to pass information to the compiler and linker. Depending on its usage, it can be divided into three categories:\n1. Pull The \u0026ldquo;pull\u0026rdquo; usage is as follows:\n1 2 3 4 5 6 import _ \u0026#34;unsafe\u0026#34; // Required to use linkname import _ \u0026#34;fmt\u0026#34; // The pulled package must be explicitly imported (except the runtime package) //go:linkname my_func fmt.Println func my_func(...any) (n int, err error) This directive format is //go:linkname \u0026lt;local function or package-level variable\u0026gt; \u0026lt;fully defined function or variable in this or another package\u0026gt;. It tells the compiler and linker that my_func should directly use fmt.Println, making my_func an alias for fmt.Println.\nThis usage allows ignoring whether functions or variables are exported, pulling even package-private elements into use. However, this method is risky and can lead to panics if there’s a type mismatch.\n2. Push The \u0026ldquo;push\u0026rdquo; usage looks like this:\n1 2 3 4 5 6 7 8 9 10 11 import _ \u0026#34;unsafe\u0026#34; // Required to use linkname //go:linkname main.fastHandle func fastHandle(input io.Writer) error { ... } // package main func fastHandle(input io.Writer) error // The main package can directly use fastHandle Here, you only need to pass the function or variable name as the first parameter to the directive, specifying the package name where it should be used. This usage signifies that the function or variable will be named xxx.yyy.\n3. Handshake The \u0026ldquo;handshake\u0026rdquo; usage combines both methods:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package mypkg import _ \u0026#34;unsafe\u0026#34; // Required to use linkname //go:linkname fastHandle func fastHandle(input io.Writer) error { ... } package main import _ \u0026#34;unsafe\u0026#34; // Required to use linkname //go:linkname fastHandle mypkg.fastHandle func fastHandle(input io.Writer) error The pull side remains unchanged, but the push side doesn\u0026rsquo;t need to specify the package name. This usage implies a handshake between the two ends, clearly marking which function or variable should be linked.\nRisks of linkname The primary risk is the ability to use package-private functions or variables without the package’s knowledge. For example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // pkg/mymath/mymath.go package mymath func uintPow(n uint) uint { return n * n } // main.go package main import ( \u0026#34;fmt\u0026#34; _ \u0026#34;linkname/pkg/mymath\u0026#34; _ \u0026#34;unsafe\u0026#34; ) //go:linkname pow linkname/pkg/mymath.uintPow func pow(n uint) uint func main() { fmt.Println(pow(6)) // 36 } Normally, uintPow shouldn\u0026rsquo;t be accessible outside its package. But linkname bypasses this restriction, which can lead to severe type-related memory errors or runtime panics.\nPositive Aspects of linkname Despite its risks, linkname exists for valid reasons, such as during the startup of Go programs. For example, in Go’s runtime:\n1 2 3 4 5 6 7 8 9 10 // runtime/proc.go //go:linkname main_main main.main func main_main() // runtime.main func main() { fn := main_main fn() } Here, linkname allows the runtime to call the user-defined main function.\nChanges to linkname in Go 1.23 Given the risks, the Go core team has decided to limit linkname usage:\nNew standard library packages will prohibit linkname. A new ldflag, -checklinkname=1, has been added to enforce restrictions. It defaults to 0 but will be set to 1 in the final release of 1.23. Pull-only linkname will be prohibited for the standard library, allowing only the handshake mode. For instance, the following code will no longer compile in 1.23:\n1 2 3 4 5 6 7 8 package main import _ \u0026#34;unsafe\u0026#34; //go:linkname corostart runtime.corostart func corostart() func main() { corostart() } Future of linkname The long-term goal is to only allow handshake mode. As developers, we should:\nUse -checklinkname=1 to audit and remove unnecessary linkname usage. Propose to make private APIs public if necessary. As a last resort, disable the restriction with -ldflags=-checklinkname=0. Conclusion In summary, avoid using //go:linkname to prevent unforeseen issues.\n","date":"2024-05-27T09:40:57+08:00","image":"https://images.yixiao9206.cn/blog-images/2024/05/c2af842b3c5763e1a1d03a92f39c1102.png","permalink":"https://huizhou92.com/p/%23-golang-1.23-changes-to-/golinkname-and-what-it-means-for-developers/","title":"Golang 1.23: Changes to `//go:linkname` and What It Means for Developers"},{"content":"Background In the previous article, we learned that panic can occur in three ways:\nInitiated by developers: by calling the panic() function. Generated by the compiler: for example, in the case of division by zero. Sent to the process by the kernel: for example, in the case of an illegal memory access. This article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nAll three cases can be categorized as calls to the panic() function, indicating that panic in Go is just a particular function call and is handled at the language level. Now that we know how panic is triggered, the next step is to understand how panic is handled. When I first started learning Go, I often had some questions in my mind:\nWhat exactly is panic? Is it a struct or a function? Why does panic cause the Go process to exit? Why does recover need to be placed inside a defer statement to take effect? Even if recover is placed inside a defer statement, why doesn\u0026rsquo;t the process recover? Why is it possible to panic again after a panic? What are the implications? Today, we will delve into the code to clarify these questions.\nBased on Go 1.21.4\nThe _panic Data Structure Let\u0026rsquo;s start by looking at an example of an actively triggered panic. Through the assembly code, we can see that the panic call is made to the runtime.gopanic function, which contains a crucial data structure called _panic.\nLet\u0026rsquo;s take a look at the _panic data structure:\n1 2 3 4 5 6 7 8 9 type _panic struct { argp unsafe.Pointer // pointer to arguments of deferred call run during panic; cannot move - known to liblink arg any // argument to panic link *_panic // link to earlier panic pc uintptr // where to return to in runtime if this panic is bypassed sp unsafe.Pointer // where to return to in runtime if this panic is bypassed recovered bool // whether this panic is over aborted bool // the panic was aborted goexit bool } Key Fields to Focus On:\nlink: A pointer to the _panic structure, indicating that _panic can form a unidirectional linked list, similar to the _defer list. recovered field: The recovery of the so-called _panic depends on whether this field is set to true. recover() actually modifies this field. Now let\u0026rsquo;s take a look at two important fields in the g structure:\n1 2 3 4 5 type g struct { _panic *_panic // panic linked list, the innermost one _defer *_defer // defer linked list, the innermost one // ... } From here, we can see that both the _defer and _panic linked lists are attached to the goroutine. When can there be multiple elements on the _panic linked list? The answer is when a panic() call is made within a defer function. Only in a defer function can an _panic linked list be formed because the panic() function only executes _defer functions!\nThe recover() Function For the sake of explanation, let\u0026rsquo;s start with a simple analysis of what the recover() function does:\n1 2 3 defer func() { recover() }() The recover() function corresponds to the gorecover function implementation in the runtime/panic.go file.\nThe gorecover Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func gorecover(argp uintptr) any { // Must be in a function running as part of a deferred call during the panic. // Must be called from the topmost function of the call // (the function used in the defer statement). // p.argp is the argument pointer of that topmost deferred function call. // Compare against argp reported by caller. // If they match, the caller is the one who can recover. gp := getg() p := gp._panic if p != nil \u0026amp;\u0026amp; !p.goexit \u0026amp;\u0026amp; !p.recovered \u0026amp;\u0026amp; argp == uintptr(p.argp) { p.recovered = true return p.arg } return nil } This function is quite simple:\nRetrieve the current goroutine structure. Get the latest _panic from the _panic linked list of the current goroutine. If it is not nil, proceed with the processing. Set the recovered field of the _panic structure to true and return the arg field. That\u0026rsquo;s all there is to the recover() function. It simply sets the value of the recovered field in the _panic structure and does not involve any magical code jumps. The assignment of recovered is effective within the logic of the panic() function.\nThe panic() Function Based on the previous assembly code, we know that the panic call is made to the runtime.gopanic function.\nThe gopanic Function The most important part of the panic mechanism is the gopanic function. All the details about panic are in this function. The complexity of understanding panic lies in two points:\nRecursive execution of gopanic when panic is nested. The program counter (pc) and stack pointer (sp) are not manipulated in the usual way of function call and return. During recovery, the instruction registers are modified directly, bypassing the remaining logic of gopanic and even the recursive logic of multiple gopanic calls. We can understand the logic of gopanic by dividing it into two parts: inside the loop and outside the loop.\nInside the Loop The actions inside the loop can be divided into the following steps:\nIterate through the defer linked list of the goroutine and retrieve a _defer deferred function. Set the d.started flag and bind the current d._panic (used for recursive detection). Execute the _defer deferred function. Remove the executed _defer function from the linked list. Check if the recovered field of the _panic structure is set to true and take appropriate action. If it is true, reset the pc and sp registers (usually starting from the deferreturn instruction) and enqueue the goroutine in the scheduler for execution. Repeat the above steps. Questions to Consider You may notice that the recovered field is only modified during the third step. It cannot be modified anywhere else.\nQuestion 1: Why does recover need to be placed inside a defer statement to take effect?\nBecause that\u0026rsquo;s the only opportunity!\nLet\u0026rsquo;s consider a few straightforward examples:\n1 2 3 4 func main() { panic(\u0026#34;test\u0026#34;) recover() } In the above example, recover() is called after panic(). Why does it still panic? Because the recover() function is never executed. The execution order is as follows:\n1 2 3 4 panic -\u0026gt; gopanic -\u0026gt; Execute the defer linked list -\u0026gt; exit Someone might argue, \u0026ldquo;What if I put recover() before panic('test')?\u0026rdquo;\n1 2 3 4 func main() { recover() panic(\u0026#34;test\u0026#34;) } No, it won\u0026rsquo;t work because when recover() is executed, the _panic is not attached to the goroutine yet. So recover() is useless in this case.\nQuestion 2: Even if recover is placed inside a defer statement, why doesn\u0026rsquo;t the process recover?\nLet\u0026rsquo;s recall the operations in the for loop:\n1 2 3 4 5 6 // Step: Iterate through the _defer linked list d := gp._defer // Step: Execute the defer function reflectcall(nil, unsafe.Pointer(d.fn), deferArgs(d), uint32(d.siz), uint32(d.siz)) // Step: Remove the executed defer function gp._defer = d.link Key Point: In the gopanic function, only the _defer functions of the current goroutine are executed. Therefore, if a recover() is performed in a defer function attached to another goroutine, it will have no effect.\nLet\u0026rsquo;s consider an example:\n1 2 3 4 5 6 7 8 func main() { // g1 go func() { // g2 defer func() { recover() }() }() panic(\u0026#34;test\u0026#34;) } Since the panic and recover are in two different goroutines, the _panic is attached to g1, and the recover in g2\u0026rsquo;s _defer chain cannot access the _panic structure of g1. Therefore, it cannot set the recovered field to true, and the program still crashes.\nQuestion 3: Why is it possible to panic again after a panic? What are the implications?\nThis is actually quite easy to understand. Some people might overthink it. Can we call panic() recursively? Yes, we can.\nThe scenario is usually as follows:\nThe gopanic function calls a _defer function. The _defer function calls panic() or gopanic(). This is just a simple function recursion, and there\u0026rsquo;s nothing special about it. In this scenario, an _panic linked list will be formed starting from gp._panic. The instructions executed by gopanic are special in two ways:\nIf the _panic structure is set to recovered, the pc and sp registers are reset, bypassing gopanic (including nested function stacks), and jumping directly to the instruction to be executed after the defer function (deferreturn). If there is no handler for the _panic data, exit the process and terminate the execution of subsequent instructions. Let\u0026rsquo;s look at an example of nested panic:\n1 2 3 4 5 6 func main() { defer func() { // defer_0 panic(\u0026#34;panic again\u0026#34;) }() panic(\u0026#34;first\u0026#34;) } The function execution is as follows:\n1 2 3 4 5 6 7 gopanic // First panic defer_0 is executed gopanic // Second panic defer_0 is removed from the linked list (recursive call), termination condition is met // Print stack trace and exit the program fatalpanic Here\u0026rsquo;s another example for comparison:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func main() { println(\u0026#34;=== begin ===\u0026#34;) defer func() { // defer_0 println(\u0026#34;=== come in defer_0 ===\u0026#34;) }() defer func() { // defer_1 recover() }() defer func() { // defer_2 panic(\u0026#34;panic 2\u0026#34;) }() panic(\u0026#34;panic 1\u0026#34;) println(\u0026#34;=== end ===\u0026#34;) } Will this function print the stack trace and exit?\nThe answer is no. The output will be:\n1 ➜ panic ./test_panic === begin === === come in defer_0 === Did you guess it correctly? Let me explain the complete route:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 main gopanic // First panic 1. Retrieve defer_2, set started 2. Execute defer_2 gopanic // Second panic 1. Retrieve defer_2, set panic as aborted 2. Remove defer_2 from the linked list 3. Execute defer_1 - Execute recover 4. Remove defer_1 5. Execute recovery, reset pc register, jump to the instruction registered in defer_1 (usually deferreturn) // Jump out of the recursive call of gopanic, directly to the execution of deferreturn; defereturn 1. Iterate through the defer function chain, there is still defer_0 left, retrieve defer_0 2. Execute defer_0 // End of the main function Here\u0026rsquo;s another example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func main() { println(\u0026#34;=== begin ===\u0026#34;) defer func() { // defer_0 println(\u0026#34;=== come in defer_0 ===\u0026#34;) }() defer func() { // defer_1 panic(\u0026#34;panic 2\u0026#34;) }() defer func() { // defer_2 recover() }() panic(\u0026#34;panic 1\u0026#34;) println(\u0026#34;=== end ===\u0026#34;) } Will this function print the stack trace and exit?\nThe answer is yes.\nThe output will be:\n1 ➜ panic ./test_panic === begin === === come in defer_0 === panic: panic 2 goroutine 1 [running]: main.main.func2() /Users/code/gopher/src/panic/test_panic.go:9 +0x39 main.main() /Users/code/gopher/src/panic/test_panic.go:11 +0xf7 The execution path is as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 main gopanic // First panic 1. Retrieve defer_2, set started 2. Execute defer_2 - Execute recover, set panic_1 as recovered 3. Remove defer_2 from the linked list 4. Execute recovery, reset pc register, jump to the instruction registered in defer_1 (usually deferreturn) // Jump out of the recursive call of gopanic, execute deferreturn; defereturn 1. Iterate through the defer function chain, retrieve defer_1 2. Remove defer_1 2. Execute defer_1 gopanic // Second panic 1. There is a defer_0 on the defer chain 2. Execute defer_0 (defer_0 does not recover, only prints one line of output) 3. Remove defer_0, the chain is empty, exit the for loop 3. Execute fatalpanic - exit(2) terminates the process Did you guess correctly?\nThe recovery Function Finally, let\u0026rsquo;s take a look at the crucial recovery function. In the gopanic function, when executing the defer functions in the loop, if the recovered field of the _panic structure is set to true, the mcall(recovery) function is called to perform the so-called \u0026ldquo;recovery.\u0026rdquo;\nLet\u0026rsquo;s take a look at the implementation of the recovery function. It\u0026rsquo;s a very simple function that resets the pc and sp registers and reschedules the Goroutine for execution.\n1 2 3 4 5 6 7 8 9 10 11 // runtime/panic.go func recovery(gp *g) { // Retrieve the values of the stack pointer and program counter sp := gp.sigcode0 pc := gp.sigcode1 // Reset the pc and sp registers of the Goroutine gp.sched.sp = sp gp.sched.pc = pc // Reschedule the Goroutine gogo(\u0026amp;gp.sched) } Resetting the pc and sp registers means what? The pc register points to the address of the instruction, in other words, it jumps to another location to execute instructions. It no longer executes the instructions sequentially after gopanic. The _defer.pc is the instruction line of the executed code, and where is this instruction?\nFor this, let\u0026rsquo;s recall the chapter on defer. When a deferred function is registered, it corresponds to a _defer structure. When creating this structure, the _defer.pc field is assigned the instruction on the next line after the new function. This was explained in detail in the chapter on \u0026ldquo;In-Depth Analysis of Defer\u0026rdquo;.\nHere\u0026rsquo;s an example: if it\u0026rsquo;s allocated on the stack, it will be in deferprocStack. So, mcall(recovery) jumps to this position, and the subsequent logic follows the deferreturn logic, executing the remaining _defer function chain.\nThat concludes the explanation of panic. It\u0026rsquo;s just a special function call, nothing special. The only thing that makes it special is the special instruction jumps it performs.\n","date":"2024-05-26T09:17:40+08:00","permalink":"https://huizhou92.com/p/the-truth-about-panic-and-recover-in-go/","title":"The Truth About Panic And Recover In Go"},{"content":"\nBacklink | |Photo by Anna Demianenko on Unsplash Background gRPC is an open-source high-performance RPC framework developed by Google. The design goal of gRPC is to run in any environment, supporting pluggable load balancing, tracing, health checking, and authentication. It not only supports service calls within and across data centers but is also suitable for the last mile of distributed computing, connecting devices, mobile applications, and browsers to backend services. For more on the motivation and principles behind gRPC\u0026rsquo;s design, refer to this article: gRPC Motivation and Design Principles.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nKey points from the official article:\nInternally, there is a framework called Stubby, but it is not based on any standard. Supports use in any environment, including IoT, mobile, and browsers. Supports streaming and flow control. In reality, performance is not the primary goal of gRPC design. So why choose HTTP/2?\nWhat is HTTP/2 Before discussing why gRPC chose HTTP/2, let\u0026rsquo;s briefly understand HTTP/2.\nHTTP/2 can be simply introduced with an image:\nFrom: https://hpbn.co/\nThe header in HTTP/1 corresponds to the HEADERS frame in HTTP/2. The payload in HTTP/1 corresponds to the DATA frame in HTTP/2.\nIn the Chrome browser, open chrome://net-internals/#http2 to see information about HTTP/2 connections.\nMany websites are already running on HTTP/2.\ngRPC Over HTTP/2 Strictly speaking, gRPC is designed in layers, with the underlying layer supporting different protocols. Currently, gRPC supports:\ngRPC over HTTP2 gRPC Web However, most discussions are based on gRPC over HTTP2. Let\u0026rsquo;s look at a real gRPC SayHello request and see how it is implemented over HTTP/2 using Wireshark:\nYou can see the following headers:\n1 2 3 4 5 6 Header: :authority: localhost:50051 Header: :path: /helloworld.Greeter/SayHello Header: :method: POST Header: :scheme: http Header: content-type: application/grpc Header: user-agent: grpc-java-netty/1.11.0 Then the request parameters are in the DATA frame:\nGRPC Message: /helloworld.Greeter/SayHello, Request\nIn short, gRPC puts metadata in HTTP/2 Headers and serialized request parameters in the DATA frame.\nAdvantages of HTTP/2 Protocol HTTP/2 Is an Open Standard Google thought this through and chose not to open-source its internal Stubby but to create something new. As technology becomes more open, the space for proprietary protocols is shrinking.\nHTTP/2 Is a Proven Standard HTTP/2 was developed based on practical experience, which is crucial. Many unsuccessful standards were created by a group of vendors before implementation, leading to chaos and unusability, such as CORBA. HTTP/2\u0026rsquo;s predecessor was Google\u0026rsquo;s SPDY. Without Google\u0026rsquo;s practice and promotion, HTTP/2 might not exist.\nHTTP/2 Naturally Supports IoT, Mobile, and Browsers In fact, mobile phones and mobile browsers were the first to adopt HTTP/2. The mobile internet has driven the development and adoption of HTTP/2.\nMulti-language Implementation of HTTP/2 is Easy Discussing only the implementation of the protocol itself, without considering serialization:\nEvery popular programming language has a mature HTTP/2 Client. HTTP/2 Clients are well-tested and reliable. Sending HTTP/2 requests with a Client is much easier than sending/receiving packets with sockets. HTTP/2 Supports Stream and Flow Control There are many streaming solutions in the industry, such as those based on WebSocket or rsocket. However, these solutions are not universal.\nStreams in HTTP/2 can also be prioritized, which might be used in complex scenarios, although less frequently in RPC.\nEasy Support for HTTP/2 in Gateway/Proxy Nginx support for gRPC: https://www.nginx.com/blog/nginx-1-13-10-grpc/ Envoy support for gRPC: https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/grpc# HTTP/2 Ensures Security HTTP/2 naturally supports SSL, although gRPC can run on a clear text protocol (i.e., unencrypted). Many proprietary RPC protocols might wrap a layer of TLS support, making it very complex to use. Do developers have enough security knowledge? Are users configuring it correctly? Can operators understand it correctly? HTTP/2 ensures secure transmission over public networks. For example, the CRIME attack is hard to prevent in proprietary protocols. Mature Authentication in HTTP/2 Authentication systems developed from HTTP/1 are mature and can be seamlessly used in HTTP/2. End-to-end authentication from front-end to back-end without any conversion or adaptation.\nFor example, traditional RPC like Dubbo requires writing a Dubbo filter and considering how to pass authentication-related information through thread local. The RPC protocol itself also needs to support it. In short, it\u0026rsquo;s very complex. In fact, most RPCs in companies do not have authentication and can be called freely. Disadvantages of HTTP/2 Protocol Inefficient Transmission of RPC Metadata Although HPAC can compress HTTP Headers, for RPC, determining a function call can be simplified to an int. Once negotiated between both ends, it can be directly looked up in a table, without the need for HPAC encoding and decoding.\nConsider optimizing an HTTP/2 parser specifically for gRPC to reduce some general processing and improve performance.\ngRPC Calls in HTTP/2 Require Two Decodings One for the HEADERS frame and one for the DATA frame.\nThe HTTP/2 standard itself only allows one TCP connection, but in practice, gRPC may have multiple TCP connections, which needs attention during use.\nChoosing HTTP/2 for gRPC means its performance won\u0026rsquo;t be top-notch. But for RPC, moderate QPS is acceptable, and generality and compatibility are the most important. Refer to the official benchmark: https://grpc.io/docs/guides/benchmarking.html\nhttps://github.com/hank-whu/rpc-benchmark\nIf your scenario is to\u0026hellip; Google\u0026rsquo;s Standard-Setting Ability In the past decade, Google\u0026rsquo;s ability to set standards has grown stronger. Here are some standards:\nHTTP/2 WebP image format WebRTC for real-time communication VP9/AV1 video encoding standards Service Worker/PWA QUIC/HTTP/3\nOf course, Google doesn\u0026rsquo;t always succeed. Many initiatives it tried to push failed, such as Chrome\u0026rsquo;s Native Client. gRPC is currently the de facto standard in the Kubernetes ecosystem. Will gRPC become the RPC standard in more areas and larger fields?\nWhy gRPC Emerged Why did an HTTP/2-based RPC emerge?\nI believe an important reason is that in the trend of Cloud Native, the need for open interoperability inevitably leads to HTTP/2-based RPC. Even without gRPC, there would be other HTTP/2-based RPCs.\ngRPC was first used internally at Google on Google Cloud Platform and public APIs: https://opensource.google.com/projects/grpc\nSummary Although gRPC may not replace internal RPC implementations, in an era of open interoperability, not just on Kubernetes, gRPC will have more and more stages to showcase its capabilities.\nReferences https://grpc.io/ https://hpbn.co/ https://grpc.io/blog/loadbalancing https://http2.github.io/faq https://github.com/grpc/grpc ","date":"2024-05-23T10:18:54+08:00","permalink":"https://huizhou92.com/p/why-did-google-choose-to-implement-grpc-using-http2/","title":"Why Did Google Choose To Implement gRPC Using HTTP/2?"},{"content":"Introduce Wireshark is a popular tool for capturing network packets. It can not only capture packets itself but also parse packet files captured by tcpdump.\ngRPC is a high-performance RPC framework developed by Google, based on the HTTP/2 protocol and the protobuf serialization protocol.\nThis article mainly introduces how to capture gRPC packets using Wireshark and parse the packet content.\nThis article was first published in the medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nWireshark version: 4.2.2\nConfiguration Since gRPC uses the protobuf serialization protocol, we need to add the protobuf file path.\nClick Wireshark -\u0026gt; Preferences... -\u0026gt; Protocols -\u0026gt; Protobuf -\u0026gt; Protobuf search paths -\u0026gt; Edit...\nClick + to add the path of your protobuf file. Don\u0026rsquo;t forget to check the Load all files on the right side.\nSpecific Operations First, we write a simple gRPC service,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 syntax = \u0026#34;proto3\u0026#34;; option go_package = \u0026#34;example.com/hxzhouh/go-example/grpc/helloworld/api\u0026#34;; package api; // The greeting service definition. service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) {} } // The request message containing the user\u0026#39;s name. message HelloRequest { string name = 1; } // The response message containing the greetings message HelloReply { string message = 1; } It has just one function Greeter. After completing the server-side code, run it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 type server struct { api.UnimplementedGreeterServer } func (s *server) SayHello(ctx context.Context, in *api.HelloRequest) (*api.HelloReply, error) { log.Printf(\u0026#34;Received: %v\u0026#34;, in.GetName()) return \u0026amp;api.HelloReply{Message: \u0026#34;Hello \u0026#34; + in.GetName()}, nil } func main() { lis, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:50051\u0026#34;) if err != nil { log.Fatalf(\u0026#34;failed to listen: %v\u0026#34;, err) } s := grpc.NewServer() api.RegisterGreeterServer(s, \u0026amp;server{}) if err := s.Serve(lis); err != nil { log.Fatalf(\u0026#34;failed to serve: %v\u0026#34;, err) } } Then open Wireshark, select the local network card, and listen to tcp.port == 50051.\nIf you are new to Wireshark, I recommend reading this article first: https://www.lifewire.com/wireshark-tutorial-4143298\nUnary Function Now we have a gRPC service running on the local 50051 port. We can use BloomRPC or any other tool you like to initiate an RPC request to the server, or use the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func Test_server_SayHello(t *testing.T) { // Set up a connection to the server. conn, err := grpc.Dial(\u0026#34;localhost:50051\u0026#34;, grpc.WithInsecure(), grpc.WithBlock()) if err != nil { log.Fatalf(\u0026#34;did not connect: %v\u0026#34;, err) } defer conn.Close() c := api.NewGreeterClient(conn) // Contact the server and print out its response. name := \u0026#34;Hello\u0026#34; ctx, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() r, err := c.SayHello(ctx, \u0026amp;api.HelloRequest{Name: name}) if err != nil { log.Fatalf(\u0026#34;could not greet: %v\u0026#34;, err) } log.Printf(\u0026#34;Greeting: %s\u0026#34;, r.GetMessage()) } At this point, Wireshark should be able to capture the traffic packets.\nAs mentioned earlier, gRPC = HTTP2 + protobuf, and since we have already loaded the protobuf file, we should now be able to parse the packet.\nUse the Wireshark shortcut shift+command+U or click Analyze -\u0026gt; Decode As... and set the packet to be decoded as HTTP2 format.\nAt this point, we can see the request clearly.\nMetadata We know that gRPC metadata is transmitted through HTTP2 headers. Now let\u0026rsquo;s verify this by capturing packets.\nSlightly modify the client code:\n1 2 3 4 5 6 7 8 9 10 11 12 func Test_server_SayHello(t *testing.T) { // Set up a connection to the server. ..... // add md md := map[string][]string{\u0026#34;timestamp\u0026#34;: {time.Now().Format(time.Stamp)}} md[\u0026#34;testmd\u0026#34;] = []string{\u0026#34;testmd\u0026#34;} ctx := metadata.NewOutgoingContext(context.Background(), md) // Contact the server and print out its response. name := \u0026#34;Hello\u0026#34; ctx, cancel := context.WithTimeout(ctx, time.Second) .... } Then capture packets again. We can see that md is indeed placed in the header.\nWe also see grpc-timeout in the header, indicating that the request timeout is also placed in the header. The specific details may be covered in a dedicated article, but today we focus on packet capturing.\nTLS The examples above use plaintext transmission, and we used grpc.WithInsecure() when dialing. However, in a production environment, we generally use TLS for encrypted transmission. Detailed information can be found in my previous article.\nhttps://medium.com/gitconnected/secure-communication-with-grpc-from-ssl-tls-certification-to-san-certification-d9464c3d706f\nLet\u0026rsquo;s modify the server-side code:\nhttps://gist.github.com/hxzhouh/e08546cf0457d28a614d59ec28870b11 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 func main() { certificate, err := tls.LoadX509KeyPair(\u0026#34;./keys/server.crt\u0026#34;, \u0026#34;./keys/server.key\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to load key pair: %v\u0026#34;, err) } certPool := x509.NewCertPool() ca, err := os.ReadFile(\u0026#34;./keys/ca.crt\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to read ca: %v\u0026#34;, err) } if ok := certPool.AppendCertsFromPEM(ca); !ok { log.Fatalf(\u0026#34;Failed to append ca certificate\u0026#34;) } opts := []grpc.ServerOption{ grpc.Creds( // Enable TLS for all incoming connections credentials.NewTLS(\u0026amp;tls.Config{ ClientAuth: tls.RequireAndVerifyClientCert, Certificates: []tls.Certificate{certificate}, ClientCAs: certPool, }, )), } listen, err := net.Listen(\u0026#34;tcp\u0026#34;, fmt.Sprintf(\u0026#34;0.0.0.0:%d\u0026#34;, 50051)) if err != nil { log.Fatalf(\u0026#34;failed to listen %d port\u0026#34;, 50051) } // Create a new gRPC server instance with the provided TLS server credentials s := grpc.NewServer(opts...) api.RegisterGreeterServer(s, \u0026amp;server{}) log.Printf(\u0026#34;server listening at %v\u0026#34;, listen.Addr()) if err := s.Serve(listen); err != nil { log.Fatalf(\u0026#34;Failed to serve: %v\u0026#34;, err) } } Now let\u0026rsquo;s also modify the client code:\nhttps://gist.github.com/hxzhouh/46a7a31e2696b87fe6fb83c8ce7e036c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 func Test_server_SayHello(t *testing.T) { certificate, err := tls.LoadX509KeyPair(\u0026#34;./keys/client.crt\u0026#34;, \u0026#34;./keys/client.key\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to load client key pair, %v\u0026#34;, err) } certPool := x509.NewCertPool() ca, err := os.ReadFile(\u0026#34;./keys/ca.crt\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to read %s, error: %v\u0026#34;, \u0026#34;./keys/ca.crt\u0026#34;, err) } if ok := certPool.AppendCertsFromPEM(ca); !ok { log.Fatalf(\u0026#34;Failed to append ca certs\u0026#34;) } opts := []grpc.DialOption{ grpc.WithTransportCredentials(credentials.NewTLS( \u0026amp;tls.Config{ ServerName: \u0026#34;localhost\u0026#34;, Certificates: []tls.Certificate{certificate}, RootCAs: certPool, })), } // Establish connection to the server conn, err := grpc.Dial(\u0026#34;localhost:50051\u0026#34;, opts...) if err != nil { log.Fatalf(\u0026#34;Connect to %s failed\u0026#34;, \u0026#34;localhost:50051\u0026#34;) } defer conn.Close() client := api.NewGreeterClient(conn) ctx, cancel := context.WithTimeout(context.Background(), time.Second*5) defer cancel() r, err := client.SayHello(ctx, \u0026amp;api.HelloRequest{Name: \u0026#34;Hello\u0026#34;}) if err != nil { log.Printf(\u0026#34;Failed to greet, error: %v\u0026#34;, err) } else { log.Printf(\u0026#34;Greeting: %v\u0026#34;, r.GetMessage()) } } At this point, we can capture packets again and use the same method to decode them. However, we will find that decoding as HTTP2 is no longer possible, but we can decode it as TLS1.3.\nConclusion This article summarizes the basic process of using Wireshark to capture gRPC packets.\nBy capturing packets, we learn that gRPC parameter transmission is done through HTTP2 data frames, and CTX and other metadata are transmitted through headers. These concepts might be familiar, but hands-on experiments enhance understanding.\nWith TLS, we can achieve secure gRPC communication. In the next article, we will attempt to decrypt TLS packets.\nReferences Wireshark Tutorial https://grpc.io/blog/wireshark/ https://www.lifewire.com/wireshark-tutorial-4143298 ","date":"2024-05-20T09:29:17+08:00","image":"https://images.yixiao9206.cn/blog-images/2024/05/a8ca43282aece789e1e0b1d2a2db7a5f.png","permalink":"https://huizhou92.com/p/how-to-capture-and-analyze-grpc-packets/","title":"How to Capture and Analyze gRPC Packets Using Wireshark"},{"content":"gRPC is a high-performance RPC framework developed by Google, which by default includes two authentication methods:\nSSL/TLS Authentication Token-based Authentication\nWithout the activation of the certificate, gRPC service and clients communicate in plaintext, leaving the information at risk of being intercepted by any third party. To ensure gRPC communication is not intercepted, altered or counterfeited by a third party, the server can activate TLS encryption features. This article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nStarting from go 1.15 version, depreciation of CommonName began, therefore, it is advised to use SAN certificates. If keys, CSR, and certificates are generated in the previous way through OpenSSL, the following error occurs:\n1 rpc error: code = Unavailable desc = connection error: desc = \u0026#34;transport: authentication handshake failed: x509: certificate relies on legacy Common Name field, use SANs instead\u0026#34;| What is SAN SAN (Subject Alternative Name) is defined as an extension in the SSL standard x509. An SSL certificate with the SAN field can expand the domain names it supports, allowing a single certificate to support multiple different domain name resolutions.\nPut simply, a SAN certificate can contain multiple complete CN (CommonName), so with a single certificate purchase, you can use it on multiple URLs. For example, the certificate of skype.com, it has many SANs.\nCreate a SAN certificate locally Next, we will use an example to generate a client \u0026amp; server bilateral SAN certificate locally.\nAssume the hostname of the gRPC server is localhost, and it is required to configure tls bilateral authentication encryption for the communication between the gRPC server and clients.\nCreate openssl.conf to store relevant information 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 [req] req_extensions = v3_req distinguished_name = req_distinguished_name prompt = no [req_distinguished_name] countryName = CN stateOrProvinceName = state localityName = city organizationName = huizhou92 commonName = hello-world [v3_req] subjectAltName = @alt_names [alt_names] DNS.1 = localhost The content is similar to when creating a ca earlier.\nGenerate ca root certificate 1 2 openssl req -x509 -newkey rsa:4096 -keyout ca.key -out ca.crt -subj \u0026#34;/CN=localhost\u0026#34; -days 3650 -nodes -nodes is to ignore the password, making it convenient to use, but please note, this may reduce the security of the private key, as anyone can read the unencrypted private key.\nGenerate server certificate 1 2 3 openssl req -newkey rsa:2048 -nodes -keyout server.key -out server.csr -subj \u0026#34;/CN=localhost\u0026#34; -config openssl.cnf openssl x509 -req -in server.csr -out server.crt -CA ca.crt -CAkey ca.key -CAcreateserial -days 365 -extensions v3_req -extfile openssl.cnf Generate client certificate 1 2 3 openssl req -newkey rsa:2048 -nodes -keyout client.key -out client.csr -subj \u0026#34;/CN=localhost\u0026#34; -config openssl.cnf openssl x509 -req -in client.csr -out client.crt -CA ca.crt -CAkey ca.key -CAcreateserial -days 365 -extensions v3_req -extfile openssl.cnf The final generated result is as follows\n1 2 3 ➜ keys git:(day1) ✗ ls ca.crt ca.key ca.srl client.crt client.csr client.key openssl.cnf server.crt server.csr server.key Testing We define the simplest grpc interface.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 // helloworld.proto syntax = \u0026#34;proto3\u0026#34;; option go_package = \u0026#34;./api;api\u0026#34;; package api; // The greeting service definition. service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) {} } // The request message containing the user\u0026#39;s name. message HelloRequest { string name = 1; } // The response message containing the greetings message HelloReply { string message = 1; } Server implementation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 package main import ( \u0026#34;context\u0026#34; \u0026#34;crypto/tls\u0026#34; \u0026#34;crypto/x509\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;google.golang.org/genproto/googleapis/rpc/errdetails\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; \u0026#34;google.golang.org/grpc/codes\u0026#34; \u0026#34;google.golang.org/grpc/credentials\u0026#34; \u0026#34;google.golang.org/grpc/status\u0026#34; \u0026#34;hello-world/api\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) type server struct { api.UnimplementedGreeterServer } func (s *server) SayHello(ctx context.Context, in *api.HelloRequest) (*api.HelloReply, error) { log.Printf(\u0026#34;Received: %v\u0026#34;, in.GetName()) select { case \u0026lt;-ctx.Done(): log.Println(\u0026#34;client timeout return\u0026#34;) return nil, ErrorWithDetails() case \u0026lt;-time.After(3 * time.Second): return \u0026amp;api.HelloReply{Message: \u0026#34;Hello \u0026#34; + in.GetName()}, nil } } func main() { certificate, err := tls.LoadX509KeyPair(\u0026#34;./keys/server.crt\u0026#34;, \u0026#34;./keys/server.key\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to load key pair: %v\u0026#34;, err) } // 通过CA创建证书池 certPool := x509.NewCertPool() ca, err := os.ReadFile(\u0026#34;./keys/ca.crt\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to read ca: %v\u0026#34;, err) } // 将来自CA的客户端证书附加到证书池 if ok := certPool.AppendCertsFromPEM(ca); !ok { log.Fatalf(\u0026#34;Failed to append ca certificate\u0026#34;) } opts := []grpc.ServerOption{ grpc.Creds( // 为所有传入的连接启用TLS credentials.NewTLS(\u0026amp;tls.Config{ ClientAuth: tls.RequireAndVerifyClientCert, Certificates: []tls.Certificate{certificate}, ClientCAs: certPool, }, )), } listen, err := net.Listen(\u0026#34;tcp\u0026#34;, fmt.Sprintf(\u0026#34;0.0.0.0:%d\u0026#34;, 50051)) if err != nil { log.Fatalf(\u0026#34;failed to listen %d port\u0026#34;, 50051) } // 通过传入的TLS服务器凭证创建新的gRPC服务实例 s := grpc.NewServer(opts...) api.RegisterGreeterServer(s, \u0026amp;server{}) log.Printf(\u0026#34;server listening at %v\u0026#34;, listen.Addr()) if err := s.Serve(listen); err != nil { log.Fatalf(\u0026#34;Failed to serve: %v\u0026#34;, err) } } func ErrorWithDetails() error { st := status.Newf(codes.Internal, fmt.Sprintf(\u0026#34;something went wrong: %v\u0026#34;, \u0026#34;api.Getter\u0026#34;)) v := \u0026amp;errdetails.PreconditionFailure_Violation{ //errDetails Type: \u0026#34;test\u0026#34;, Subject: \u0026#34;12\u0026#34;, Description: \u0026#34;32\u0026#34;, } br := \u0026amp;errdetails.PreconditionFailure{} br.Violations = append(br.Violations, v) st, _ = st.WithDetails(br) return st.Err() } We directly run the server go run main.go\nClient First, we use a request without a certificate\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func Test_server_SayHello_No_Cert(t *testing.T) { conn, err := grpc.Dial(\u0026#34;localhost:50051\u0026#34;, grpc.WithTransportCredentials(insecure.NewCredentials())) if err != nil { log.Fatalf(\u0026#34;Connect to %s failed\u0026#34;, \u0026#34;localhost:50051\u0026#34;) } defer conn.Close() client := api.NewGreeterClient(conn) // 创建带有超时时间的上下文, cancel可以取消上下文 ctx, cancel := context.WithTimeout(context.Background(), time.Second*5) defer cancel() // 业务代码处理部分 ... r, err := client.SayHello(ctx, \u0026amp;api.HelloRequest{Name: \u0026#34;Hello\u0026#34;}) if err != nil { log.Printf(\u0026#34;Failed to greet, error: %v\u0026#34;, err) } else { log.Printf(\u0026#34;Greeting: %v\u0026#34;, r.GetMessage()) } } Output\n1 2 2024/05/12 19:18:51 Failed to greet, error: rpc error: code = Unavailable desc = connection error: desc = \u0026#34;error reading server preface: EOF\u0026#34; Service is unavailable\nNow, let\u0026rsquo;s try a request carrying the certificate\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 func Test_server_SayHello(t *testing.T) { certificate, err := tls.LoadX509KeyPair(\u0026#34;./keys/client.crt\u0026#34;, \u0026#34;./keys/client.key\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to load client key pair, %v\u0026#34;, err) } certPool := x509.NewCertPool() ca, err := os.ReadFile(\u0026#34;./keys/ca.crt\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to read %s, error: %v\u0026#34;, \u0026#34;./keys/ca.crt\u0026#34;, err) } if ok := certPool.AppendCertsFromPEM(ca); !ok { log.Fatalf(\u0026#34;Failed to append ca certs\u0026#34;) } opts := []grpc.DialOption{ grpc.WithTransportCredentials(credentials.NewTLS( \u0026amp;tls.Config{ ServerName: \u0026#34;localhost\u0026#34;, Certificates: []tls.Certificate{certificate}, RootCAs: certPool, })), } conn, err := grpc.Dial(\u0026#34;localhost:50051\u0026#34;, opts...) if err != nil { log.Fatalf(\u0026#34;Connect to %s failed\u0026#34;, \u0026#34;localhost:50051\u0026#34;) } defer conn.Close() client := api.NewGreeterClient(conn) ctx, cancel := context.WithTimeout(context.Background(), time.Second*5) defer cancel() r, err := client.SayHello(ctx, \u0026amp;api.HelloRequest{Name: \u0026#34;Hello\u0026#34;}) if err != nil { log.Printf(\u0026#34;Failed to greet, error: %v\u0026#34;, err) } else { log.Printf(\u0026#34;Greeting: %v\u0026#34;, r.GetMessage()) } } Output\n1 2 === RUN Test_server_SayHello 2024/05/12 19:20:17 Greeting: Hello Hello Conclusion We can use tls to implement gRPC encryption communication, Starting from go1.15, the use of CA is not recommended, instead SAN certificates are utilized. ","date":"2024-05-13T09:15:00Z","image":"https://images.yixiao9206.cn/blog-images/2024/05/4976a194a8daca00ff6991a866c2ee53.png","permalink":"https://huizhou92.com/p/secure-communication-with-grpc-from-ssl/tls-certification-to-san-certification/","title":"Secure Communication with gRPC: From SSL/TLS Certification to SAN Certification"},{"content":"With the increasing popularity of open-source products, it is crucial for a backend engineer to be able to clearly identify whether an abnormal machine has been compromised. Based on my personal work experience, I have compiled several common scenarios of machines being hacked for reference.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nThe following scenarios are observed on CentOS systems and are similar for other Linux distributions.\n1. Intruders May Delete Machine Logs Check if log information still exists or has been cleared using the following commands:\n2. Intruders May Create a New File for Storing Usernames and Passwords Check /etc/passwd and /etc/shadow files for any alterations using the following commands:\n3. Intruders May Modify Usernames and Passwords Examine the contents of /etc/passwd and /etc/shadow files for any changes using the following commands:\n4. Check Recent Successful and Last Unsuccessful Login Events on the Machine Refer to the log \u0026ldquo;/var/log/lastlog\u0026rdquo; using the following commands:\n5. Use who to View All Currently Logged-in Users on the Machine Refer to the log file \u0026ldquo;/var/run/utmp\u0026rdquo;:\n6. Use last to View Users Logged in Since Machine Creation Refer to the log file \u0026ldquo;/var/log/wtmp\u0026rdquo;:\n7. Use ac to View Connection Time (in Hours) for All Users on the Machine Refer to the log file \u0026ldquo;/var/log/wtmp\u0026rdquo;:\n8. If Abnormal Traffic is Detected Use \u0026ldquo;tcpdump\u0026rdquo; to capture network packets or \u0026ldquo;iperf\u0026rdquo; to check traffic.\n9. Review the /var/log/secure Log File Attempt to identify information about intruders using the following commands:\n10. Identify Scripts Executed by Abnormal Processes a. Use the top command to view the PID of abnormal processes:\nb. Search for the executable file of the process in the virtual file system directory:\n11. File Recovery After Confirming Intrusion and Deletion of Important Files When a process opens a file, even if it\u0026rsquo;s deleted, it remains on the disk as long as the process keeps it open. To recover such files, use lsof from the /proc directory.\nMost lsof information is stored in directories named after the process\u0026rsquo;s PID, such as /proc/1234, containing information for PID 1234. Each process directory contains various files providing insight into the process\u0026rsquo;s memory space, file descriptor list, symbolic links to files on disk, and other system information. lsof uses this and other kernel internal state information to generate its output.\nUsing the information above, you can retrieve the data by examining /proc/\u0026lt;PID\u0026gt;/fd/\u0026lt;descriptor\u0026gt;.\nFor example, to recover /var/log/secure, follow these steps:\na. Check /var/log/secure, confirming its absence:\nb. Use lsof to check if any process is currently accessing /var/log/secure:\nc. From the information above, PID 1264 (rsyslogd) has opened the file with a file descriptor of 4. It\u0026rsquo;s marked as deleted. Therefore, you can check the corresponding information in /proc/1264/fd/4:\nd. You can recover the data by redirecting it to a file using I/O redirection:\ne. Confirm the existence of /var/log/secure again. This method is particularly useful for many applications, especially log files and databases.\nThe above is the method I summarized for dealing with Linux intrusion. It can generally handle most problems. If you encounter an unresolved issue, it is best to seek advice from a professional IT operations and maintenance engineer.\nI may not have written it completely correctly, so if you have different opinions, please leave a comment and let me know.\n","date":"2024-05-10T09:31:00Z","image":"https://images.yixiao9206.cn/blog-images/2024/05/8a6d3ceaf014801bc93c355d1e164d4e.png","permalink":"https://huizhou92.com/p/11-tips-for-detecting-and-responding-to-intrusions-on-linux/","title":"11 Tips for Detecting and Responding to Intrusions on Linux"},{"content":"I’ve spent the past few years building my second brain, and here are some lessons I’ve learned along the way.\nFrequent Switching of Note-Taking Software/Blogging Systems I’ve tried EverNote, WizNote, VNote, CSDN blog, Google blogspot, WordPress, only to end up scattering my blogs across multiple corners of the internet. The solution? Going all in one. I’ve now settled on Obsidian.\nConstantly Switching Note Formats I’ve experimented with txt, orgmode, markdown, rich text, and more, only to find myself frustrated with format conversions. Similar to the first point, each note system may not be universally compatible. The reason I chose Obsidian is for its markdown syntax. If need be, I can easily migrate it to any note system.\nMixing Fleeting Thoughts with Truly Useful Notes Fleeting thoughts serve to capture a moment of inspiration, but it’s only meaningful if you revisit them within a day or two and turn them into useful, relevant notes. Without timely review, good ideas drown in a sea of whims. Most of our daily thoughts are insignificant and should be discarded, while those with potential significance must be identified.\nMixing Project Notes with Knowledge Notes Only recording notes relevant to specific projects leads to the loss of interesting insights or ideas during the project. The correct approach is to extract universal knowledge from projects. I recommend using the P.A.R.A method to organize notes. For more information on P.A.R.A, you can refer to this page\nObsessive Note Organization A large pile of notes leads to an urge to organize knowledge, but too many attempts can affect the confidence to keep recording. The solution is to define the areas of interest and responsibility and not completely adopt a bottom-up knowledge management approach. When reaching the point of psychological pressure, use the MOCS (Maps of Content) method to organize notes (bidirectional linking is definitely worth trying). The most important aspect of a knowledge management system is to record your insights in one place, with the same format, and consistent standards.\n","date":"2024-05-08T23:46:56Z","image":"https://images.yixiao9206.cn/blog-images/2024/05/5ab6b54893dc2241704444526269572a.jpg","permalink":"https://huizhou92.com/p/crafting-your-second-brain-lessons-learned-from-my-note-taking-journey/","title":"Crafting Your Second Brain Lessons Learned from My Note-Taking Journey"},{"content":"Introduction In this article, we’ll continue our in-depth analysis of another high-performance network programming framework: nbio.\nThe nbio project also includes nbhttp built on top of nbio, but that\u0026rsquo;s outside the scope of our discussion.\nLike evio, nbio adopts the classic Reactor pattern. In fact, many asynchronous network frameworks in Go are designed based on this pattern.\nLet’s start by running the nbio program code.\nServer: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 package main ​ import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/lesismal/nbio\u0026#34; ) ​ func main() { g := nbio.NewGopher(nbio.Config{ Network: \u0026#34;tcp\u0026#34;, Addrs: []string{\u0026#34;:8888\u0026#34;}, MaxWriteBufferSize: 6 * 1024 * 1024, }) ​ g.OnData(func(c *nbio.Conn, data []byte) { c.Write(append([]byte{}, data...)) }) ​ err := g.Start() if err != nil { fmt.Printf(\u0026#34;nbio.Start failed: %v\\n\u0026#34;, err) return } ​ defer g.Stop() g.Wait() } Here, we create a new Engine instance using the nbio.NewGopher() function. We pass a nbio.Config struct to configure the Engine instance, including:\nNetwork: The type of network to use, which is \u0026ldquo;TCP\u0026rdquo; in this case. Addrs: The addresses and ports the server should listen to, here it\u0026rsquo;s \u0026ldquo;:8888\u0026rdquo; (listening on port 8888 of the local machine). MaxWriteBufferSize: The maximum size of the write buffer, is set to 6MB here. Other configurations can be explored further. Then, we register a data reception callback function using g.OnData() the Engine instance. This callback function is invoked when data is received. It takes two parameters: the connection object c and the received data data. Inside the callback function, we use c.Write() a method to write the received data back to the client.\nClient: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 package main ​ import ( \u0026#34;bytes\u0026#34; \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;math/rand\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/lesismal/nbio\u0026#34; \u0026#34;github.com/lesismal/nbio/logging\u0026#34; ) ​ func main() { var ( ret []byte buf = make([]byte, 1024*1024*4) addr = \u0026#34;localhost:8888\u0026#34; ctx, _ = context.WithTimeout(context.Background(), 60*time.Second) ) ​ logging.SetLevel(logging.LevelInfo) rand.Read(buf) ​ g := nbio.NewGopher(nbio.Config{}) done := make(chan int) ​ g.OnData(func(c *nbio.Conn, data []byte) { ret = append(ret, data...) if len(ret) == len(buf) { if bytes.Equal(buf, ret) { close(done) } } }) ​ err := g.Start() if err != nil { fmt.Printf(\u0026#34;Start failed: %v\\n\u0026#34;, err) } ​ defer g.Stop() ​ c, err := nbio.Dial(\u0026#34;tcp\u0026#34;, addr) if err != nil { fmt.Printf(\u0026#34;Dial failed: %v\\n\u0026#34;, err) } ​ g.AddConn(c) c.Write(buf) ​ select { case \u0026lt;-ctx.Done(): logging.Error(\u0026#34;timeout\u0026#34;) case \u0026lt;-done: logging.Info(\u0026#34;success\u0026#34;) } } At first glance, it might seem a bit cumbersome. Actually, the server and client share the same set of structures.\nThe client connects to the server through nbio.Dial, and upon successful connection, it encapsulates into nbio.Conn. Here, nbio.Conn implements the net.Conn interface of the standard library. Finally, it adds this connection via g.AddConn(c) and writes data to the server. When the server receives the data, its handling logic is to send the data back to the client as is. When the client receives the data, the OnData callback is triggered. This callback checks if the received data length matches the sent data length, and if so, it closes the connection.\nNow, let’s delve into a few key structures.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 type Engine struct { //... sync.WaitGroup //... mux sync.Mutex wgConn sync.WaitGroup network string addrs []string //... connsStd map[*Conn]struct{} connsUnix []*Conn listeners []*poller pollers []*poller onOpen func(c *Conn) onClose func(c *Conn, err error) onRead func(c *Conn) onData func(c *Conn, data []byte) onReadBufferAlloc func(c *Conn) []byte onReadBufferFree func(c *Conn, buffer []byte) //... } The Engine is essentially the core manager, responsible for managing all listeners, pollers, and worker pollers.\nWhat’s the difference between these two types of pollers?\nThe difference lies in their responsibilities.\nThe listener poller is responsible only for accepting new connections. When a new client conn arrives, it selects a worker poller from pollers and adds conn to the corresponding worker poller. Subsequently, the worker poller is responsible for handling the read/write events of this conn.\nTherefore, when we start the program, if only one address is being listened to, the number of polls in the program equals 1 (listener poller) + pollerNum.\nFrom the fields above, you can customize some configurations and callbacks. For example, you can set a callback function onOpen when a new connection arrives, or set a callback function onData when data arrives, etc.\n1 2 3 4 5 6 7 8 9 type Conn struct { mux sync.Mutex p *poller fd int //... writeBuffer []byte //... DataHandler func(c *Conn, data []byte) } The Conn structure represents a network connection. Each conn belongs to only one poller. writeBuffer: When data is not completely written at once, the remaining data is first stored in writeBuffer and waits for the next writable event to continue writing.\n1 2 3 4 5 6 7 8 9 10 11 12 type poller struct { g *Engine epfd int evtfd int index int shutdown bool listener net.Listener isListener bool unixSockAddr string ReadBuffer []byte pollType string } As for the poller structure, it\u0026rsquo;s an abstract concept used to manage underlying multiplexed I/O operations (such as epoll on Linux, kqueue on Darwin, etc.).\nPay attention to pollType, nbio defaults to epoll using level-triggered (LT) mode, but users can also set it to edge-triggered (ET) mode.\nAfter introducing the basic structures, let’s move on to the code flow.\nWhen you start the server code provided above, when you call Start:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 func (g *Engine) Start() error { //... switch g.network { // First part: initialize listener case \u0026#34;unix\u0026#34;, \u0026#34;tcp\u0026#34;, \u0026#34;tcp4\u0026#34;, \u0026#34;tcp6\u0026#34;: for i := range g.addrs { ln, err := newPoller(g, true, i) if err != nil { for j := 0; j \u0026lt; i; j++ { g.listeners[j].stop() } return err } g.addrs[i] = ln.listener.Addr().String() g.listeners = append(g.listeners, ln) } //... // Second part: initialize a certain number of pollers for i := 0; i \u0026lt; g.pollerNum; i++ { p, err := newPoller(g, false, i) if err != nil { for j := 0; j \u0026lt; len(g.listeners); j++ { g.listeners[j].stop() } for j := 0; j \u0026lt; i; j++ { g.pollers[j].stop() } return err } g.pollers[i] = p } //... // Third part: start all worker pollers for i := 0; i \u0026lt; g.pollerNum; i++ { g.pollers[i].ReadBuffer = make([]byte, g.readBufferSize) g.Add(1) go g.pollers[i].start() } // Fourth part: start all listeners for _, l := range g.listeners { g.Add(1) go l.start() } //... (ignore UDP) //... } The code is understandable. It’s divided into four parts:\nFirst part: initialize listener\nBased on the g.network value (e.g., \u0026ldquo;unix\u0026rdquo;, \u0026ldquo;tcp\u0026rdquo;, \u0026ldquo;tcp4\u0026rdquo;, \u0026ldquo;tcp6\u0026rdquo;), create a new poller for each address to listen on. This poller mainly manages events on the listening socket. If an error occurs during creation, stop all previously created listeners and return an error.\nSecond part: initialize a certain number of pollers\nCreate the specified number of worker pollers ( pollerNum). These pollers handle read/write events on connected sockets. If an error occurs during creation, stop all listeners and previously created worker pollers, and then return an error.\nThird part: start all worker pollers\nAssign a read buffer for each worker poller concurrently and start these pollers.\nFourth part: start all listeners\nStart all previously created listeners and begin listening for connection requests on respective addresses.\nRegarding the startup of pollers:\n1 2 3 4 5 6 7 8 9 10 11 12 13 func (p *poller) start() { defer p.g.Done() //... if p.isListener { p.acceptorLoop() } else { defer func() { syscall.Close(p.epfd) syscall.Close(p.evtfd) }() p.readWriteLoop() } } It’s divided into two cases. If it’s a listener poller:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 func (p *poller) acceptorLoop() { // If you want the current goroutine not to be scheduled to other operation threads. if p.g.lockListener { runtime.LockOSThread() defer runtime.UnlockOSThread() } p.shutdown = false for !p.shutdown { conn, err := p.listener.Accept() if err == nil { var c *Conn c, err = NBConn(conn) if err != nil { conn.Close() continue } // p.g.pollers[c.Hash()%len(p.g.pollers)].addConn(c) } else { var ne net.Error if ok := errors.As(err, \u0026amp;ne); ok \u0026amp;\u0026amp; ne.Timeout() { logging.Error(\u0026#34;NBIO[%v][%v_%v] Accept failed: temporary error, retrying...\u0026#34;, p.g.Name, p.pollType, p.index) time.Sleep(time.Second / 20) } else { if !p.shutdown { logging.Error(\u0026#34;NBIO[%v][%v_%v] Accept failed: %v, exit...\u0026#34;, p.g.Name, p.pollType, p.index, err) } break } } } } The listener poller waits for new connections and, upon arrival, encapsulates them into nbio.Conn after acceptance. Then, it adds the conn to the corresponding worker poller.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func (p *poller) addConn(c *Conn) { c.p = p if c.typ != ConnTypeUDPServer { p.g.onOpen(c) } fd := c.fd p.g.connsUnix[fd] = c err := p.addRead(fd) if err != nil { p.g.connsUnix[fd] = nil c.closeWithError(err) logging.Error(\u0026#34;[%v] add read event failed: %v\u0026#34;, c.fd, err) } } An interesting design here is the management of conns. The structure is a slice, and the author directly uses conn\u0026rsquo;s fd as the index. This has its benefits:\nWith a large number of connections, the burden during garbage collection is smaller compared to using a map. It prevents serial number issues. Finally, the corresponding conn fd is added to epoll by calling addRead.\n1 2 3 4 5 6 7 8 9 10 func (p *poller) addRead(fd int) error { switch p.g.epollMod { case EPOLLET: return syscall.EpollCtl(p.epfd, syscall.EPOLL_CTL_ADD, fd, \u0026amp;syscall.EpollEvent{Fd: int32(fd), Events: syscall.EPOLLERR | syscall.EPOLLHUP | syscall.EPOLLRDHUP | syscall.EPOLLPRI | syscall.EPOLLIN | syscall.EPOLLET}) default: return syscall.EpollCtl(p.epfd, syscall.EPOLL_CTL_ADD, fd, \u0026amp;syscall.E ​ pollEvent{Fd: int32(fd), Events: syscall.EPOLLERR | syscall.EPOLLHUP | syscall.EPOLLRDHUP | syscall.EPOLLPRI | syscall.EPOLLIN}) } } It’s reasonable not to register the write event here because there’s no data to send on a new connection. This approach avoids some unnecessary system calls, thereby enhancing program performance.\nIf it’s a worker poller’s startup, its job is to wait for events from the added conns and handle them accordingly.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 func (p *poller) readWriteLoop() { //... msec := -1 events := make([]syscall.EpollEvent, 1024) //... for !p.shutdown { n, err := syscall.EpollWait(p.epfd, events, msec) if err != nil \u0026amp;\u0026amp; !errors.Is(err, syscall.EINTR) { return } if n \u0026lt;= 0 { msec = -1 continue } msec = 20 // Traverse events for _, ev := range events[:n] { fd := int(ev.Fd) switch fd { case p.evtfd: default: c := p.getConn(fd) if c != nil { if ev.Events\u0026amp;epollEventsError != 0 { c.closeWithError(io.EOF) continue } // If it\u0026#39;s writable, flush the data if ev.Events\u0026amp;epollEventsWrite != 0 { c.flush() } // Read event if ev.Events\u0026amp;epollEventsRead != 0 { if p.g.onRead == nil { for i := 0; i \u0026lt; p.g.maxConnReadTimesPerEventLoop; i++ { buffer := p.g.borrow(c) rc, n, err := c.ReadAndGetConn(buffer) if n \u0026gt; 0 { p.g.onData(rc, buffer[:n]) } p.g.payback(c, buffer) //... if n \u0026lt; len(buffer) { break } } } else { p.g.onRead(c) } } } else { syscall.Close(fd) } } } } } This piece of code is also straightforward. It waits for events to arrive, traverses the event list, and handles each event accordingly.\n1 func EpollWait(epfd int, events []EpollEvent, msec int) (n int, err error) In EpollWait, only msec is user-modifiable. Usually, we set msec = -1 to make the function block until at least one event occurs; otherwise, it blocks indefinitely. This method is very useful when there are few events because it minimizes CPU usage.\nIf you want to respond to events as quickly as possible, you can set msec = 0. This makes EpollWait return immediately without waiting for any events. In this case, your program may call EpollWait more frequently, but it can process events immediately after they occur, leading to higher CPU usage.\nIf your program can tolerate some delay and you want to reduce CPU usage, you can set msec it to a positive number. This makes EpollWait waiting for events for the specified time. If no events occur during this time, the function returns, and you can choose to call EpollWait again later. This method can reduce CPU usage but may result in longer response times.\nNbio adjusts the value msec according to event counts. If the count is greater than 0, msec is set to 20.\nThe code for ByteDance’s netpoll is similar; if the event count is greater than 0, msec it is set to 0. If the event count is less than or equal to 0, msec it is set to -1, and then Gosched() is called to voluntarily yield the current Goroutine.\n1 2 3 4 5 6 7 8 9 10 11 var msec = -1 for { n, err = syscall.EpollWait(epfd, events, msec) if n \u0026lt;= 0 { msec = -1 runtime.Gosched() continue } msec = 0 ... } However, the code for voluntary switching in nbio has been commented out. According to the author’s explanation in the issue, initially, he referred to ByteDance’s method and added voluntary switching.\nHowever, during performance testing of nbio, it was found that adding or not adding voluntary switching did not significantly affect performance. Therefore, it was ultimately decided to remove it.\nThe processing part of the event.\nIf it is a readable event, you can obtain the corresponding buffer through built-in or custom memory allocators, and then call ReadAndGetConn to read the data without needing to allocate a buffer every time.\nIf it is a writable event, flush will be called to send out the unsent data in the buffer.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 func (c *Conn) flush() error { //..... old := c.writeBuffer n, err := c.doWrite(old) if err != nil \u0026amp;\u0026amp; !errors.Is(err, syscall.EINTR) \u0026amp;\u0026amp; !errors.Is(err, syscall.EAGAIN) { //..... } ​ if n \u0026lt; 0 { n = 0 } left := len(old) - n // The description is not finished, so store the rest in writeBuffer for next writing. if left \u0026gt; 0 { if n \u0026gt; 0 { c.writeBuffer = mempool.Malloc(left) copy(c.writeBuffer, old[n:]) mempool.Free(old) } // c.modWrite() } else { mempool.Free(old) c.writeBuffer = nil if c.wTimer != nil { c.wTimer.Stop() c.wTimer = nil } // The explanation is finished, reset the conn to only have read events first. c.resetRead() //... } ​ c.mux.Unlock() return nil } The logic is also very simple, write as much as there is, if it cannot be written, put the remaining data back into the writeBuffer and write again when epollWait triggers.\nIf writing is completed, then there is no more data to be written, reset the event of this connection to a read event.\nThat’s basically how the main logic works.\nWait a minute, when we initially mentioned that a new connection comes in, we only registered a read event for the connection and didn’t register a write event. When was the write event registered?\nOf course, it is registered when you call conn.Write.\n1 2 3 4 5 6 7 8 9 g := nbio.NewGopher(nbio.Config{ Network: \u0026#34;tcp\u0026#34;, Addrs: []string{\u0026#34;:8888\u0026#34;}, MaxWriteBufferSize: 6 * 1024 * 1024, }) ​ g.OnData(func(c *nbio.Conn, data []byte) { c.Write(append([]byte{}, data...)) }) When conn data arrives, the bottom layer will call back the OnData function after reading the data. At this time, you can call Write to send data to the other end.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 g := nbio.NewGopher(nbio.Config{ Network: \u0026#34;tcp\u0026#34;, Addrs: []string{\u0026#34;:8888\u0026#34;}, MaxWriteBufferSize: 6 * 1024 * 1024, }) ​ g.OnData(func(c *nbio.Conn, data []byte) { c.Write(append([]byte{}, data...)) }) // When data arrives on conn, the underlying layer will read the data and callback the OnData function. At this time, you can call Write to send data to the other end. ​ func (c *Conn) Write(b []byte) (int, error) { //.... n, err := c.write(b) if err != nil \u0026amp;\u0026amp; !errors.Is(err, syscall.EINTR) \u0026amp;\u0026amp; !errors.Is(err, syscall.EAGAIN) { //..... return n, err } ​ if len(c.writeBuffer) == 0 { if c.wTimer != nil { c.wTimer.Stop() c.wTimer = nil } } else { // There is still data that has not been written, add a write event. c.modWrite() } //..... return n, err } ​ func (c *Conn) write(b []byte) (int, error) { //... if len(c.writeBuffer) == 0 { n, err := c.doWrite(b) if err != nil \u0026amp;\u0026amp; !errors.Is(err, syscall.EINTR) \u0026amp;\u0026amp; !errors.Is(err, syscall.EAGAIN) { return n, err } //..... left := len(b) - n // Not finished yet, put the remaining into writeBuffer. if left \u0026gt; 0 \u0026amp;\u0026amp; c.typ == ConnTypeTCP { c.writeBuffer = mempool.Malloc(left) copy(c.writeBuffer, b[n:]) c.modWrite() } return len(b), nil } // If there is still unwritten data in the writeBuffer, the new data will also be appended. c.writeBuffer = mempool.Append(c.writeBuffer, b...) ​ return len(b), nil } When the data is not completely written, the remaining data is put into writeBuffer, which will trigger the execution of modWrite and register the write event of conn to epoll.\nSummary Compared to Evio, nbio does not have a thundering herd effect.\nEvio achieves logical correctness by constantly waking up epoll invalidly. Nbio tries to minimize system calls and reduce unnecessary overhead.\nIn terms of usability, nbio implements a standard library net.Conn and many settings are configurable, allowing users to customize with high flexibility.\nPre-allocated buffers are used for reading and writing to improve application performance.\nIn conclusion, nbio is a good high-performance non-blocking network framework.\n","date":"2024-05-05T12:24:00Z","image":"https://images.yixiao9206.cn/blog-images/2024/05/8876d50feac67c7fe0d47c28960064e5.png","permalink":"https://huizhou92.com/p/analyzing-high-performance-network-framework-nbio-in-go/","title":"Analyzing High-Performance Network Framework nbio in Go"},{"content":" source ：https://tonybai.com/2024/04/22/gopher-rust-first-lesson-all-about-rust\nTo talk about which backend programming language has been the hottest in the past two years,If Rust claims to be second , and no one dares to say first .\nRust has topped the Stack Overflow\u0026rsquo;s most admired programming languages for 8 consecutive years, and it has even been referred to as the \u0026ldquo;perfect programming language\u0026rdquo; by Jack Dorsey, the co-founder of Twitter:\nrust is a perfect programming language\n\u0026mdash; jack (@jack) December 24, 2021 1. Why should I learn Rust? Learning Rust isn\u0026rsquo;t about riding the hype train; it\u0026rsquo;s about practical development needs. In certain situations, Go\u0026rsquo;s performance isn\u0026rsquo;t particularly good due to issues like STW. From benchmarks, Rust\u0026rsquo;s performance is three times better than Go\u0026rsquo;s: The Computer Language Benchmarks Game Visualization In our own business, after rewriting a gateway service in Rust, performance improved by about 70%. This is a significant improvement for our business scenario. Moreover, Rust\u0026rsquo;s widespread use in the Linux kernel validates its reliability, making Rust worth trying.\nHowever, the rise of Rust in certain domains has indeed sparked some dissatisfaction and controversy in other programming language communities. Particularly, the proposition from some in the Rust community to \u0026ldquo;Rewrite Everything in Rust\u0026rdquo; has made many programming language communities, especially the C++ community, quite uneasy. The Go community, on the other hand, is relatively more open and friendly. The mainstream view is that Go and Rust can complement each other, with each language playing its role in its respective areas of strength. Through cooperation rather than confrontation, developers can be provided with better choices. For more details, you can refer to an article co-authored by Steve Francia, former product manager of the Go team and author of Hugo, Rust vs. Go: Why They’re Better Together.\nIn other words, Go is still my primary language, but considering the requirements of my work, I need to systematically learn Rust. To avoid the \u0026ldquo;from learning to giving up\u0026rdquo; scenario, I plan to learn Rust while outputting. On one hand, this can motivate me to learn, and on the other hand, I hope to interact with readers promptly and correct any misunderstandings in learning.\nI\u0026rsquo;ve always believed that when you start learning a new language, you must understand its history and current status. This way, you can build an overall understanding of the language and anticipate its future direction. Moreover, it can establish a sense of \u0026ldquo;security\u0026rdquo; in learning, believing that it can bring you enough value and returns, thus enabling you to learn more confidently.\nIn this article, I\u0026rsquo;ll first explore the history of Rust\u0026rsquo;s development and its current state, as well as its unique design philosophy. I\u0026rsquo;ll also make a simple comparison with Go, hoping to provide myself and the readers with a preliminary understanding of Rust.\n2. Rust\u0026rsquo;s History and Current Status 2.1 The Birth and Evolution of Rust Rust was born in 2006, a year earlier than the \u0026ldquo;conspiracy\u0026rdquo; of the three Google giants to create the Go language. However, compared to the three founders of Go: Ken Thompson, Turing Award winner, co-inventor of C syntax, and father of Unix; Rob Pike, leader of the Plan 9 operating system and original designer of UTF-8 encoding; and Robert Griesemer, one of the designers of the HotSpot virtual machine for Java and the JavaScript V8 engine for the Chrome browser, the identity and status of Rust\u0026rsquo;s father, Graydon Hoare, were not so \u0026ldquo;prominent\u0026rdquo;. At that time, he was just a Canadian developer under 30 years old working at Mozilla Research:\nThe birth of a new generation programming language often comes with a story, such as the founders of Go frequently encountering long compile times in C++ projects at Google. Whenever they started compiling a C++ project, they had to wait for a long time, during which they could drink several cups of coffee. This deeply impressed them and made them realize the need to design a new language with faster compilation speed, thus Go was born. Similar to the \u0026ldquo;drink coffee, wait for C++ project to compile\u0026rdquo; scenario, the birth of Rust also has a story:\nIn 2006, 29-year-old Hoare returned home to Vancouver one day, only to find that the elevator was broken, and the elevator software crashed! He had to climb the stairs back to his 21st-floor apartment. As he climbed the stairs, he felt very annoyed. He thought, \u0026ldquo;We programmers actually can\u0026rsquo;t create an elevator that can work properly without crashing!\u0026rdquo; Hoare knew that many such crashes were caused by issues with memory usage in programs. The software inside devices like elevators is usually written in C++ or C, languages known for allowing programmers to write code that runs very fast and is quite compact. The problem is that these languages also make it easy to inadvertently introduce memory errors, which can lead to crashes. Hoare decided to do something about it. So he opened his laptop and started designing a new programming language, one that he hoped could write small and fast code without memory errors, and he named it Rust.\nThis story is obviously unverifiable. But what can be confirmed is that for several years starting from 2006, the personal language project Rust created by Hoare was not actually used to improve elevator systems, but was sponsored by Mozilla and used in the continuous development of Mozilla\u0026rsquo;s browser engine Servo. Mozilla officially announced the project in 2010, and Hoare also introduced the Rust language for the first time in a speech in 2010:\nThe first line of Rust code was also open-sourced in 2010:\nIn addition, the initial Rust compiler was implemented in OCaml, and in 2011, the Rust team re-implemented the compiler in Rust based on LLVM and achieved bootstrapping. That same year, Rust also got its own logo, inspired by a bicycle gear:\nIn 2012, Graydon Hoare was interviewed by InfoQ and talked about leading the Rust team in developing Rust, a systems programming language at Mozilla, including Rust\u0026rsquo;s features, advantages and differences compared to C/C++/Java/Go, and Rust 1.0 release plans.\nHowever, in the following year, in 2013, Graydon Hoare resigned as leader of the Rust team due to exhaustion, leaving his own Rust team and distancing himself from Rust development. Hoare\u0026rsquo;s departure was a significant loss for the Rust team and the language itself, but the Rust community and team took proactive measures to ensure the continued development and evolution of Rust.\nIn November 2014, Rust officially announced cargo and crates.io. Cargo is Rust\u0026rsquo;s project build manager, while crates.io is the central package repository for Rust code maintained by the Rust team. With cargo, developers can easily build and publish packages to crates.io or pull dependencies of Rust code from crates.io.\nOn May 15, 2015, Rust reached a milestone moment: Rust 1.0 was officially released!, which was 3 years later than the release of Go 1.0. However, as the official blog stated, \u0026ldquo;the release of version 1.0 marks the end of the chaos. This version is the formal beginning of our stability commitment, providing a solid foundation for building applications and libraries. From now on, major changes are essentially out of scope (some minor warnings apply, such as compiler errors).\u0026rdquo;\nAfter the release of Rust 1.0, the release cycle and rhythm of Rust were determined, with a stable version released every 6 weeks. Following this rhythm, Rust 1.1 Beta was released simultaneously with Rust 1.0. After six weeks of testing, Rust 1.1 Beta became Rust 1.1 stable, and Rust 1.2 Beta was released, and so on. Of course, Rust also has a nightly build version, which contains the latest but unstable features. Compared to the Go community and developers who can only \u0026ldquo;get high\u0026rdquo; twice a year, Rust developers and the community are more fortunate to \u0026ldquo;get high\u0026rdquo; every six weeks!\nRust\u0026rsquo;s evolution is driven by RFCs (Request For Comments), and this measure was established based on RFCs before the release of Rust 1.0. This is similar to the Go Proposal process, but it feels more standardized and rigorous, which of course is related to the composition and rules of governance structures of the two languages.\nHowever, the\nevolution and development of Rust is not as smooth and perfect as one might expect. For example, at the Rust 2018 edition launch event, Alex Crichton, one of the core members of the Rust team, mentioned in the keynote speech that in the evolution of the Rust language, some past design decisions may not have been optimal, which sometimes hinders the language\u0026rsquo;s progress. One example is the error handling mechanism. However, Rust\u0026rsquo;s development team has proven that they are good at introspection and adjustment. For example, the aforementioned error handling mechanism was greatly improved with the introduction of the \u0026ldquo;Try\u0026rdquo; trait in Rust 1.26, and with the release of Rust 1.31, the Rust 2018 edition resolved many issues with Rust\u0026rsquo;s design.\n2.2 Rust\u0026rsquo;s Current Status In the past decade, Rust has gradually developed from a personal project of Graydon Hoare to a popular systems programming language with a large and active community. According to GitHub\u0026rsquo;s 2020 Octoverse report, Rust was the fastest-growing programming language in the GitHub open-source community, with 235% more contributors than in 2019, making it the most loved language in the Stack Overflow Developer Survey for five consecutive years from 2016 to 2020.\nHowever, it should be pointed out that Rust is still not widely used in large-scale commercial projects, especially compared to C/C++/Java/Go. The main reason is that Rust\u0026rsquo;s ecosystem and maturity are still not comparable to those languages. For example, the Alibaba e-commerce team mentioned in 2017 that they have 100,000+ Java developers and have invested heavily in Java and the JVM ecosystem. They also mentioned that in recent years, Alibaba has considered using Rust to solve some distributed computing problems, but has not yet considered using Rust in large-scale commercial projects due to concerns about the immaturity of Rust\u0026rsquo;s ecosystem.\nHowever, Rust is gradually being adopted and applied in some fields and areas where C/C++ is traditionally dominant, such as systems programming, game development, and blockchain systems. For example, in the blockchain field, in addition to the Solana project I mentioned earlier, the Parity project, which is known for the development of the Polkadot blockchain framework, is also developed in Rust. In addition, in the blockchain field, some small and medium-sized blockchain projects also use Rust to develop blockchain systems. For example, the Bitcoin Light client library BTC-Rust implemented by the Bitcoin-NG team is developed in Rust.\n3. The Design Philosophy of Rust The design philosophy of a programming language is like the values of a person—it dictates its behavior. If you don\u0026rsquo;t agree with someone\u0026rsquo;s values, it\u0026rsquo;s hard to maintain a continuous relationship, as they say, \u0026ldquo;different paths do not converge.\u0026rdquo; Similarly, if you don\u0026rsquo;t agree with the design philosophy of a programming language, you\u0026rsquo;ll likely encounter the problems mentioned earlier in your subsequent language learning, which may dampen your motivation to continue learning. Therefore, before diving into Rust syntax and coding, let\u0026rsquo;s first understand Rust\u0026rsquo;s design philosophy. After understanding these, you\u0026rsquo;ll have a deeper understanding of why you\u0026rsquo;re learning Rust.\n3.1 Core Values of Rust In June 2019, Rust core team member Stephen Klabnik delivered a speech titled How Rust Views Tradeoffs at QCon London, where he outlined his personal understanding of Rust\u0026rsquo;s core values, which are the points Rust\u0026rsquo;s team refuses to compromise on when making design decisions, including memory safety, execution speed, and productivity:\nAccording to Stephen Klabnik, these three core values are ordered, with memory safety being paramount, followed by high performance, and finally productivity. When conflicts arise among them, decisions are made according to the highest value!\nThis is consistent with the official description of Rust:\nThe \u0026ldquo;Reliable\u0026rdquo; corresponds to memory safety, while \u0026ldquo;efficient\u0026rdquo; has two meanings: runtime efficiency and high productivity during development.\nThese three values are the design goals of the Rust language and the essence of its characteristics and advantages. After losing Graydon Hoare, the father of the language, these values became the fundamental basis for the Rust core team to determine the direction of language evolution.\nMemory Safety\nMemory safety is the most important value for Rust. It means that Rust programs won\u0026rsquo;t suffer from memory leaks, buffer overflows, dangling pointers, and other memory-related errors at runtime (without using unsafe code). These errors not only lead to program crashes but can also result in security vulnerabilities. Rust ensures memory safety by features like ownership, lifetimes, and borrowing, which are thoroughly checked at compile time. Rust\u0026rsquo;s memory safety mechanism not only enhances program stability and reliability but also reduces development and maintenance complexity. With Rust\u0026rsquo;s ability to detect memory errors at compile time, developers don\u0026rsquo;t have to spend a lot of time and effort searching for and fixing these errors.\nHigh Performance High performance is the second core value of Rust, closely following memory safety. One of Rust\u0026rsquo;s design goals is to be a high-performance systems programming language. Through features like zero-cost abstractions, move semantics, and generic programming, Rust enables programs to achieve performance comparable to traditional systems programming languages like C and C++ at runtime.\nRust\u0026rsquo;s high-performance mechanism not only improves program execution speed but also reduces hardware costs. Because Rust can better utilize hardware resources, Rust programs typically outperform programs in other languages with the same hardware conditions and resource overhead.\nProductivity Productivity is the third core value of Rust. One of Rust\u0026rsquo;s design goals is to be a language that enhances developer productivity. With features like the Cargo package manager, smart editor support, rich library ecosystem, and detailed system documentation, Rust makes it easier for developers to write, debug, and maintain Rust programs.\n3.2 Secondary Values of Rust Stephen Klabnik also summarized three secondary values of Rust:\nWe see that Rust\u0026rsquo;s secondary values include ergonomics, compile times, and correctness. These values are also design goals of the Rust language, but unlike the primary core values mentioned above, they are subject to compromise.\nErgonomics refers to the ease of use of the Rust language, which is an important design goal of Rust. Rust aims to make it easier for developers to write Rust programs through simple syntax and a rich library ecosystem.\nCompile Times refer to the compilation time of the Rust compiler. Rust\u0026rsquo;s compiler is slow, which is a problem that the Rust team is actively working to optimize. However, the Rust team is more concerned about the final execution speed of the binary than making the compiler faster, hence why compile time is a secondary value.\nCorrectness refers to the correctness of Rust programs. Rust really cares about whether your program is correct and aims to ensure the correctness of Rust programs as much as possible through a powerful type system and static checks. However, Rust is not willing to rely entirely on types and proof assistants to prove the correctness of your code.\n3.3 Comparison with Go\u0026rsquo;s Values Let\u0026rsquo;s compare Go\u0026rsquo;s official introduction to Go\u0026rsquo;s implicit values (design philosophy):\nIn the official description of Go, there are three keywords: Simple, Secure, and Scalable.\nSimple is the primary design principle of Go. The designers of Go hope that Go can be simple and easy to use, enabling developers to learn and use Go more quickly to rapidly develop production capabilities. Go has a simple and understandable syntax, and it removes many complex features found in other programming languages, such as type hierarchies and inheritance, making Go more concise, easy to learn, read, use, and maintain.\nSecure is about making Go more secure and reliable, avoiding common security vulnerabilities found in many other programming languages. Go achieves this by automatically managing memory through garbage collection, avoiding common memory leaks and buffer overflow issues found in many other programming languages. Additionally, Go provides lightweight goroutines and channels, making it easier for developers to implement concurrent programming. With data race detection tools, Go also prevents common data race issues in concurrent programming. Furthermore, Go provides a simple and easy-to-use explicit error handling mechanism, ensuring no error handling is missed by developers.\nScalable is reflected in Go\u0026rsquo;s engineering orientation, built-in concurrency, and a philosophy that emphasizes composition. The designers of Go hope that Go can better support scalability, enabling Go programs to better adapt to different organizational scales, workloads, and hardware environments. Go achieves this through simple syntax, module-based reproducible build management, extremely fast compilation speed, high-quality standard library, practical toolchain, powerful built-in concurrency mechanism, and interface-oriented programming, making Go programs more scalable and productive.\n4 Conclusion In summary, Rust prioritizes safety, low-level control, and optimal performance, while Go emphasizes simplicity, security, scalability, and engineering efficiency. There are differences in their positioning and design philosophies, but they also share some common characteristics, such as modern toolchains and active communities.\nIn this article, we\u0026rsquo;ve learned about the birth, current development, and unique design philosophy of Rust. By comparing it with Go, we can see some differences in their backgrounds, goals, and design philosophies.\nAs software systems become increasingly complex, the demand for security, performance, and concurrency is also rising. As a new language focused on low-level systems programming and performance optimization, Rust is attracting more and more developers\u0026rsquo; attention. I believe that through comprehensive and systematic learning of Rust later on, we will all gain a deeper understanding and mastery of Rust.\nIf you find Rust\u0026rsquo;s values align with yours and you agree with Rust\u0026rsquo;s future development, stay tuned for the next article, where we\u0026rsquo;ll start hands-on learning Rust!\n5 References Rust Wikipedia How Rust went from a side project to the world’s most-loved programming language 2022 Review | The adoption of Rust in Business How Rust Views Tradeoffs Unofficial Rust mascot Ferris ","date":"2024-04-26T09:43:00Z","permalink":"https://huizhou92.com/p/rust-lesson-1-a-gophers-view-on-rust/","title":"Rust Lesson 1： A Gopher's View on Rust"},{"content":"Handling time is a common task for programmers. In Go, the standard library time provides the necessary capabilities.\nThis article will introduce some important functions and methods in the time package, hoping to help those who often need to deal with time-related issues in Go.\nHandle Time Zones\nIn programming, we often encounter the issue of an eight-hour time difference. This is caused by differences in time zones. To better handle them, we need to understand several time definition standards.\nGMT (Greenwich Mean Time) is based on the Earth\u0026rsquo;s rotation and revolution to calculate time. It defines noon as the time when the sun passes through the Royal Greenwich Observatory in the suburbs of London, UK. GMT was the former world standard time.\nUTC (Coordinated Universal Time) is more precise than GMT, calculated based on atomic clocks. In situations where precision to the second is not required, UTC can be considered equivalent to GMT. UTC is the current world standard time.\nFrom the Prime Meridian at Greenwich, going east is positive, going west is negative. The globe is divided into 24 standard time zones, with neighboring time zones differing by one hour.\n1 2 3 4 5 6 7 8 9 10 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { fmt.Println(time.Now()) } In mainland China, the standard time used is in the GMT+8 time zone, known as China Standard Time (CST).\n1 2 $ go run main.go 2022-07-17 16:37:31.186043 +0800 CST m=+0.000066647 This is the result under the default time zone, and +0800 CST is indicated in the time.Now() printout.\nSuppose we are in the Los Angeles time zone in the United States, what result do we get?\n1 2 $ TZ=\u0026#34;America/Los_Angeles\u0026#34; go run main.go 2022-07-17 01:39:12.391505 -0700 PDT m=+0.000069514 As seen, the result at this time is -0700 PDT, which is Pacific Daylight Time. Due to the time zone difference, the results of the two executions differ by 15 hours.\nNote that when using Docker containers, the system\u0026rsquo;s default time zone is UTC time (0 time zone), which is eight hours behind Beijing time as we need, leading to the classic scenario of the eight-hour time difference problem.\nStrategies for dealing with time zone issues can be found in detail in the loading logic of the initLocal() function in src/time/zoneinfo_unix.go. For example, you can solve it by specifying the environment variable TZ or modifying the /etc/localtime file.\nBecause time zone issues are very important, they are discussed in the first part of the article. Let\u0026rsquo;s now move on to the usage of the time package.\nTime Instant time.Time The core object in the time package is the time.Time struct. Its definition, used to represent a specific moment in time, is as follows:\n1 2 3 4 5 6 7 8 type Time struct { // wall and ext encode the wall time seconds, // wall time nanoseconds, and optional monotonic // clock reading in nanoseconds. wall uint64 ext int64 loc *Location } In computer time handling, two types of clocks are mainly involved:\nWall clock, also known as clock time, represents specific dates and times. Monotonic clocks, which always guarantee that time moves forward without the issue of wall clock rollback, making them suitable for measuring durations. The wall and ext fields are used to record wall clock and monotonic clock times with nanosecond precision. The bits of these fields are associated with specific information such as years, months, days, hours, minutes, and seconds.\nThe loc field records the time zone location. When loc is nil, it defaults to UTC.\nBecause time.Time is used to represent time instants with nanosecond precision, it is typically stored and passed as a value in programs, rather than a pointer.\nThat is, in time variables or struct fields, we should use time.Time rather than *time.Time.\nGetting time.Time We can get the current local time using the Now function:\n1 func Now() Time {} Or, using the Date function, we can get a specified time based on the year, month, day, and other parameters, along with the time zone:\n1 func Date(year int, month Month, day, hour, min, sec, nsec int, loc *Location) Time {} Converting Timestamps In the computer world, UTC time 0 on January 1, 1970, is considered Unix time 0. To convert a time instant into a Unix timestamp, we calculate the number of seconds, microseconds, etc., elapsed from Unix time 0 to the specified instant.\n1 2 3 4 func (t Time) Unix() int64 // Seconds since Unix time 0 func (t Time) UnixMicro() int64 // Microseconds since Unix time 0 func (t Time) UnixMilli() int64 // Milliseconds since Unix time 0 func (t Time) UnixNano() int64 // Nanoseconds since Unix time 0 Getting Basic Fields 1 2 3 4 5 6 7 8 9 10 11 12 13 t := time.Now() fmt.Println(t.Date()) // 2022 July 17 fmt.Println(t.Year()) // 2022 fmt.Println(t.Month()) // July fmt.Println(t.ISOWeek()) // 2022 28 fmt.Println(t.Clock()) // 22 21 56 fmt.Println(t.Day()) // 17 fmt.Println(t.Weekday()) // Sunday fmt.Println(t.Hour()) // 22 fmt.Println(t.Minute()) // 21 fmt.Println(t.Second()) // 56 fmt.Println(t.Nanosecond()) // 494313000 fmt.Println(t.YearDay()) // 198 Duration time.Duration time.Duration represents the time elapsed between two time.Time instants. It uses an int64 to represent the count of nanoseconds, allowing for approximately 290 years of representation.\n1 2 3 4 // A Duration represents the elapsed time between two instants // as an int64 nanosecond count. The representation limits the // largest representable duration to approximately 290 years. type Duration int64 In Go, time.Duration is simply a number in nanoseconds. If a duration is equal to 1000000000, it represents 1 second, or 1000 milliseconds, or 1000000 microseconds, or 1000000000 nanoseconds.\nFor example, the duration between two time instants separated by 1 hour can be calculated as:\n1 1*60*60*1000*1000*1000 The time package defines constant values for these durations:\n1 2 3 4 5 6 7 8 9 10 const ( Nanosecond Duration = 1 Microsecond = 1000 * Nanosecond Millisecond = 1000 * Microsecond Second = 1000 * Millisecond Minute = 60 * Second Hour = 60 * Minute ) Additionally, time.Duration provides methods to get values at various time granularities:\n1 2 3 4 5 6 func (d Duration) Nanoseconds() int64 // Nanoseconds func (d Duration) Microseconds() int64 // Microseconds func (d Duration) Milliseconds() int64 // Milliseconds func (d Duration) Seconds() float64 // Seconds func (d Duration) Minutes() float64 // Minutes func (d Duration) Hours() float64 // Hours Time Calculation After learning about time instants and durations, let\u0026rsquo;s see how to perform time calculations.\n1 func (t Time) Add(d Duration) Time {} Add adds or subtracts (positive d means addition, negative d means subtraction) a time.Duration to a time.Time. We can add or subtract durations of specific nanosecond levels to a specific instant in time. 1 func (t Time) Sub(u Time) Duration {} Sub returns the duration between two time instants. 1 func (t Time) AddDate(years int, months int, days int) Time {} AddDate adds or subtracts values based on the year, month, and day dimensions to a time.Time. Of course, calculating based on the current time instant time.Now() is the most common requirement. Therefore, the time package also provides the following convenient time calculation functions:\n1 func Since(t Time) Duration {} Since is a shortcut for time.Now().Sub(t).\n1 func Until(t Time) Duration {} Until is a shortcut for t.Sub(time.Now()).\nUsage Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 t := time.Now() fmt.Println(t) // 2022-07-17 22:41:06.001567 +0800 CST m=+0.000057466 // Adding 1 hour to the time fmt.Println(t.Add(time.Hour * 1)) // 2022-07-17 23:41:06.001567 +0800 CST m=+3600.000057466 // Adding 15 minutes fmt.Println(t.Add(time.Minute * 15)) // 2022-07-17 22:56:06.001567 +0800 CST m=+900.000057466 // Adding 10 seconds fmt.Println(t.Add(time.Second * 10)) // 2022-07-17 22:41:16.001567 +0800 CST m=+10.000057466 // Subtracting 1 hour fmt.Println(t.Add(-time.Hour * 1)) // 2022-07-17 21:41:06.001567 +0800 CST m=-3599.999942534 // Subtracting 15 minutes fmt.Println(t.Add(-time.Minute * 15)) // 2022-07-17 22:26:06.001567 +0800 CST m=-899.999942534 // Subtracting 10 seconds fmt.Println(t.Add(-time.Second * 10)) // 2022-07-17 22:40:56.001567 +0800 CST m=-9.999942534 time.Sleep(time.Second * 5) t2 := time.Now() // Calculating the duration from t to t2 fmt.Println(t2.Sub(t)) // 5.004318874s // Time after 1 year t3 := t2.AddDate(1, 0, 0) // Calculating the duration from t to the current time fmt.Println(time.Since(t)) // 5.004442316s // Calculating the duration from now to next year fmt.Println(time.Until(t3)) // 8759h59m59.999864s Formatting Time In other languages, a universal time template is typically used to format time. For example, in Python, %Y represents year, %m represents month, %d represents day, and so on.\nHowever, Go is different. It uses a fixed time (it\u0026rsquo;s important to note that using other times is not allowed) as the layout template, and this fixed time is the birth time of the Go language.\n1 Mon Jan 2 15:04:05 MST 2006 Formatting time involves two conversion functions:\n1 func Parse(layout, value string) (Time, error) {} Parse converts a time string to a time.Time object based on the layout it can correspond to. 1 func (t Time) Format(layout string) string {} Format converts a time.Time object to a time string based on the given layout. Example 1 2 3 4 5 6 7 8 const ( layoutISO = \u0026#34;2006-01-02\u0026#34; layoutUS = \u0026#34;January 2, 2006\u0026#34; ) date := \u0026#34;2012-08-09\u0026#34; t, _ := time.Parse(layoutISO, date) fmt.Println(t) // 2012-08-09 00:00:00 +0000 UTC fmt.Println(t.Format(layoutUS)) // August 9, 2012 In the time package, Go provides some predefined layout template constants that can be directly used.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 const ( Layout = \u0026#34;01/02 03:04:05PM \u0026#39;06 -0700\u0026#34; // The reference time, in numerical order. ANSIC = \u0026#34;Mon Jan _2 15:04:05 2006\u0026#34; UnixDate = \u0026#34;Mon Jan _2 15:04:05 MST 2006\u0026#34; RubyDate = \u0026#34;Mon Jan 02 15:04:05 -0700 2006\u0026#34; RFC822 = \u0026#34;02 Jan 06 15:04 MST\u0026#34; RFC822Z = \u0026#34;02 Jan 06 15:04 -0700\u0026#34; // RFC822 with numeric zone RFC850 = \u0026#34;Monday, 02-Jan-06 15:04:05 MST\u0026#34; RFC1123 = \u0026#34;Mon, 02 Jan 2006 15:04:05 MST\u0026#34; RFC1123Z = \u0026#34;Mon, 02 Jan 2006 15:04:05 -0700\u0026#34; // RFC1123 with numeric zone RFC3339 = \u0026#34;2006-01-02T15:04:05Z07:00\u0026#34; RFC3339Nano = \u0026#34;2006-01-02T15:04:05.999999999Z07:00\u0026#34; Kitchen = \u0026#34;3:04PM\u0026#34; // Handy time stamps. Stamp = \u0026#34;Jan _2 15:04:05\u0026#34; StampMilli = \u0026#34;Jan _2 15:04:05.000\u0026#34; StampMicro = \u0026#34;Jan _2 15:04:05.000000\u0026#34; StampNano = \u0026#34;Jan _2 15:04:05.000000000\u0026#34; ) Here\u0026rsquo;s a table of optional layout parameters:\n1 2 3 4 5 6 7 8 9 10 11 12 13 Year 06/2006 Month 01/1/Jan/January Day 02/2/_2 Weekday Mon/Monday Hour 03/3/15 Minute 04/4 Second 05/5 Milliseconds .000/.999 Microseconds .000000/.999999 Nanoseconds .000000000/.999999999 am/pm PM/pm Timezone MST Timezone offset -0700/-07/-07:00/Z0700/Z07:00 Timezone Conversion At the beginning of the article, we discussed timezone issues. If in your code, you need to get the result of the same time.Time in different time zones, you can use its In method.\n1 func (t Time) In(loc *Location) Time {} It\u0026rsquo;s straightforward to use. Let\u0026rsquo;s see an example code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 now := time.Now() fmt.Println(now) // 2022-07-18 21:19:59.9636 +0800 CST m=+0.000069242 loc, _ := time.LoadLocation(\u0026#34;UTC\u0026#34;) fmt.Println(now.In(loc)) // 2022-07-18 13:19:59.9636 +0000 UTC loc, _ = time.LoadLocation(\u0026#34;Europe/Berlin\u0026#34;) fmt.Println(now.In(loc)) // 2022-07-18 15:19:59.9636 +0200 CEST loc, _ = time.LoadLocation(\u0026#34;America/New_York\u0026#34;) fmt.Println(now.In(loc)) // 2022-07-18 09:19:59.9636 -0400 EDT loc, _ = time.LoadLocation(\u0026#34;Asia/Dubai\u0026#34;) fmt.Println(now.In(loc)) // 2022-07-18 17:19:59.9636 +0400 +04 Conclusion In general, the functions and methods provided by the time package for time processing meet our usage needs.\nInterestingly, Go\u0026rsquo;s time formatting conversion must adopt Go\u0026rsquo;s birth time. It\u0026rsquo;s quite self-centered.\n","date":"2024-04-19T18:43:00Z","image":"https://images.yixiao9206.cn/blog-images/2024/04/407a476e9b0e72dfefa2c636f2fd71ce.png","permalink":"https://huizhou92.com/p/learn-how-to-handle-time-in-golang/","title":"Learn How to Handle Time In Golang"},{"content":"Introduction In the previous article, we verified the context switch overhead of Linux processes and threads experimentally, which was approximately between 3-5 microseconds. This overhead is not significant, but for massively concurrent internet servers and typical computer programs, the characteristics are as follows:\nHigh concurrency: Thousands to tens of thousands of user requests need to be processed per second. Short cycles: The processing time per user should be as short as possible, often in the millisecond range. High network I/O: Often requires network I/O from other machines, such as Redis, MySQL, etc. Low computation: General CPU-intensive operations are not frequent. This article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nEven with a context switch overhead of 3-5 microseconds, it can still appear somewhat performance-degrading if the context switch volume is particularly high. For example, the Apache web server, which was the software product under this model, suffered from this. (In fact, when Linux operating system was designed, its goal was to be a general-purpose operating system rather than specifically designed for high-concurrency server-side applications.)\nTo avoid frequent context switches, there is another asynchronous non-blocking development model. That is to use a process or thread to handle a large number of user requests and then improve performance through IO multiplexing (processes or threads do not block, saving the overhead of context switches). Nginx and Node.js are typical representatives of this model. Frankly speaking, in terms of program execution efficiency, this model is the most machine-friendly, with the highest efficiency (better than the coroutine development model mentioned below). Therefore, Nginx has replaced Apache as the preferred web server. However, the problem with this programming model lies in its unfriendliness to development, which is overly mechanized and deviates from the original intention of abstracting the concept of processes. Normal linear thinking of humans is disrupted, and application layer developers are forced to write code with non-human-like thinking, making code debugging extremely difficult.\nSo, some smart heads continued to brainstorm at the application layer and designed \u0026ldquo;threads\u0026rdquo; that do not require process/thread context switching, called coroutines. Using coroutines to handle high-concurrency application scenarios can not only meet the original intention of processes but also allow developers to use normal linear thinking to handle their business, while also eliminating the expensive overhead of process/thread context switches. Therefore, it can be said that coroutines are a good patch for the process model in the scenario of processing massive requests on Linux.\nWith the background introduced, what I want to say is that although coroutine encapsulation is lightweight, it still incurs some additional costs. So, let\u0026rsquo;s take a look at how small these additional costs are.\nCoroutine Overhead Test This article is based on go 1.22.1.\n1. Coroutine Context Switch CPU Overhead\nThe test process involves continuously yielding the CPU between coroutines. The core code is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;time\u0026#34; ) func cal() { for i := 0; i \u0026lt; 1000000; i++ { runtime.Gosched() } } func main() { runtime.GOMAXPROCS(1) currentTime := time.Now() fmt.Println(currentTime) go cal() for i := 0; i \u0026lt; 1000000; i++ { runtime.Gosched() } fmt.Println(time.Now().Sub(currentTime) / 2000000) } Compilation and execution\n1 2 3 4 ➜ trace git:(main) ✗ go run main.go 2024-03-20 19:52:24.772579 +0800 CST m=+0.000114834 54ns ➜ trace git:(main) ✗ The average overhead of each coroutine switch is 54ns, which is approximately 1/70 of the context switch overhead measured in the previous article, about 3.5 microseconds, and is approximately 70 times lower than the overhead caused by system calls.\nCoroutine Memory Overhead\nIn terms of space, when coroutines are initialized and created, a stack of 2KB is allocated for them. The stack of threads is much larger than this number, which can be checked through the ulimit command, usually in several megabytes. On my Mac, it\u0026rsquo;s 8MB. If a coroutine is created for each user to handle, 2GB of memory is sufficient for handling 1 million concurrent user requests, while the thread model would require 8TB.\n1 2 3 4 5 6 7 8 9 10 ➜ trace git:(main) ✗ ulimit -a -t: cpu time (seconds) unlimited -f: file size (blocks) unlimited -d: data seg size (kbytes) unlimited -s: stack size (kbytes) 8176 -c: core file size (blocks) 0 -v: address space (kbytes) unlimited -l: locked-in-memory size (kbytes) unlimited -u: processes 2666 -n: file descriptors 12544 Conclusion Since coroutines complete context switches in user space, the switch time is only slightly over 50ns, which is 70 times higher than process switches. The stack memory required by a single coroutine is also quite small, only requiring 2KB. Therefore, coroutines have shined in high-concurrency scenarios in backend internet applications in recent years.\nWhether in terms of space or time performance, they are much better than processes (threads). Then why doesn\u0026rsquo;t Linus implement them in the operating system? For the sake of better real-time performance, the operating system may preempt the CPU of processes with higher priorities. However, coroutines cannot achieve this and still rely on the coroutines currently using the CPU to release it actively, which is not consistent with the implementation purpose of the operating system. Therefore, the efficiency of coroutines comes at the cost of sacrificing preemption.\nCoroutines ultimately execute attached to operating system threads.\nA question we need to consider is:\nDoes using coroutines mean threads no longer switch? The frequency of thread switches basically depends on the number of threads. When using coroutines, you need to specify tasks for each thread. For the same workload, the number of threads required by coroutines should always be higher than that of automatically allocated thread pools.\nTherefore:\nUsing threads = thread switch overhead (low)\nUsing coroutines = thread switch overhead (high) + coroutine switch overhead\nThen CPU overhead:\nInstruction cycles of threads = interrupt detection + instruction execution (including fetch, decode, and execute)\nInstruction cycles of coroutines = interrupt detection + instruction execution + interrupt detection + coroutine signal detection\nSo, I have the following conclusion:\nIn terms of performance, IO multiplexing + thread pool completely outperforms coroutines; but in terms of convenience, coroutines are still easier to use.\nBecause calling coroutines in Go is so convenient, some Go programmers use the go keyword casually. It should be noted that before switching to a coroutine, the coroutine must be created first. Once created, plus the scheduling overhead, it increases to 400ns, which is almost equivalent to the time consumed by a system call. Although coroutines are efficient, they should not be used casually。\n","date":"2024-03-20T16:32:00Z","image":"https://images.yixiao9206.cn/blog-images/2024/03/057d94f39fc9a6838acdbc94b915f654.png","permalink":"https://huizhou92.com/p/performance-analysis-of-goroutine-switching/","title":"Performance analysis of goroutine switching"},{"content":"The process is one of the great inventions of the operating system, which shields application programs from hardware details such as CPU scheduling and memory management, abstracting the concept of a process, allowing applications to focus on implementing their business logic, and \u0026ldquo;simultaneously\u0026rdquo; performing many tasks on a limited CPU. However, while it brings convenience to users, it also introduces some additional overhead. As shown in the figure below, during the running time of a process, although the CPU is busy, it does not complete any user work, which is the additional overhead brought by the process mechanism.\nDuring the process switch from process A to process B, first save the context of process A so that when A resumes running, it knows what the next instruction of process A is. Then, restore the context of process B to the register. This process is called a context switch. The context switch overhead is not significant in scenarios with few processes and infrequent switches. However, now the Linux operating system is used in high-concurrency network backend servers. When a single machine supports thousands of user requests, this overhead needs to be addressed. Because user processes are blocked by network I/O such as Redis, Mysql data, or when the process time slice is up, it will trigger a context switch.\nA Simple Experiment on Process Context Switch Overhead Without further ado, let\u0026rsquo;s conduct an experiment to see how long it takes for a context switch! The experimental method is to create two processes and transfer a token between them. One process will be blocked when reading the token, and the other process will be blocked when waiting for its return. After back-and-forth transmission for a certain number of times, we can then calculate their average single-switch time overhead.\ntest04\n1 2 3 4 # gcc main.c -o main # ./main./main Before Context Switch Time1565352257 s, 774767 us After Context SWitch Time1565352257 s, 842852 us The time for each execution may vary, but the average time for each context switch is around 3.5 microseconds. Of course, this number varies depending on the machine, and it is recommended to test it on real hardware.\nWhen we tested system calls earlier, the minimum value was 200 nanoseconds. It can be seen that the context switch overhead is greater than the system call overhead. While a system call only switches from user mode to kernel mode within the process and then switches back, a context switch directly switches from process A to process B. Obviously, this context switch requires more work to be done.\n{{}}\nTypes of Overhead in Process Context Switching So what specific CPU overheads are involved in context switching? Overheads can be divided into two types: direct overhead and indirect overhead.\nDirect overhead includes tasks that the CPU must perform during the switch, including:\nSwitching the page table global directory. Switching the kernel stack. Switching hardware contexts (all data to be loaded into registers before the process resumes, collectively referred to as hardware context) ip(instruction pointer): points to the next instruction being executed bp(base pointer): used to store the stack bottom address of the executing function\u0026rsquo;s stack frame sp(stack pointer): used to store the stack top address of the executing function\u0026rsquo;s stack frame cr3: Page Directory Base Register, stores the physical address of the page directory table \u0026hellip;\u0026hellip; Refreshing TLB. Execution of the system scheduler\u0026rsquo;s code. Indirect overhead mainly refers to the fact that when switching to a new process, due to various caches not being hot, the speed of execution will be slower. If the process is always scheduled on the same CPU, it\u0026rsquo;s somewhat better, but if it crosses CPUs, the previously warmed TLB, L1, L2, L3 caches become useless because the running process has changed, leading to more memory IO penetrations for the new process. In fact, our previous experiment did not measure this situation well, so the actual context switch overhead may be greater than 3.5 microseconds.\nFor students who want to understand the detailed operation process, please refer to Chapter 3 and Chapter 9 of \u0026ldquo;Understanding the Linux Kernel.\u0026rdquo;\nA More Professional Testing Tool - Lmbench lmbench is an open-source benchmark for evaluating system performance on multiple platforms, which can test various aspects of performance including document reading and writing, memory operations, process creation and destruction overhead, and networking. The usage is simple, but it runs a bit slow. Interested students can try it out themselves.\nThe advantage of this tool is that it conducts multiple experiments, each with 2 processes, 8 processes, and 16 processes. The size of data used by each process also varies, fully simulating the impact of cache misses. I used it for testing and the results are as follows:\n1 2 3 4 5 ------------------------------------------------------------------------- Host OS 2p/0K 2p/16K 2p/64K 8p/16K 8p/64K 16p/16K 16p/64K ctxsw ctxsw ctxsw ctxsw ctxsw ctxsw ctxsw --------- ------------- ------ ------ ------ ------ ------ ------- ------- bjzw_46_7 Linux 2.6.32- 2.7800 2.7800 2.7000 4.3800 4.0400 4.75000 5.48000 lmbench shows that the process context switch time ranges from 2.7 microseconds to 5.48 microseconds.\nThread Context Switching Time Previously, we tested the overhead of process context switching, now let\u0026rsquo;s continue to test threads in Linux. Let\u0026rsquo;s see if threads can be faster than processes, and if so, how much faster.\nIn Linux, there are actually no threads, but just to cater to developers\u0026rsquo; taste, a lightweight process was created and called a thread. Like processes, lightweight processes also have their own independent task_struct process descriptors and their own independent PIDs. From the perspective of the operating system, there is no difference in scheduling between threads and processes; they are just selecting a task_struct from the waiting queue to switch to the running state. The only difference between lightweight processes and regular processes is that lightweight processes can share the same memory address space, code segment, global variables, and the same set of open files.\nFor threads in the same process, the PID seen by getpid() is actually the same, but there is a tgid field in the task_struct. For multi-threaded programs, what getpid() system call actually gets is this tgid, so multiple threads belonging to\nthe same process appear to have the same PID.\nWe\u0026rsquo;ll use an experiment to test this test06. The principle is similar to the process test. We create 20 threads and use a pipe to pass signals between them. When a signal is received, it will wake up, then pass the signal to the next thread, and sleep by itself. In this experiment, we separately considered the additional overhead of passing signals through the pipe and calculated it in the first step.\n1 2 3 # gcc -lpthread main.c -o main 0.508250 4.363495 The results may vary each time, and the above results are averages of multiple runs. The approximate time for each thread switch is around 3.8 microseconds. From the perspective of context switch time, Linux threads (lightweight processes) are actually not much different from processes.\nLinux Related Commands Now that we know the CPU time consumed by context switching, how can we check how many switches are happening in Linux? If context switches are affecting the overall system performance, is there a way to identify problematic processes and optimize them?\n1 2 3 4 5 6 7 8 # vmstat 1 procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 2 0 0 595504 5724 190884 0 0 295 297 0 0 14 6 75 0 4 5 0 0 593016 5732 193288 0 0 0 92 19889 29104 20 6 67 0 7 3 0 0 591292 5732 195476 0 0 0 0 20151 28487 20 6 66 0 8 4 0 0 589296 5732 196800 0 0 116 384 19326 27693 20 7 67 0 7 4 0 0 586956 5740 199496 0 0 216 24 18321 24018 22 8 62 0 8 or\n1 2 3 4 5 6 7 8 9 10 11 12 13 # sar -w 1 proc/s Total number of tasks created per second. cswch/s Total number of context switches per second. 11:19:20 AM proc/s cswch/s 11:19:21 AM 110.28 23468.22 11:19:22 AM 128.85 33910.58 11:19:23 AM 47.52 40733.66 11:19:24 AM 35.85 30972.64 11:19:25 AM 47.62 24951.43 11:19:26 AM 47.52 42950.50 ...... The above environment is a production machine with a configuration of 8 cores and 8GB of KVM virtual machine, running nginx+fpm. There are about 100 user interface requests processed per second on average. The cs column in the above output indicates the number of context switches that occurred in 1 second, and it\u0026rsquo;s about 40,000 times per second. Roughly estimating, each core needs to switch about 5,000 times per second, which means nearly 20 milliseconds are spent on context switching per second. Considering this is a virtual machine, there are some additional overheads in virtualization, and it also consumes CPU for user interface logic processing, system call kernel logic processing, networking, and soft interrupt processing, so a 20ms overhead is not low.\nSo, further, let\u0026rsquo;s see which processes are causing frequent context switches?\n1 2 3 4 5 6 # pidstat -w 1 11:07:56 AM PID cswch/s nvcswch/s Command 11:07:56 AM 32316 4.00 0.00 php-fpm 11:07:56 AM 32508 160.00 34.00 php-fpm 11:07:56 AM 32726 131.00 8.00 php-fpm ...... Because fpm operates in synchronous blocking mode, most of the switches are voluntary, with fewer involuntary switches occurring only when the time slice expires.\nIf you want to see the total context switch situation of a specific process, you can directly check it under the /proc interface, but this is the total value.\n1 2 3 grep ctxt /proc/32583/status voluntary_ctxt_switches: 573066 nonvoluntary_ctxt_switches: 89260 Conclusion We don\u0026rsquo;t need to remember exactly what the context switch does, just remember one conclusion: the context switch overhead on my working machine is about 2.7-5.48 microseconds, you can test it on your own machine using the code or tools I provided. You can use vmstat sar and other tools to view the context switches of processes and then locate performance issues. lmbench is relatively more accurate because it considers the additional overhead caused by cache misses after the switch. Long Time Link If you find my blog helpful, please subscribe to me via RSS Or follow me on X If you have a Medium account, follow me there. My articles will be published there as soon as possible. ","date":"2024-03-15T12:48:15.391Z","image":"https://miro.medium.com/v2/resize:fit:4800/format:webp/0*3q_OdOTK2k1i1R3j","permalink":"https://huizhou92.com/p/the-time-in-the-computers-context-switching/","title":"The Time In The Computers, Context Switching"},{"content":"Analyzing Performance Bottlenecks in Go Applications with Expvar In the development process, it’s essential to collect and sample performance data using pprof to analyze the performance bottlenecks of Go applications. There are two methods for collecting and sampling data:\nAt the micro level, performance benchmark tests can be used to collect and sample data, suitable for identifying performance bottleneck points within functions or method implementations. At the macro level, independent programs are used to collect and sample data. However, when sampling performance data through independent programs, it’s often challenging to quickly capture the real bottleneck points, especially for complex internal structures, excessive business logic, and concurrent Go programs. When sampling performance for such programs, the real bottleneck points may be overshadowed by other data. So, how can we efficiently capture the performance bottleneck points of an application?\nWe can deploy agents or use other methods to obtain probing data by querying external features of the application (such as checking if a certain port of the application responds and returns the correct data or status code). In addition to these messages, we may also want to know some introspective messages, such as more context information about the application’s state. This context information can be about the application’s usage of various resources, such as how much memory the application consumes, or custom performance metric information, such as the number of external requests processed per unit time, response latency, queue backlog, etc.\nAt this point, we need another package provided by Go’s official, expvar .\nPackage expvar provides a standardized interface to public variables, such as operation counters in servers. It exposes these variables via HTTP at /debug/vars in JSON format.\nWe can easily use the expvar package the Go standard library provides to output custom measurement data in a unified interface with the consistent data format and metric definitions. Let’s explore using expvar to output custom performance measurement data together in this post.\nAn Example Let’s start with an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main import ( _ \u0026#34;expvar\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { http.Handle(\u0026#34;/hi\u0026#34;, http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\u0026#34;hi\\n\u0026#34;)) })) fmt.Println(http.ListenAndServe(\u0026#34;localhost:8080\u0026#34;, nil)) } run and then we access http://localhost:8080/debug/vars .\nexpvar returns data in standard JSON format.\nThe default returned status data contains two fields: cmdline and memstats.\nThese output data are variables published by the expvar package in the init function:\n1 2 3 4 5 6 src/expvar/expvar.go func init() { http.HandleFunc(\u0026#34;/debug/vars\u0026#34;, expvarHandler) Publish(\u0026#34;cmdline\u0026#34;, Func(cmdline)) Publish(\u0026#34;memstats\u0026#34;, Func(memstats)) } cmdline: The meaning of the cmdline field is the application name of the output data. Here, since it is an application run through go run, the value cmdline is an application under a temporary path.\nmemstats: The data output memstats corresponds to the runtime.Memstats structure, reflecting the status of heap memory allocation, stack memory allocation, and GC during the application\u0026rsquo;s runtime. The fields of the runtime.Memstats structure may change with the evolution of Go versions, and their specific meanings can be found in the comments of the Memstats structure.\nOutputting Custom Measurement Data through Expvar Similarly, let’s illustrate how to output custom measurement data with an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package main import ( \u0026#34;expvar\u0026#34; _ \u0026#34;expvar\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) ​ var customVar = new(expvar.Map).Init() func init() { customVar.Set(\u0026#34;hi_count\u0026#34;, new(expvar.Int)) expvar.Publish(\u0026#34;custom\u0026#34;, customVar) } ​ func main() { http.Handle(\u0026#34;/hi\u0026#34;, http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { defer func() { customVar.Add(\u0026#34;hi_count\u0026#34;, 1) }() w.Write([]byte(\u0026#34;hi\\n\u0026#34;)) })) fmt.Println(http.ListenAndServe(\u0026#34;localhost:8080\u0026#34;, nil)) } As shown in the example above, after defining a expvar.Map type variable, we can add metrics to this composite metric variable, such as \u0026ldquo;hi_count\u0026rdquo; in the example.\nThen rerun the example. First, call localhost:8080/hi twice:\n1 2 3 4 5 6 7 8 9 10 ➜ test git:(main) ✗ curl localhost:8080/hi hi ➜ test git:(main) ✗ curl localhost:8080/hi hi ➜ test git:(main) ✗ curl http://localhost:8080/debug/vars { \u0026#34;cmdline\u0026#34;: [\u0026#34;/Users/hxzhouh/Library/Caches/JetBrains/GoLand2023.3/tmp/GoLand/___go_build_github_com_hxzhouh_go_example_expvar\u0026#34;], \u0026#34;custom\u0026#34;: {\u0026#34;hi_count\u0026#34;: 2}, \u0026#34;memstats\u0026#34;:\u0026#34;.....\u0026#34; } expvar outputs the desired results. Expvar can output desired results, but we won’t delve into that here.\nDisplaying Output Data Through the /debug/vars service endpoint, we can obtain application internal state data in standard JSON format. Once the data is collected, it can be transformed and displayed according to the needs of different developers. JSON format text is easy to deserialize, and developers can parse and use it themselves, such as writing a Prometheus exporter to import data into the storage behind Prometheus (such as InfluxDB) and visually displaying it through some web-based graphical methods; or importing it into Elasticsearch, and then displaying it through Kibana or Grafana.\nHere we showcase expvarmon developed by Go developer Ivan Daniluk. First, install it:\n1 go get github.com/divan/expvarmon Then, view the data:\n1 expvarmon -ports=\u0026#34;8080\u0026#34; -vars=\u0026#34;custom.hi_count,mem:memstats.Alloc,mem:memstats.Sys,mem:memstats.HeapAlloc,mem:memstats.HeapInuse,duration:memstats.PauseNs,duration:memstats.PauseTotalNs\u0026#34; The result is quite geeky.\nThe expvar package can not only assist in narrowing down the range of performance bottlenecks but also be used to output measurement data to monitor the running status of the application. In this way, when problems occur in the program, we can quickly identify the problem and diagnose and locate it quickly using the output measurement data.\nDifference from Pprof pprof is another built-in performance profiling tool in Go that is mainly used to analyze the CPU usage and memory consumption of programs. Unlike, which provides real-time monitoring of internal variables, pprof is more focused on performance analysis and optimization. In short, expvar helps us observe the \u0026ldquo;active\u0026rdquo; data of the program while pprof focusing on \u0026ldquo;performance\u0026rdquo; data.\nexpvar exposes real-time status data of the application simply, such as the current number of active connections or the counter of requests processed. Its design intent is to monitor the application state over the long term, making it a straightforward tool for developers who want to understand the application\u0026rsquo;s health status quickly.\nConclusion expvar is a powerful tool that can help us monitor the internal situation of applications and easily integrate with other monitoring platforms. It is a tool that every gopher should master.\n","date":"2024-03-14T00:00:00Z","image":"https://images.hxzhouh.com/blog-images/2024/08/218d2745de5b0d61bd11d396580772c2.png","permalink":"https://huizhou92.com/p/analyzing-performance-bottlenecks-in-go-applications-with-expvar/","title":"Analyzing Performance Bottlenecks in Go Applications with expvar"},{"content":"In the Go source code, there is an implementation of the heap data structure. Here is its description\n// Package heap provides heap operations for any type that implements\n// heap.Interface. A heap is a tree with the property that each node is the\n// minimum-valued node in its subtree.\nWhat is a heap? In the \u0026ldquo;Data Structures\u0026rdquo; course at university, we learned about the classic data structure called a heap. Let\u0026rsquo;s refresh our memory about heaps through the Wikipedia page.\nIn computer science, a heap is a tree-based data structure that satisfies the heap property : In a max heap, for any given node C, if P is a parent node of C, then the key (the value) of P is greater than or equal to the key of C. In a min heap, the key of P is less than or equal to the key of C. [1] The node at the \u0026ldquo;top\u0026rdquo; of the heap (with no parents) is called the root node.\nIf you haven\u0026rsquo;t studied computer science or have yet to systematically learn about data structures before, I strongly recommend taking the course Coursera: Algorithms I \u0026amp; II. It is the highest-rated algorithm course on Coursera. Professor Robert Sedgewick has a magical ability to explain even the most complex algorithms clearly and engagingly.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nImplementation of Go Heap Based on Go 1.21.4\nThe basic operations of a heap are as follows:\nNow, let\u0026rsquo;s take a look at the implementation of heap. It\u0026rsquo;s quite simple, with only a sort interface and two methods: push and pop.\n1 2 3 4 5 6 7 8 9 10 11 12 13 type Interface interface { sort.Interface Push(x any) // add x as element Len() Pop() any // remove and return element Len() - 1. } // sort.Interface src/sort/sort.go type Interface interface { // Len is the number of elements in the collection. Len() int Less(i, j int) bool // Swap swaps the elements with indexes i and j. Swap(i, j int) } By implementing the sort.Interface, we can obtain a robust heap implementation. Different implementations Less can achieve either a max heap or a min heap. The more complex part of heap maintenance is already implemented in the source code. Since this article is not a data structures course, we won\u0026rsquo;t delve into the derivation of its principles. Instead, I will use an example to describe the adjustment process.\nExample Now let\u0026rsquo;s use a heap to solve a practical problem—yes, it\u0026rsquo;s time to brush up on LeetCode.\n215. Kth Largest Element in an Array Given an integer array nums and an integer k , return the k th largest element in the array. This problem can be perfectly solved using a min heap.\nExample 2: Input: nums = [3,2,3,1,2,4,5,5,6], k = 4 Output: 4\n692. Top K Frequent Words The solution is similar; we need to change the implementation of Less.\nUsage Heap in Real-world gosrc: The Go language\u0026rsquo;s garbage collector (GC) source code uses a heap. bandUtilHeap and ValHeap in the GC source code both utilize heaps.\netcd: The lease implementation in etcd also uses a heap.\nDiffering Opinions Some Gophers think heap is difficult to use. Although the standard library provides a heap implementation, it is considered challenging to use for the following reasons:\nIt uses a functional approach instead of an intuitive object-oriented approach. It requires implementing three methods ( Len, Less, Swap) of the Interface interface ( sort.Interface), as well as Push(x any) and Pop() any. The package provides methods such as heap.Init, heap.Fix, heap.Pop, heap.Push, and heap.Remove. The names Pop and Push conflict with the methods of Interface, which can be confusing. heap.Pop and Interface.Pop have no relationship, and the same applies to heap.Push and Interface.Push. Although heap.Push internally calls Interface.Push, there are additional processing steps. However, some people have implemented simpler alternatives. An article titled Why Are Golang Heaps So Complicated discusses this issue.\n","date":"2024-03-12T10:27:31Z","image":"https://images.yixiao9206.cn/blog-images/2024/06/0158f02d1102610416ac3f6296e8ffe0.png","permalink":"https://huizhou92.com/p/go-internal-data-structure-heap/","title":"Go: Internal Data Structure: heap"},{"content":"Kafka is known for addressing large-scale data processing problems and is widely deployed in the infrastructure of many well-known companies. As early as 2015, LinkedIn had 60 clusters with 1100 Brokers, processing 13 million messages per second.\nBut it turns out that scale isn\u0026rsquo;t the only thing Kafka excels at. Its advocated programming paradigm \u0026ndash; partitioned, ordered, event processing \u0026ndash; is a good solution for many problems you may encounter. For example, if events represent rows to be indexed into a search database, the last modification is the last index, which is essential, otherwise, searches will indefinitely return stale data. Similarly, if events represent user behavior, processing the second event (\u0026ldquo;user account upgrade\u0026rdquo;) may depend on the first (\u0026ldquo;user account creation\u0026rdquo;). This paradigm differs from traditional job queue systems, where many workers simultaneously pop up events from the queue, which is simple and scalable but breaks any ordering guarantee. Suppose you want ordered processing, but perhaps you don\u0026rsquo;t want to use Kafka because of its reputation as a difficult-to-operate or expensive heavyweight system. How does Redis, with its \u0026ldquo;stream\u0026rdquo; data structure released with version 5.0, compare? Does it solve the same problems?\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nKafka Architecture Let\u0026rsquo;s first take a look at the basic architecture of Kafka. The fundamental data structure is the topic. It\u0026rsquo;s a time-ordered record sequence, appended-only. The benefits of using this data structure are well described in Jay Kreps\u0026rsquo; classic blog post, The Log.\nTopics are partitioned to allow them to scale: each topic can be hosted on separate Kafka instances. Records in each partition are assigned consecutive IDs called offsets, which uniquely identify each record in the partition. Consumers process records sequentially, keeping track of the last offset they\u0026rsquo;ve seen. Since records persist in a topic, multiple consumers can independently process records.\nIn practice, you may distribute your processing across many machines. To achieve this, Kafka provides an abstraction called a \u0026ldquo;consumer group,\u0026rdquo; a group of cooperating processes consuming data from a topic. Partitions of a topic are assigned to members of the group. Then, when members join or leave the group, partitions must be reassigned to ensure each member gets a fair share of the partitions. This is called the rebalancing algorithm.\nNote that a partition is processed by only one member of the consumer group. (A member may be responsible for multiple partitions.) This ensures strictly ordered processing.\nThis toolkit is handy. You can quickly scale your processing by adding more workers, while Kafka handles distributed coordination problems.\nRedis Stream Data Structure How does Redis\u0026rsquo; \u0026ldquo;stream\u0026rdquo; data structure compare? Redis streams conceptually equate to partitioning the abovementioned Kafka topic but with minor differences.\nIt\u0026rsquo;s a persistent, ordered event storage (similar to Kafka). It has a configurable maximum length (as opposed to a retention period in Kafka). Events store keys and values akin to Redis Hash (as opposed to a single key value in Kafka). The most significant difference is that consumer groups in Redis are entirely different from those in Kafka.\nIn Redis, a consumer group is a set of processes that read from the same stream. Redis ensures that events are only delivered to one consumer within the group. For example, in the diagram below, Consumer 1 won\u0026rsquo;t process \u0026lsquo;9\u0026rsquo;. It will skip it because Consumer 2 has already seen it. Consumer 1 will get the next event not seen by any other group member.\nThe role of groups in Redis is to parallelize the processing of a single stream. This structure resembles a traditional job queue. Unfortunately, it loses the ordering guarantee essential to stream processing.\nStream Processing as a Client Library So, if Redis effectively only provides topics with job queue semantics, how can we build a stream processing engine on top of Redis? If you want Kafka\u0026rsquo;s features, you need to make them yourself. That means implementing.\nEvent partitioning. You need to create N streams and treat each stream as a partition. Then, upon sending, you must decide which partition should receive it based on the event\u0026rsquo;s hash value or one of its fields. A worker partition assignment system. To scale and support multiple workers, you must create an algorithm to distribute partitions among them, ensuring each worker has an exclusive subset, akin to Kafka\u0026rsquo;s \u0026ldquo;rebalancing\u0026rdquo; system. Ordered processing with acknowledgment. Each worker needs to iterate through each partition, tracking its offset. Though Redis consumer groups have job queue semantics, they can help here. The trick is to have each group use one consumer and then create a group for each partition. Then, each partition will be processed sequentially, and you can leverage built-in consumer group state tracking. Redis can track not only offsets but also acknowledgments for each event, which is powerful. This is the absolute minimum requirement. Suppose you want your solution to be robust. In that case, you might also consider error handling: In addition to crashing your workers, perhaps you\u0026rsquo;d like a mechanism to forward errors to a \u0026ldquo;dead letter\u0026rdquo; stream and continue processing.\nThe good news is that if you\u0026rsquo;re a Python enthusiast, a newly released library called Runnel addresses these problems and more. You can check out Kafka-like semantics on Redis. Below is what it looks like in one of the Kafka diagrams above.\nWorkers coordinate their ownership of partitions via locks implemented in Redis. They communicate with each other through a special \u0026ldquo;control\u0026rdquo; stream. For more information, including a detailed breakdown of the architecture and rebalancing algorithm, please refer to the Runnel documentation.\nsummary Is Redis a good choice for large-scale event processing? There\u0026rsquo;s a fundamental trade-off: because everything is in memory, you get unparalleled processing speed, but it\u0026rsquo;s unsuitable for storing unlimited data. With Kafka, you can retain all your events indefinitely. Still, with Redis, you\u0026rsquo;re storing a fixed window of the most recent events \u0026ndash; just enough for your processors to have a comfortable buffer in case they slow down or crash. This means you should also use an external long-term event store, such as S3, to be able to replay them, which adds complexity to your architecture but reduces costs.\nThe primary motivation for researching this issue is the ease of use and low cost of deploying and operating Redis. That\u0026rsquo;s why it\u0026rsquo;s attractive compared to Kafka. It\u0026rsquo;s also a magical toolkit that stood the test of time, quite impressive. It turns out that, with effort, it can also support the distributed stream processing paradigm.\n","date":"2024-03-01T14:55:16+08:00","image":"https://images.yixiao9206.cn/blog-images/2024/06/2320e4fa0276aa272f2516ae5b793337.png","permalink":"https://huizhou92.com/p/redis-stream-vs-kafka-a-clash-of-two-kings/","title":"Redis stream VS Kafka, A Clash Of Two Kings"},{"content":"\nMechanical hard disk drives (HDD) and solid-state drives (SSD) are two of the most common types of hard drives. As external storage for computers, it takes a long time for the CPU to access the data stored on them. According to the table below, accessing 4KB of data randomly in an SSD takes 1,500 times longer than accessing main memory, while the seek time for a mechanical disk is 100,000 times longer than accessing main memory:\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nLatency Numbers Every Programmer Should Know https://gist.github.com/jboner/2841832\nAlthough the seek time for a disk is only 10 ms, it is already a very long time for the CPU. When we scale up the times mentioned above proportionally, we can intuitively feel the performance differences. For example, if accessing L1 cache takes 1 second for the CPU, accessing main memory would take 3 minutes, reading data randomly from an SSD would take 3.4 days, disk seek would take 2 months, and network transmission could take over a year.\nIn computer architecture, hard disks are common input/output devices, and the operating system does not necessarily need a hard disk to start. It can start through a hard disk, a network device, or an external device. Therefore, a hard disk is not a necessary condition for computer operation.\nInput/Output Devices\nAs an external input/output device, compared to CPU cache and memory, the slow read and write speed of a hard disk is reasonable. However, the several thousand to several hundred thousand times difference in speed does make it difficult to imagine or accept. In this article, we will analyze why accessing a hard disk is very slow for the CPU:\nThe process of CPU accessing data from a hard disk is complex. It first reads the data from the disk into memory through I/O operations and then accesses the data stored in memory. Mechanical hard disks rely on mechanical structures to access the data in the disk, which requires moving the mechanical arm of the disk. I/O Operations In order for the CPU to access data from the disk, it must first read the data from the disk into memory through I/O operations, and then access the data stored in memory. There are three common types of I/O operations in computers: Programmed I/O, Interrupt-driven I/O, and Direct Memory Access (DMA). We will introduce these operations one by one:\nThe simplest form of performing I/O operations is using Programmed I/O. When using Programmed I/O, the CPU is responsible for all the work. For example, if we want to output “Hello World” on the screen, the CPU will write a new character to the I/O device each time, and after writing, it will poll the device’s status and wait for it to complete its work before writing a new character. This method is simple but it occupies all the CPU resources, which can cause serious waste of computing resources in some complex systems.\nInterrupt-driven I/O is a more efficient way to perform I/O operations. In Programmed I/O, the CPU actively retrieves the device’s status and waits for the device to become idle. However, if Interrupt-driven I/O is used, the device will actively initiate an interrupt when it is idle, pause the current process, and save the context. The operating system will then execute the interrupt handler for the I/O device:\nIf there are no characters to be printed at the moment, the interrupt handler stops and resumes the paused process. If there are characters to be printed, the next character is copied to the device and the paused process is resumed. Using Interrupt-driven I/O allows the CPU to handle other tasks when the device is busy, thus maximizing CPU utilization and avoiding wasting precious computing resources. Compared to Programmed I/O, Interrupt-driven I/O delegates some work to the I/O device, thereby improving resource utilization.\nDirect Memory Access (DMA) uses a DMA controller to perform I/O operations. Interrupt-driven I/O requires triggering an operating system interrupt for each character, which consumes CPU time. When we use a DMA controller, the CPU reads all the data from the buffer into the DMA controller at once, and the DMA controller is responsible for writing the data to the I/O device character by character。 Although the DMA controller can free up the CPU and reduce the number of interrupts, its execution speed is much slower than the CPU. If the DMA controller cannot drive the I/O device quickly, the CPU may have to wait for the DMA controller to trigger an interrupt. In this case, Interrupt-driven I/O or even Programmed I/O can provide faster access speed.\nBy default, we use a DMA controller to perform I/O tasks. However, Programmed I/O and Interrupt-driven I/O are also acceptable options. When the CPU frequently needs to wait for the DMA controller to perform I/O tasks, using Interrupt-driven I/O or even polling Programmed I/O can achieve higher throughput. However, regardless of the method used, I/O is a complex and time-consuming operation in the program.\nMechanical Hard Disk A mechanical hard disk drive (HDD) is an electronic, non-volatile mechanical data storage device. It uses magnetic storage to store and retrieve data on the disk. During the process of reading and writing data, the disk head connected to the mechanical arm of the hard disk reads and writes bits on the surface of the disk 4.\nBecause the disk has a complex mechanical structure, reading and writing data on the disk takes a lot of time. The read and write performance of databases also depends on the performance of the disk. If we randomly query a piece of data in a database using a mechanical hard disk, it may trigger random I/O on the disk. However, it requires a significant cost to read data from the disk into memory. Loading data from a regular disk (non-SSD) involves processes such as queueing, seeking, rotating, and transferring data, which takes about 10 ms.\nWhen estimating the query performance of a database, we can use the order of magnitude of 10 ms to estimate the time occupied by random I/O. It is worth mentioning that random I/O has a significant impact on the query performance of databases. On the other hand, reading data sequentially from a disk can achieve a speed of up to 40 MB/s, which is several orders of magnitude faster. Therefore, we should try to minimize the number of random I/O operations in order to improve performance.\nA solid-state drive (SSD) is a computer storage device that uses flash memory as persistent storage. Unlike mechanical hard disks, SSDs do not contain any mechanical structures. When we read or write data using an SSD, no mechanical structures are involved because everything is done by circuits. Therefore, the read and write speed of an SSD is much faster than that of an HDD.\nFigure 5 — HDD and SSD Prices\nSince their inception, the prices of both mechanical hard disks and SSDs have been continuously decreasing. Mechanical hard disks are the main external storage used in data centers today. Most general-purpose commercial servers use mechanical hard disks as their main external storage. However, because the read and write speed of SSDs is tens of times faster than that of mechanical hard disks, more and more servers, especially databases, use SSDs as their external storage. However, as an external storage device with mechanical structures, it is susceptible to external interference when subjected to vibration.\nConclusion Hard disks are external storage devices in computers that can store large amounts of data persistently. However, the CPU cannot directly access the data on the hard disk. When a computer starts, the operating system loads the necessary data from the disk into memory for CPU access. But if the data the CPU wants to access is not in memory, it takes several thousand to several hundred thousand times longer to read the data. This is mainly due to the following two reasons:\nCPU needs to access data in external storage through I/O operations. The three methods of Programmed I/O, Interrupt-driven I/O, and DMA all incur additional overhead and consume a significant amount of CPU time. Mechanical hard disks access the data in the disk through mechanical structures. Each random I/O operation on the hard disk requires several processes such as queueing, seeking, rotating, and transferring data, which takes about 10 ms. As mentioned in the article, a hard disk is not a necessary hardware device for computer operation. A computer can load the necessary data for startup from any external storage device such as a disk or CD-ROM into memory and start normally. However, hard disks are currently the most common external storage devices. In the end, let’s look at some open-ended questions related to the topic. Interested readers can carefully consider the following questions:\nIs data written to a hard disk always persistently stored without loss? Why is the data in memory cleared after a power outage and restart? If you have any questions about the content of the article or want to learn more about the reasons behind some design decisions in software engineering, you can leave a comment below the blog post. The author will reply to related questions in a timely manner and select suitable topics for future content based on them\n","date":"2024-02-19T03:02:00Z","permalink":"https://huizhou92.com/p/why-the-design-why-is-the-cpu-slow-to-access-the-hard-disk/","title":"Why The Design :Why is the CPU slow to access the hard disk?"},{"content":"Today, I read a paper about real-world Go concurrency error bugs, and here’s a transcript of what I read as a start to learning about Go concurrency programming.\nLink\nRecently, I came across a paper in one of the newsletters I subscribe to. The paper from the University of Pennsylvania presents the first systematic study of concurrency-related bugs in several major open-source Golang software projects. The researchers examined the commit histories of the following software: Docker, Kubernetes, etc, gRPC, CockroachDB, and BoltDB`, and drew several interesting conclusions.\nResearch Method This study focused on concurrency-related bugs. The researchers examined the commit histories of these projects, searching for keywords such as “race,” “deadlock,” “synchronization,” and Golang-specific synchronization primitives like “context,” “once,” and “WaitGroup.” They identified fixes for synchronization bugs and even performed bug replays and reproductions. The bugs were classified as either “blocking” or “non-blocking.”\nNumber and types of bugs in different projects\nBlocking Bugs Blocking bugs refer to bugs where one or more Goroutines are blocked, leading to partial or global deadlock. These bugs typically arise from circular dependencies. Here’s an example:\n1 2 3 4 5 6 7 8 9 10 // goroutine 1 func goroutine1() { m.Lock() - ch \u0026lt;- request // blocks + select { + case ch \u0026lt;- request + default: + } m.Unlock() }// goroutine 1 func goroutine1() { m.Lock() - ch \u0026lt;- request // blocks + select { + case ch \u0026lt;- request + default: + } m.Unlock() goroutine 2 func goroutine2() { for { m.Lock() // blocks m.Unlock() request \u0026lt;- ch } 1 2 3 4 5 6 7 8 // goroutine 2 func goroutine2() { for { m.Lock() // blocks m.Unlock() request \u0026lt;- ch } } In the author’s study, it was found that the proportion of bugs caused by message passing was even higher than those caused by traditional mutexes. Furthermore, there are currently no mature detection methods for such bugs.\nOverall, we found that there are around 42% blocking bugs caused by errors in protecting shared memory, and 58% are caused by errors in message passing. Considering that shared memory primitives are used more frequently than message passing ones (Section 3.2), message passing operations are even more likely to cause blocking bugs.\nPersonal opinion: The reasons for these issues may include the unfamiliarity with new channel synchronization primitives or the overreliance on channels, leading to a relaxed attitude towards bugs. In summary, channels are powerful, but they are not a panacea for solving all synchronization problems.\nNon-blocking Bugs Non-blocking bugs refer to data race issues caused by inadequate memory protection, as well as Goroutine leaks due to delayed sending or receiving on Goroutine channels, which are unique to Go.\nHere’s an example where the original code uses select-case, resulting in the default case being unintentionally executed multiple times. The fix completely replaces the original code with Once:\n1 2 3 4 5 6 7 8 9 10 11 12 // when multiple goroutines execute the following code, default // can execute multiple times, closing the channel more than once, // which leads to panic in Go runtime - select { - case \u0026lt;- c.closed: // do something - default: + Once.Do(func() { close(c.closed) + }) - }// when multiple goroutines execute the following code, default // can execute multiple times, closing the channel more than once, // which leads to panic in Go runtime - select { - case \u0026lt;- c.closed: // do something - default: + Once.Do(func() { close(c.closed) + }) - } Additionally, the paper explores different methods for modifying different types of bugs and provides recommendations for the development of future bug detection tools.\nConclusions and Reflections Go channels provide powerful concurrency patterns, but they are not a panacea. The study found that message passing can cause a higher proportion of blocking bugs. Currently, there are no well-established detection methods. Concurrency issues are inherently complex, and while language features may reduce complexity, they cannot effortlessly solve all problems through casual analysis. In Go programs, synchronization of shared memory (e.g., classic lock/unlock) still accounts for a higher proportion. Furthermore, misuse of channels can lead to performance issues ( see this article). The author observes that many Go bugs exhibit similar patterns, which suggests the possibility of developing more static analysis tools dedicated to analyzing specific types of problems. Personal opinion: When writing Go code, whether using synchronization locks or channels, it is advisable to keep the synchronized code concise, clear, and easy to verify and inspect in order to reduce bugs. The Go compiler includes deadlock and data race detection, but it cannot detect many situations. More in-depth bug research could consider using debuggers like gdb and runtime profiling tools like pprof ( see this article and official documentation). There is further discussion of this paper on HackerNews ( link). Commit messages are not only helpful for tracing bugs, but also for systematically analyzing historical bugs in the future. (Remember not to write “asdfasdf” in Git commit messages, kids!) ","date":"2024-02-04T21:32:27Z","image":"https://images.yixiao9206.cn/blog-images/2024/05/3f9b7292094086961ecb0d8658528cbe.png","permalink":"https://huizhou92.com/p/study-gos-concurrency-bugs-in-the-real-world/","title":"Study Go‘s Concurrency Bugs In The Real-World"},{"content":"TCP protocol is one of the network protocols we use in our daily lives. It is responsible for establishing and terminating connections. In the previous blog post, we analyzed why TCP requires three handshakes to establish a connection. When establishing a connection, we need to ensure the issues of historical connections and sequence numbers. Unlike the three-way handshake during connection establishment, disconnecting a TCP connection requires a four-way handshake. This article will explore why TCP disconnects require a four-way handshake instead of three or any other number.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nOverview Before delving into why a four-way handshake is necessary, let’s first understand the process of TCP connection termination. Typically, when one party in the communication decides to terminate the connection, it sends a FIN (Finish) control message to the other party, indicating that it has no more data to send. The receiving party responds with an ACK (Acknowledgment) control message to acknowledge and enters a half-closed state, indicating that it will no longer send data but can still receive data. When the other party also has no more data to send, it sends a FIN message to indicate its intention to disconnect. The receiving party then sends another ACK message to confirm, and only then will the connection be fully closed.\nDesign Why does TCP disconnect require a four-way handshake? Let’s analyze this question from several aspects:\nEnsuring Data Integrity TCP is a connection-oriented reliable transport protocol that guarantees data reliability and integrity. During connection termination, both parties may still have unsent or unacknowledged data packets. If only a three-way handshake is performed, the other party may not know whether the FIN message it sent has been received during the final handshake. This could result in the other party not fully receiving the data, leading to data loss. Therefore, by performing a four-way handshake, both parties can ensure that they receive each other’s data completely and maintain data integrity.\nHandling Network Latency and Packet Loss In a network, data packets may be delayed or lost due to network latency or packet loss. During connection termination, if only a three-way handshake is performed, the other party may not know whether the FIN message it sent has been received, which may prevent timely closure of the connection. By performing a four-way handshake, sufficient time is provided for the network to handle latency and packet loss issues, ensuring that the connection can be properly closed.\nWaiting for Unsent Data to Be Sent During connection termination, both parties may still have unsent data packets. If only a three-way handshake is performed, the other party may not know whether there is any unsent data before sending the FIN message. By performing a four-way handshake, both parties have enough time to send and receive the remaining data, ensuring data integrity and correctness.\nHandling the Half-Closed State During the TCP connection termination process, one party enters a half-closed state by sending a FIN message, indicating that it will no longer send data but can still receive data. If only a three-way handshake is performed, the other party will immediately close the connection upon receiving the FIN message, which may prevent proper handling of the half-closed state. By performing a four-way handshake, both parties can ensure the correct handling of the half-closed state, avoiding data loss and confusion.\nConclusion Based on the analysis above, we can conclude that a four-way handshake is necessary for TCP disconnection to ensure data integrity, handle network latency and packet loss, wait for unsent data to be sent, and handle the half-closed state. Through the four-way handshake, both parties can better coordinate and handle the disconnection, ensuring the correct transmission of data and secure closure of the connection.\nWhen discussing TCP disconnection, we should not focus on why a four-way handshake is used, but rather understand why multiple handshakes are needed to ensure data integrity and proper connection closure. By gaining a deep understanding of TCP protocol design, we can better apply and comprehend the principles and mechanisms of network communication.\nWhether it is three handshakes or four handshakes, the first element of a tcp connection to consider is always security and data integrity, although it seems that three handshakes and four handshakes seem to be very inefficient, but the vast majority of Internet traffic is based on the tcp protocol, which is enough to prove that its reliability, on the issue of performance, there are other ways to optimize the performance of the protocol, such as udp, and later on, we will analyze how UDP and UDP-based QUIC protocol will affect the Internet in the next ten years. and how the UDP-based QUIC protocol will influence the next decade of the Internet.\nIf you found my article enjoyable, feel free to follow me and give it a 👏. Your support would be greatly appreciated.\nReferences RFC 793 — Transmission Control Protocol — IETF Tools Why do we need a 4-way handshake to terminate a TCP connection? why-tcp-connect-termination-need-4-way-handshake ","date":"2024-02-01T11:11:00Z","permalink":"https://huizhou92.com/p/why-does-it-take-four-waves-for-tcp-to-disconnect/","title":"Why Does It Take Four Waves For TCP To Disconnect?"},{"content":"IPv6 has been around for a long time, and I have dealt with many IPv6 tasks in my work. However, I never thought about switching my EC2 instance to IPv4. Yesterday, while going through my email trash, I came across a message stating that AWS will start charging for IPv4 addresses from February 1, 2024. This caught my attention, so I decided to switch my EC2 instance to IPv6 today. The process was a bit of a hassle. This article is not only applicable to EC2 instances but should also work for other Linux hosts.\n![[Pasted image 20240619101143.png]]\nThis article was first published in the medium MPP plan. If you are a medium user, please follow me on Medium](https://medium.huizhou92.com/). Thank you very much.\nAdding an IPv6 Address to EC2 Since my EC2 instance’s DNS resolution is handled by Cloudflare, I mainly referred to this blog post: Amazon’s $2bn IPv4 Tax — and How to Avoid Paying It\nAnd also, the official AWS documentation on Migrating Your VPC from IPv4 to IPv6.\nIt’s worth noting that the demo in the “Migrating Your VPC from IPv4 to IPv6” documentation assumes that the VPC has both a public and a private subnet. If you, like me, only have a public subnet, you can skip that part.\nI must say, AWS documentation is well-written, and there’s a lot to learn from it.\nThe result after completing the setup should look like this, with both IPv4 and IPv6 addresses. Make sure to add the same rules for IPv6 in the security group.\nApplication Support On my EC2 instance, I only have Nginx and Docker running, and I usually log in via SSH. So, I need to add IPv6 support for Nginx and SSH.\nNginx For your HTTP server block (the one listening on port 80), add the line listen [::]:80;. This allows Nginx to listen to both IPv4 and IPv6 HTTP traffic. Your modified server block should look like this:\n1 2 3 4 5 6 server { listen 80; listen [::]:80; server_name hexo.hxzhouh.com; return 301 https://$host$request_uri; } For each HTTPS server block (those listening on port 443), add listen [::]:443 ssl; inside each block. This enables Nginx to listen for HTTPS traffic on IPv6. For example, for the first HTTPS server block, you need to make the following modification:\n1 2 3 4 5 6 server { listen 443 ssl; listen [::]:443 ssl; server_name hexo.hxzhouh.com; # other configurations... } Make these modifications for each HTTPS server block. Then, test the Nginx configuration with Nginx -t. If there are no issues, reload the Nginx configuration with systemctl reload nginx.\nSSHD In the sshd_config file, uncomment the line AddressFamily any (i.e., remove the preceding #) to enable IPv6 listening for SSH and other applications.\n1 2 3 4 5 6 7 vim /etc/ssh/sshd_config ​ #Port 22 AddressFamily any AddressFamily inet #ListenAddress 0.0.0.0 #ListenAddress :: Then, restart SSHD with sudo systemctl reload sshd. Use the netstat -tupln command to check if SSH is successfully listening on IPv6. If you see the following output, it means SSH is listening on IPv6:\nNow the application layer modifications are complete.\nDNS Configuration Finally, in Cloudflare, modify the DNS settings by changing the previous IPv4 A records to AAAA records for IPv6.\nTest Test everything to make sure it’s working fine, and then you can delete the IPv4 address to avoid being charged.\nUppublished: Running instances cannot have their IPv4 addresses removed, but you can rebuild them using an AMI. It’s a bit of a hassle, but at least AWS won’t send me any more emails. ✌️\nReferences How to remove IPv4 public IP address from EC2 instances before February 2024? (IPv6)\n","date":"2024-01-19T20:27:55Z","image":"https://cdn-images-1.medium.com/max/800/0*32WLSHGCAPKEBvI3","permalink":"https://huizhou92.com/p/aws-ec2-switch-to-ipv6save-43-per-year/","title":"Aws ec2 switch to ipv6,Save $43 per year"},{"content":"In 2023, there have been some changes to Go’s concurrency library, and this article provides an overview of these changes. Minor details such as typos and documentation changes will not be covered.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nsync.Map In Go 1.21.0, three functions related to Once were added to the sync package to facilitate the usage of Once:\n1 2 3 func OnceFunc(f func()) func() func OnceValue[T any](f func() T) func() T func OnceValues[T1, T2 any](f func() (T1, T2)) func() (T1, T2) The functionality of these three functions is as follows:\nOnceFunc: Returns a function g that, when called multiple times, will only execute f once. If f panics during execution, subsequent calls to g will not execute f, but each call will still panic. OnceValue: Returns a function g that, when called multiple times, will only execute f once. The return type of g is T, which is an additional return value compared to the previous function. The panic behavior is the same as OnceFunc. OnceValues: Returns a function g that, when called multiple times, will only execute f once. The return type of g is (T1, T2), which is an additional return value compared to the previous function. The panic behavior is the same as OnceFunc. In theory, you can add more functions and return more values. However, since Go does not have a tuple type, the return values of function g cannot be simplified to a tuple type. In any case, Go 1.21.0 only added these three functions.\nWhat are the benefits of these functions? Previously, when using sync.Once, such as initializing a thread pool, we needed to define a variable for the thread pool and call sync.Once.Do every time we accessed the thread pool variable:\n1 2 3 4 5 6 7 8 9 10 11 12 func TestOnce(t *testing.T) { var pool any var once sync.Once var initFn = func() { // initialize pool pool = 1 } for i := 0; i \u0026lt; 10; i++ { once.Do(initFn) t.Log(pool) } } With OnceValue, the code can be simplified:\n1 2 3 4 5 6 7 8 9 func TestOnceValue(t *testing.T) { var initPool = func() any { return 1 } var poolGenerator = sync.OnceValue(initPool) for i := 0; i \u0026lt; 10; i++ { t.Log(poolGenerator()) } } The code is slightly simplified, and you only need to call the returned function g to obtain the singleton.\nIn summary, these three functions are just encapsulations of sync.Once to make it more convenient to use.\nUnderstanding copyChecker We know that sync.Cond has two fields, noCopy and checker. noCopy can be statically checked using the go vet tool, but checker is checked at runtime:\n1 2 3 4 5 6 7 type Cond struct { noCopy noCopy // L is held while observing or changing the condition L Locker notify notifyList checker copyChecker } Previously, the conditions for copyChecker were as follows, although it is just three simple lines, it is not easy to understand:\n1 2 3 4 5 6 7 func (c *copyChecker) check() { if uintptr(*c) != uintptr(unsafe.Pointer(c)) \u0026amp;\u0026amp; !atomic.CompareAndSwapUintptr((*uintptr)(c), 0, uintptr(unsafe.Pointer(c))) \u0026amp;\u0026amp; uintptr(*c) != uintptr(unsafe.Pointer(c)) { panic(\u0026#34;sync.Cond is copied\u0026#34;) } } Now, with added comments, the meaning of these three lines is explained:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func (c *copyChecker) check() { ​ // Check if c has been copied in three steps: ​ // 1. The first comparison is the fast-path. If c has been initialized and not copied, this will return immediately. Otherwise, c is either not initialized, or has been copied. ​ // 2. Ensure c is initialized. If the CAS succeeds, we\u0026#39;re done. If it fails, c was either initialized concurrently and we simply lost the race, or c has been copied. ​ // 3. Do step 1 again. Now that c is definitely initialized, if this fails, c was copied. if uintptr(*c) != uintptr(unsafe.Pointer(c)) \u0026amp;\u0026amp; !atomic.CompareAndSwapUintptr((*uintptr)(c), 0, uintptr(unsafe.Pointer(c))) \u0026amp;\u0026amp; uintptr(*c) != uintptr(unsafe.Pointer(c)) { panic(\u0026#34;sync.Cond is copied\u0026#34;) } } The main logic is as follows:\nThe first step is a fast check, directly comparing the pointer of c and the pointer of c itself. If they are not equal, it means that c has been copied. This is the fastest check path. The second step ensures that c is initialized. It initializes c using CAS (CompareAndSwap). If the CAS succeeds, we\u0026rsquo;re done. If it fails, it means that c was either initialized concurrently and we simply lost the race, or c has been copied. The third step repeats the first step’s check. Since we know that c is definitely initialized at this point, if the check fails, it means that c was copied. The entire logic uses CAS combined with two pointer checks to ensure the correctness of the judgment.\nIn summary, the first step is a performance optimization. The second step uses CAS to ensure initialization. The third step rechecks to ensure the judgment.\nOptimization in sync.Map Previously, the implementation of the Range function in sync.Map was as follows:\n1 2 3 4 5 6 7 8 9 10 func (m *Map) Range(f func(key, value any) bool) { ... if read.amended { read = readOnly{m: m.dirty} m.read.Store(\u0026amp;read) m.dirty = nil m.misses = 0 } ... } There was a line of code: m.read.Store(\u0026amp;read), which caused read to escape to the heap. To avoid the escape of read, a small trick was employed by introducing a new variable:\n1 2 3 4 5 6 7 8 9 10 11 func (m *Map) Range(f func(key, value any) bool) { ... if read.amended { read = readOnly{m: m.dirty} copyRead := read m.read.Store(\u0026amp;Read) m.dirty = nil m.misses = 0 } ... } Issue #62404 analyzed this problem.\nReplacing Done in sync.Once Implementation with atomic.Uint32 Previously, the implementation of sync.Once was as follows:\n1 2 3 4 type Once struct { done uint32 m Mutex } The done field was of type uint32 to indicate whether Once has been executed. The reason for using uint32 instead of bool is that uint32 can be used with atomic operations from the atomic package, while bool cannot.\nNow, the implementation of sync.Once is as follows:\n1 2 3 4 type Once struct { done atomic.Uint32 m Mutex } Since Go 1.19, the standard library has provided atomic wrappers for basic types, and a large amount of code in the Go standard library has been replaced with atomic.XXX types.\nIn my opinion, this modification may result in a performance decrease compared to the previous implementation in certain cases. I will write an article specifically to explore this.\nBesides sync.Once, there are other types that have been replaced with atomic.XXX types in their usage. Is it necessary to replace them?\nOptimization in Initial Implementation of sync.OnceFunc The initial implementation of sync.OnceFunc was as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func OnceFunc(f func()) func() { var ( once Once valid bool p any ) g := func() { defer func() { p = recover() if !valid { panic(p) } }() f() valid = true } return func() { once.Do(g) if !valid { panic(p) } } } If you look closely at this code, you will notice that the function f passed to OnceFunc/OnceValue/OnceValues remains alive even after it has been executed once, as long as the returned function g is not garbage collected. This is unnecessary because f only needs to be executed once and can be garbage collected afterwards. Therefore, an optimization can be made to set f to nil after it is executed, allowing it to be garbage collected.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func OnceFunc(f func()) func() { var ( once Once valid bool p any ) // Construct the inner closure just once to reduce costs on the fast path. g := func() { defer func() { p = recover() if !valid { // Re-panic immediately so on the first call the user gets a // complete stack trace into f. panic(p) } }() f() f = nil // Do not keep f alive after invoking it. valid = true // Set only if f does not panic. } return func() { once.Do(g) if !valid { panic(p) } } } Context As we know, in Go 1.20, a new method WithCancelCause was added ( func WithCancelCause(parent Context) (ctx Context, cancel CancelCauseFunc)), which allows us to pass the cancellation cause to the Context generated by WithCancelCause. This allows us to retrieve the cancellation cause using the context.Cause method.\n1 2 3 4 ctx, cancel := context.WithCancelCause(parent) cancel(myError) ctx.Err() // returns context.Canceled context.Cause(ctx) // returns myError Of course, this implementation is only halfway done, as timeout-related Context also needs this functionality. Therefore, in Go 1.21.0, two additional functions were added:\n1 2 func WithDeadlineCause(parent Context, d time.Time, cause error) (Context, CancelFunc) func WithTimeoutCause(parent Context, timeout time.Duration, cause error) (Context, CancelFunc) These two functions, unlike WithCancelCause, directly pass the cause as a parameter instead of using the returned cancel function.\nGo 1.21.0 also introduced a function AfterFunc, which is similar to time.AfterFunc, but it returns a Context that is automatically canceled after the timeout. The implementation of this function is as follows:\n1 func AfterFunc(ctx Context, f func()) (stop func() bool) The specified Context triggers the invocation of f immediately when done (either due to timeout or cancellation). The returned stop function is used to stop the invocation of f. If stop is called and returns true, f will not be invoked.\nThis is a helper function, but it may be difficult to understand, and it is unlikely to be widely used.\nOther minor performance optimizations, such as replacing type emptyCtx int with type emptyCtx struct{}, are not mentioned here.\nAn additional function func WithoutCancel(parent Context) Context was added, which creates a Context that is not affected when the parent is canceled.\ngolang.org/x/sync No Significant Changes in Sync errgroup now supports setting the cause using withCancelCause. singleflight added an Unwrap method to panicError.\n","date":"2024-01-19T12:45:00Z","image":"https://images.yixiao9206.cn/blog-images/2024/05/a0fe00350455e762e3a3891a422913ff.png","permalink":"https://huizhou92.com/p/overview-of-changes-in-gos-concurrency-library-in-2023/","title":"Overview of Changes in Go’s Concurrency Library in 2023"},{"content":"The inaugural v2 version of the standard library in Go hails from the esteemed math/rand/v2 repository. It is set to make its grand debut with the official release of Go1.22, poised to serve as a reliable and production-ready resource.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nReasons The original math/rand library in the standard package had numerous deficiencies and areas for improvement. These included outdated generators, slow algorithms (performance), and unfortunate conflicts with crypto/rand.Read, among other issues. There is a plan in place to upgrade the v2 versions of standard libraries. Starting with math allows for the accumulation of experience and resolving tooling ecosystem challenges (such as support from tools like gopls and goimports for v2 packages). Subsequent iterations can then address higher-risk packages, like sync/v2 or encoding/json/v2. Go1 requires compatibility guarantees, making it impractical to directly modify the original library. The issues with math/rand are also more prominent and evident. change list Removed Rand.Read and the top-level Read function. Removed Source.Seed, Rand.Seed, and the top-level Seed function (meaning that top-level functions like Int will always use random seeding). Removed Source64, as Source now provides the Uint64 method, making the original methods unnecessary. Utilized a more direct implementation for Float32 and Float64. For example, in the case of Float64, the original implementation used float64(r.Int63()) / (1\u0026lt;\u0026lt;63). However, this had a problem of occasionally rounding to 1.0, while Float64 should never round. The improvement involves changing it to float64(r.Int63n(1\u0026lt;\u0026lt;53)) / (1\u0026lt;\u0026lt;53), which avoids the rounding issue. Implemented Rand.Perm using Rand.Shuffle. This improves efficiency and ensures only one implementation. Renamed Int31, Int31n, Int63, and Int64n to Int32, Int32n, Int64, and Int64n, respectively. These names were unnecessary and confusing. Added Uint32, Uint32n, Uint64, Uint64n, Uint, and Uintn as top-level functions and methods on Rand. Utilized Lemire’s algorithm in Intn, Uintn, Int32n, Uint32n, Int64n, and Uint64n, resulting in improved performance. Introduced a new implementation of Source called PCG-DXSM, including related APIs like NewPCG. Removed the Mitchell \u0026amp; Reeds LFSR generator and NewSource. example Read \u0026amp; Seed The functions Read and Seed have been removed. It is recommended to use crypto/rand\u0026rsquo;s Read function instead.\n1 2 3 4 5 6 7 8 9 10 11 12 13 import ( \u0026#34;crypto/rand\u0026#34; \u0026#34;fmt\u0026#34; ) ​ func main() { b := make([]byte, 3) _, err := rand.Read(b) if err != nil { panic(err) } fmt.Printf(\u0026#34;hxzhouh: %v\\n\u0026#34;, b) } output：\n1 hxzhouh: [48 71 122] For the Seed function, it is advised to call New(NewSource(seed)) in order to reinitialize the random number generator.\ninternal The functions N, IntN, and UintN now utilize a novel implementation algorithm. Interested individuals are encouraged to allocate extra time to examine it in detail: A fast alternative to the modulo reduction\nThe functions Intn, Int31, Int31n, Int63, and Int64n have been renamed as follows: IntN, Int32, Int32N, Int64, and Int64N, respectively.\nAdditionally, new functions Uint32, Uint32N, Uint64, Uint64N, Uint, and UintN have been introduced to generate random unsigned integers. They have also been added as corresponding functions within the Rand structure.\nThe newly added function N generates random numbers of arbitrary integer types. This function is implemented using generics, and the following integer types are its type parameters:\nint int8 int16 int32 int64 Summary Today, we have shared and further described the new math/rand/v2 library, highlighting key changes including performance optimization (algorithm rewrite), standardization, and additions of new random generators.\nGiven the substantial amount of information covered, we have selected and presented only the aspects that are essential for understanding and using the library. However, for those who are interested in delving deeper, it is recommended to refer to the full documentation of https://pkg.go.dev/math/rand/v2@master\n","date":"2024-01-13T20:50:00Z","image":"https://images.yixiao9206.cn/blog-images/2024/06/891b8023b8e7e301c90a69bfd10bdcb0.png","permalink":"https://huizhou92.com/p/go1.22-add-frist-v2-lib-math/rand/v2-more-fast-and-more-standard/","title":"Go1.22 add frist v2 lib, math/rand/v2 more fast and more Standard"},{"content":"\nWarning\nThis article has expired, please do not use it\nGitHub Copilot is a AI assistant developed by GitHub to help developers write code. It is a Visual Studio Code plugin based on OpenAI Codex, providing features such as code suggestions, auto-completion, auto-fixing, and auto-refactoring. Currently, GitHub Copilot also supports chat functionality, powered by GPT-4. However, it is only available within the context of VS Code. In this article, I will explain how to set up a chat service using GitHub Copilot, allowing you to use it anywhere.\nOpen-Source Components ChatGPT-Next-Web: A well-designed, cross-platform ChatGPT web UI with support for GPT3, GPT4, and Gemini Pro. copilot-gpt4-service An AWS EC2 server is needed to run the copilot-gpt4-service, but you can choose any cloud service provider you prefer or run it locally. Vercel: A free static website hosting service to deploy the ChatGPT-Next-Web service. Cloudflare: A free CDN service used for domain name resolution. Obtaining the GitHub Copilot Token Please refer to this document, GitHub Copilot Token. On a Mac, you can use the following command to obtain the token:\n1 bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/aaamoon/copilot-gpt4-service/master/shells/get_copilot_token.sh)\u0026#34; As shown in the screenshot, you need to copy the token to the clipboard for later use.\nRunning the copilot-gpt4-service The copilot-gpt4-service supports Docker deployment, and the official project provides a docker-compose file. You only need to modify the environment variables. Add a server block in NGINX: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 server { listen 443 ssl; listen [::]:443 ssl; server_name chat.example.com; ​ ssl_certificate /etc/nginx/cert/public.pem; ssl_certificate_key /etc/nginx/cert/private.key; ​ location / { proxy_pass http://127.0.0.1:8086/; # Replace with the copilot-gpt4-service address rewrite ^/(.*)$ /$1 break; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Upgrade-Insecure-Requests 1; proxy_set_header X-Forwarded-Proto https; } } Don\u0026rsquo;t forget to reload the NGINX configuration file.\nAdd a domain name resolution for \u0026ldquo;chat.example.com\u0026rdquo; on Cloudflare, pointing to the IP address of your NGINX server. Deploying the ChatGPT-Next-Web Service on Vercel ChatGPT-Next-Web supports one-click deployment to Vercel. You don\u0026rsquo;t need to make any changes.\nOnce the deployment on Vercel is complete, you can add a custom domain name, such as \u0026ldquo;chatnext.example.com\u0026rdquo;.\nTesting Open your browser and enter \u0026ldquo;chatnext.example.com\u0026rdquo;. Click on the settings button in the bottom left corner and select \u0026ldquo;Custom Interface\u0026rdquo;. Enter \u0026quot; https://chat.example.com\u0026quot; as the API endpoint, and use the GitHub Copilot token copied in the first step as the API token. Click on \u0026ldquo;Save\u0026rdquo;, and you can start chatting.\n","date":"2024-01-12T20:16:03Z","permalink":"https://huizhou92.com/p/how-to-set-up-chatgpt-4-service-using-github-copilot/","title":"How to Set Up ChatGPT-4 Service Using GitHub Copilot"}]