[{"content":"\nIntroduce Wireshark is a popular tool for capturing network packets. It can not only capture packets itself but also parse packet files captured by tcpdump.\ngRPC is a high-performance RPC framework developed by Google, based on the HTTP/2 protocol and the protobuf serialization protocol.\nThis article mainly introduces how to capture gRPC packets using Wireshark and parse the packet content.\nThis article was first published in the medium MPP plan. If you are a Medium user, please follow me on Medium. Thank you very much.\nWireshark version: 4.2.2\nConfiguration Since gRPC uses the protobuf serialization protocol, we need to add the protobuf file path.\nClick Wireshark -\u0026gt; Preferences\u0026hellip; -\u0026gt; Protocols -\u0026gt; Protobuf -\u0026gt; Protobuf search paths -\u0026gt; Edit\u0026hellip;\nClick + to add the path of your protobuf file. Don\u0026rsquo;t forget to check the Load all files on the right side.\nSpecific Operations First, we write a simple gRPC service,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 syntax = \u0026#34;proto3\u0026#34;; option go_package = \u0026#34;example.com/hxzhouh/go-example/grpc/helloworld/api\u0026#34;; package api; // The greeting service definition. service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) {} } // The request message containing the user\u0026#39;s name. message HelloRequest { string name = 1; } // The response message containing the greetings message HelloReply { string message = 1; } It has just one function Greeter. After completing the server-side code, run it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 type server struct { api.UnimplementedGreeterServer } func (s *server) SayHello(ctx context.Context, in *api.HelloRequest) (*api.HelloReply, error) { log.Printf(\u0026#34;Received: %v\u0026#34;, in.GetName()) return \u0026amp;api.HelloReply{Message: \u0026#34;Hello \u0026#34; + in.GetName()}, nil } func main() { lis, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:50051\u0026#34;) if err != nil { log.Fatalf(\u0026#34;failed to listen: %v\u0026#34;, err) } s := grpc.NewServer() api.RegisterGreeterServer(s, \u0026amp;server{}) if err := s.Serve(lis); err != nil { log.Fatalf(\u0026#34;failed to serve: %v\u0026#34;, err) } } Then open Wireshark, select the local network card, and listen to tcp.port == 50051.\nIf you are new to Wireshark, I recommend reading this article first: https://www.lifewire.com/wireshark-tutorial-4143298\nUnary Function Now we have a gRPC service running on the local 50051 port. We can use BloomRPC or any other tool you like to initiate an RPC request to the server, or use the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func Test_server_SayHello(t *testing.T) { // Set up a connection to the server. conn, err := grpc.Dial(\u0026#34;localhost:50051\u0026#34;, grpc.WithInsecure(), grpc.WithBlock()) if err != nil { log.Fatalf(\u0026#34;did not connect: %v\u0026#34;, err) } defer conn.Close() c := api.NewGreeterClient(conn) // Contact the server and print out its response. name := \u0026#34;Hello\u0026#34; ctx, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() r, err := c.SayHello(ctx, \u0026amp;api.HelloRequest{Name: name}) if err != nil { log.Fatalf(\u0026#34;could not greet: %v\u0026#34;, err) } log.Printf(\u0026#34;Greeting: %s\u0026#34;, r.GetMessage()) } At this point, Wireshark should be able to capture the traffic packets.\nAs mentioned earlier, gRPC = HTTP2 + protobuf, and since we have already loaded the protobuf file, we should now be able to parse the packet.\nUse the Wireshark shortcut shift+command+U or click Analyze -\u0026gt; Decode As... and set the packet to be decoded as HTTP2 format.\nAt this point, we can see the request clearly.\nMetadata We know that gRPC metadata is transmitted through HTTP2 headers. Now let\u0026rsquo;s verify this by capturing packets.\nSlightly modify the client code:\n1 2 3 4 5 6 7 8 9 10 11 12 func Test_server_SayHello(t *testing.T) { // Set up a connection to the server. ..... // add md md := map[string][]string{\u0026#34;timestamp\u0026#34;: {time.Now().Format(time.Stamp)}} md[\u0026#34;testmd\u0026#34;] = []string{\u0026#34;testmd\u0026#34;} ctx := metadata.NewOutgoingContext(context.Background(), md) // Contact the server and print out its response. name := \u0026#34;Hello\u0026#34; ctx, cancel := context.WithTimeout(ctx, time.Second) .... } Then capture packets again. We can see that md is indeed placed in the header.\nWe also see grpc-timeout in the header, indicating that the request timeout is also placed in the header. The specific details may be covered in a dedicated article, but today we focus on packet capturing.\nTLS The examples above use plaintext transmission, and we used grpc.WithInsecure() when dialing. However, in a production environment, we generally use TLS for encrypted transmission. Detailed information can be found in my previous article.\nhttps://medium.com/gitconnected/secure-communication-with-grpc-from-ssl-tls-certification-to-san-certification-d9464c3d706f\nLet\u0026rsquo;s modify the server-side code:\nhttps://gist.github.com/hxzhouh/e08546cf0457d28a614d59ec28870b11\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 func main() { certificate, err := tls.LoadX509KeyPair(\u0026#34;./keys/server.crt\u0026#34;, \u0026#34;./keys/server.key\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to load key pair: %v\u0026#34;, err) } certPool := x509.NewCertPool() ca, err := os.ReadFile(\u0026#34;./keys/ca.crt\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to read ca: %v\u0026#34;, err) } if ok := certPool.AppendCertsFromPEM(ca); !ok { log.Fatalf(\u0026#34;Failed to append ca certificate\u0026#34;) } opts := []grpc.ServerOption{ grpc.Creds( // Enable TLS for all incoming connections credentials.NewTLS(\u0026amp;tls.Config{ ClientAuth: tls.RequireAndVerifyClientCert, Certificates: []tls.Certificate{certificate}, ClientCAs: certPool, }, )), } listen, err := net.Listen(\u0026#34;tcp\u0026#34;, fmt.Sprintf(\u0026#34;0.0.0.0:%d\u0026#34;, 50051)) if err != nil { log.Fatalf(\u0026#34;failed to listen %d port\u0026#34;, 50051) } // Create a new gRPC server instance with the provided TLS server credentials s := grpc.NewServer(opts...) api.RegisterGreeterServer(s, \u0026amp;server{}) log.Printf(\u0026#34;server listening at %v\u0026#34;, listen.Addr()) if err := s.Serve(listen); err != nil { log.Fatalf(\u0026#34;Failed to serve: %v\u0026#34;, err) } } Now let\u0026rsquo;s also modify the client code:\nhttps://gist.github.com/hxzhouh/46a7a31e2696b87fe6fb83c8ce7e036c\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 func Test_server_SayHello(t *testing.T) { certificate, err := tls.LoadX509KeyPair(\u0026#34;./keys/client.crt\u0026#34;, \u0026#34;./keys/client.key\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to load client key pair, %v\u0026#34;, err) } certPool := x509.NewCertPool() ca, err := os.ReadFile(\u0026#34;./keys/ca.crt\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to read %s, error: %v\u0026#34;, \u0026#34;./keys/ca.crt\u0026#34;, err) } if ok := certPool.AppendCertsFromPEM(ca); !ok { log.Fatalf(\u0026#34;Failed to append ca certs\u0026#34;) } opts := []grpc.DialOption{ grpc.WithTransportCredentials(credentials.NewTLS( \u0026amp;tls.Config{ ServerName: \u0026#34;localhost\u0026#34;, Certificates: []tls.Certificate{certificate}, RootCAs: certPool, })), } // Establish connection to the server conn, err := grpc.Dial(\u0026#34;localhost:50051\u0026#34;, opts...) if err != nil { log.Fatalf(\u0026#34;Connect to %s failed\u0026#34;, \u0026#34;localhost:50051\u0026#34;) } defer conn.Close() client := api.NewGreeterClient(conn) ctx, cancel := context.WithTimeout(context.Background(), time.Second*5) defer cancel() r, err := client.SayHello(ctx, \u0026amp;api.HelloRequest{Name: \u0026#34;Hello\u0026#34;}) if err != nil { log.Printf(\u0026#34;Failed to greet, error: %v\u0026#34;, err) } else { log.Printf(\u0026#34;Greeting: %v\u0026#34;, r.GetMessage()) } } At this point, we can capture packets again and use the same method to decode them. However, we will find that decoding as HTTP2 is no longer possible, but we can decode it as TLS1.3.\nConclusion This article summarizes the basic process of using Wireshark to capture gRPC packets.\nBy capturing packets, we learn that gRPC parameter transmission is done through HTTP2 data frames, and CTX and other metadata are transmitted through headers. These concepts might be familiar, but hands-on experiments enhance understanding.\nWith TLS, we can achieve secure gRPC communication. In the next article, we will attempt to decrypt TLS packets.\nReferences Wireshark Tutorial https://grpc.io/blog/wireshark/ https://www.lifewire.com/wireshark-tutorial-4143298 ","date":"2024-05-20T09:29:17+08:00","permalink":"https://example.com/p/how-to-capture-and-analyze-grpc-packets/","title":"How to Capture and Analyze gRPC Packets Using Wireshark"},{"content":"\ngRPC generally avoids defining errors within messages. After all, each gRPC service inherently comes with an error return value, serving as a dedicated channel for error transmission. All error returns in gRPC should either be nil or an error generated by status.Status. This ensures that errors can be directly recognized by the calling Client.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\n1. Basic Usage Simply returning upon encountering a Go error won’t be recognizable by downstream clients. The proper approach is:\nCall the status.New method and pass an appropriate error code to generate a status.Status object. Call the status.Err method to generate an error recognizable by the calling party, then return. 1 2 st := status.New(codes.NotFound, \u0026#34;some description\u0026#34;) err := st.Err() The error code passed in is of type codes.Code. Alternatively, you can use status.Error for a more convenient method that eliminates manual conversion.\n1 err := status.Error(codes.NotFound, \u0026#34;some description\u0026#34;) 2. Advanced Usage The aforementioned errors have a limitation: the error codes defined by code.Code only cover certain scenarios and cannot comprehensively express the error scenarios encountered in business.\ngRPC provides a mechanism to supplement information within errors: the status.WithDetails method.\nClients can directly retrieve the contents by converting the error back to status.Status and using the status.Details method.\nstatus.Details returns a slice, which is a slice of interface{}. However, Go automatically performs type conversion, allowing direct usage through assertion.\nServer-Side Example • Generate a status.Status object • Populate additional error information 1 2 3 4 5 6 7 8 9 10 11 12 func ErrorWithDetails() error { st := status.Newf(codes.Internal, fmt.Sprintf(\u0026#34;something went wrong: %v\u0026#34;, \u0026#34;api.Getter\u0026#34;)) v := \u0026amp;errdetails.PreconditionFailure_Violation{ //errDetails Type: \u0026#34;test\u0026#34;, Subject: \u0026#34;12\u0026#34;, Description: \u0026#34;32\u0026#34;, } br := \u0026amp;errdetails.PreconditionFailure{} br.Violations = append(br.Violations, v) st, _ = st.WithDetails(br) return st.Err() } Client-Side Example Parse error information after RPC error Retrieve error details directly through assertion 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 resp, err := odinApp.CreatePlan(cli.StaffId.AssetId, gentRatePlanMeta(cli.StaffId)) ​ if status.Code(err) != codes.InvalidArgument { logger.Error(\u0026#34;create plan error:%v\u0026#34;, err) } else { for _, d := range status.Convert(err).Details() { // switch info := d.(type) { case *errdetails.QuotaFailure: logger.Info(\u0026#34;Quota failure: %s\u0026#34;, info) case *errdetails.PreconditionFailure: detail := d.(*errdetails.PreconditionFailure).Violations for _, v1 := range detail { logger.Info(fmt.Sprintf(\u0026#34;details: %+v\u0026#34;, v1)) } case *errdetails.ResourceInfo: logger.Info(\u0026#34;ResourceInfo: %s\u0026#34;, info) case *errdetails.BadRequest: logger.Info(\u0026#34;ResourceInfo: %s\u0026#34;, info) default: logger.Info(\u0026#34;Unexpected type: %s\u0026#34;, info) } } } logger.Infof(\u0026#34;create plan success,resp=%v\u0026#34;, resp) Principles How are these errors passed to the calling Client? They are placed in metadata, then in the HTTP header. Metadata is in the format of key-value pairs. In error transmission, the key is a fixed value: grpc-status-details-bin. The value is encoded by proto and is binary-safe. Most languages have implemented this mechanism.\nNote gRPC imposes restrictions on response headers, with a limit of 8K, so errors should not be too large.\nReference: Protocol Buffers Tutorial errdetails ","date":"2024-05-14T21:26:12Z","permalink":"https://example.com/p/go-action-error-handling-in-grpc/","title":"Go Action: Error Handling In gRPC"},{"content":"\ngRPC is a high-performance RPC framework developed by Google, which by default includes two authentication methods:\nSSL/TLS Authentication Token-based Authentication\nWithout the activation of the certificate, gRPC service and clients communicate in plaintext, leaving the information at risk of being intercepted by any third party. To ensure gRPC communication is not intercepted, altered or counterfeited by a third party, the server can activate TLS encryption features. This article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nStarting from go 1.15 version, depreciation of CommonName began, therefore, it is advised to use SAN certificates. If keys, CSR, and certificates are generated in the previous way through OpenSSL, the following error occurs:\n1 rpc error: code = Unavailable desc = connection error: desc = \u0026#34;transport: authentication handshake failed: x509: certificate relies on legacy Common Name field, use SANs instead\u0026#34;| What is SAN SAN (Subject Alternative Name) is defined as an extension in the SSL standard x509. An SSL certificate with the SAN field can expand the domain names it supports, allowing a single certificate to support multiple different domain name resolutions.\nPut simply, a SAN certificate can contain multiple complete CN (CommonName), so with a single certificate purchase, you can use it on multiple URLs. For example, the certificate of skype.com, it has many SANs.\nCreate a SAN certificate locally Next, we will use an example to generate a client \u0026amp; server bilateral SAN certificate locally.\nAssume the hostname of the gRPC server is localhost, and it is required to configure tls bilateral authentication encryption for the communication between the gRPC server and clients.\nCreate openssl.conf to store relevant information 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 [req] req_extensions = v3_req distinguished_name = req_distinguished_name prompt = no [req_distinguished_name] countryName = CN stateOrProvinceName = state localityName = city organizationName = huizhou92 commonName = hello-world [v3_req] subjectAltName = @alt_names [alt_names] DNS.1 = localhost The content is similar to when creating a ca earlier.\nGenerate ca root certificate 1 2 openssl req -x509 -newkey rsa:4096 -keyout ca.key -out ca.crt -subj \u0026#34;/CN=localhost\u0026#34; -days 3650 -nodes -nodes is to ignore the password, making it convenient to use, but please note, this may reduce the security of the private key, as anyone can read the unencrypted private key.\nGenerate server certificate 1 2 3 openssl req -newkey rsa:2048 -nodes -keyout server.key -out server.csr -subj \u0026#34;/CN=localhost\u0026#34; -config openssl.cnf openssl x509 -req -in server.csr -out server.crt -CA ca.crt -CAkey ca.key -CAcreateserial -days 365 -extensions v3_req -extfile openssl.cnf Generate client certificate 1 2 3 openssl req -newkey rsa:2048 -nodes -keyout client.key -out client.csr -subj \u0026#34;/CN=localhost\u0026#34; -config openssl.cnf openssl x509 -req -in client.csr -out client.crt -CA ca.crt -CAkey ca.key -CAcreateserial -days 365 -extensions v3_req -extfile openssl.cnf The final generated result is as follows\n1 2 3 ➜ keys git:(day1) ✗ ls ca.crt ca.key ca.srl client.crt client.csr client.key openssl.cnf server.crt server.csr server.key Testing We define the simplest grpc interface.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 // helloworld.proto syntax = \u0026#34;proto3\u0026#34;; option go_package = \u0026#34;./api;api\u0026#34;; package api; // The greeting service definition. service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) {} } // The request message containing the user\u0026#39;s name. message HelloRequest { string name = 1; } // The response message containing the greetings message HelloReply { string message = 1; } Server implementation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 package main import ( \u0026#34;context\u0026#34; \u0026#34;crypto/tls\u0026#34; \u0026#34;crypto/x509\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;google.golang.org/genproto/googleapis/rpc/errdetails\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; \u0026#34;google.golang.org/grpc/codes\u0026#34; \u0026#34;google.golang.org/grpc/credentials\u0026#34; \u0026#34;google.golang.org/grpc/status\u0026#34; \u0026#34;hello-world/api\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) type server struct { api.UnimplementedGreeterServer } func (s *server) SayHello(ctx context.Context, in *api.HelloRequest) (*api.HelloReply, error) { log.Printf(\u0026#34;Received: %v\u0026#34;, in.GetName()) select { case \u0026lt;-ctx.Done(): log.Println(\u0026#34;client timeout return\u0026#34;) return nil, ErrorWithDetails() case \u0026lt;-time.After(3 * time.Second): return \u0026amp;api.HelloReply{Message: \u0026#34;Hello \u0026#34; + in.GetName()}, nil } } func main() { certificate, err := tls.LoadX509KeyPair(\u0026#34;./keys/server.crt\u0026#34;, \u0026#34;./keys/server.key\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to load key pair: %v\u0026#34;, err) } // 通过CA创建证书池 certPool := x509.NewCertPool() ca, err := os.ReadFile(\u0026#34;./keys/ca.crt\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to read ca: %v\u0026#34;, err) } // 将来自CA的客户端证书附加到证书池 if ok := certPool.AppendCertsFromPEM(ca); !ok { log.Fatalf(\u0026#34;Failed to append ca certificate\u0026#34;) } opts := []grpc.ServerOption{ grpc.Creds( // 为所有传入的连接启用TLS credentials.NewTLS(\u0026amp;tls.Config{ ClientAuth: tls.RequireAndVerifyClientCert, Certificates: []tls.Certificate{certificate}, ClientCAs: certPool, }, )), } listen, err := net.Listen(\u0026#34;tcp\u0026#34;, fmt.Sprintf(\u0026#34;0.0.0.0:%d\u0026#34;, 50051)) if err != nil { log.Fatalf(\u0026#34;failed to listen %d port\u0026#34;, 50051) } // 通过传入的TLS服务器凭证创建新的gRPC服务实例 s := grpc.NewServer(opts...) api.RegisterGreeterServer(s, \u0026amp;server{}) log.Printf(\u0026#34;server listening at %v\u0026#34;, listen.Addr()) if err := s.Serve(listen); err != nil { log.Fatalf(\u0026#34;Failed to serve: %v\u0026#34;, err) } } func ErrorWithDetails() error { st := status.Newf(codes.Internal, fmt.Sprintf(\u0026#34;something went wrong: %v\u0026#34;, \u0026#34;api.Getter\u0026#34;)) v := \u0026amp;errdetails.PreconditionFailure_Violation{ //errDetails Type: \u0026#34;test\u0026#34;, Subject: \u0026#34;12\u0026#34;, Description: \u0026#34;32\u0026#34;, } br := \u0026amp;errdetails.PreconditionFailure{} br.Violations = append(br.Violations, v) st, _ = st.WithDetails(br) return st.Err() } We directly run the server go run main.go\nClient First, we use a request without a certificate\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func Test_server_SayHello_No_Cert(t *testing.T) { conn, err := grpc.Dial(\u0026#34;localhost:50051\u0026#34;, grpc.WithTransportCredentials(insecure.NewCredentials())) if err != nil { log.Fatalf(\u0026#34;Connect to %s failed\u0026#34;, \u0026#34;localhost:50051\u0026#34;) } defer conn.Close() client := api.NewGreeterClient(conn) // 创建带有超时时间的上下文, cancel可以取消上下文 ctx, cancel := context.WithTimeout(context.Background(), time.Second*5) defer cancel() // 业务代码处理部分 ... r, err := client.SayHello(ctx, \u0026amp;api.HelloRequest{Name: \u0026#34;Hello\u0026#34;}) if err != nil { log.Printf(\u0026#34;Failed to greet, error: %v\u0026#34;, err) } else { log.Printf(\u0026#34;Greeting: %v\u0026#34;, r.GetMessage()) } } Output\n1 2 2024/05/12 19:18:51 Failed to greet, error: rpc error: code = Unavailable desc = connection error: desc = \u0026#34;error reading server preface: EOF\u0026#34; Service is unavailable\nNow, let\u0026rsquo;s try a request carrying the certificate\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 func Test_server_SayHello(t *testing.T) { certificate, err := tls.LoadX509KeyPair(\u0026#34;./keys/client.crt\u0026#34;, \u0026#34;./keys/client.key\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to load client key pair, %v\u0026#34;, err) } certPool := x509.NewCertPool() ca, err := os.ReadFile(\u0026#34;./keys/ca.crt\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to read %s, error: %v\u0026#34;, \u0026#34;./keys/ca.crt\u0026#34;, err) } if ok := certPool.AppendCertsFromPEM(ca); !ok { log.Fatalf(\u0026#34;Failed to append ca certs\u0026#34;) } opts := []grpc.DialOption{ grpc.WithTransportCredentials(credentials.NewTLS( \u0026amp;tls.Config{ ServerName: \u0026#34;localhost\u0026#34;, Certificates: []tls.Certificate{certificate}, RootCAs: certPool, })), } conn, err := grpc.Dial(\u0026#34;localhost:50051\u0026#34;, opts...) if err != nil { log.Fatalf(\u0026#34;Connect to %s failed\u0026#34;, \u0026#34;localhost:50051\u0026#34;) } defer conn.Close() client := api.NewGreeterClient(conn) ctx, cancel := context.WithTimeout(context.Background(), time.Second*5) defer cancel() r, err := client.SayHello(ctx, \u0026amp;api.HelloRequest{Name: \u0026#34;Hello\u0026#34;}) if err != nil { log.Printf(\u0026#34;Failed to greet, error: %v\u0026#34;, err) } else { log.Printf(\u0026#34;Greeting: %v\u0026#34;, r.GetMessage()) } } Output\n1 2 === RUN Test_server_SayHello 2024/05/12 19:20:17 Greeting: Hello Hello Conclusion We can use tls to implement gRPC encryption communication, Starting from go1.15, the use of CA is not recommended, instead SAN certificates are utilized. ","date":"2024-05-13T09:15:00Z","permalink":"https://example.com/p/secure-communication-with-grpc-from-ssl/tls-certification-to-san-certification/","title":"Secure Communication with gRPC: From SSL/TLS Certification to SAN Certification"},{"content":"Thanks to Moore\u0026rsquo;s Law, computer performance has greatly improved, along with advancements in databases and various anti-pattern designs advocated by microservices. As a result, we now have fewer opportunities to write complex SQL queries. The industry (yes, even Google) has started advocating against specialized SQL optimization, as the resources saved do not outweigh the cost of employee salaries. However, as engineers, we should strive for technical excellence to become rocket scientists in our field.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nIn this article, I will introduce eight common SQL slow query statements and explain how to optimize their performance. I hope this will be helpful to you.\nLIMIT Statement Pagination is one of the most commonly used scenarios, but it is also prone to problems. For example, for the simple statement below, a typical solution suggested by DBAs is to add a composite index on the type, name, and create_time fields. This way, the conditions and sorting can effectively utilize the index, resulting in a significant performance improvement.\n1 2 3 4 5 6 SELECT * FROM operation WHERE type = \u0026#39;SQLStats\u0026#39; AND name = \u0026#39;SlowLog\u0026#39; ORDER BY create_time LIMIT 1000, 10; Okay, this might solve the problem for over 90% of DBAs. However, when the LIMIT clause becomes \u0026ldquo;LIMIT 1000000, 10\u0026rdquo;, programmers still complain, \u0026ldquo;Why is it slow when I\u0026rsquo;m only fetching 10 records?\u0026rdquo; You see, the database doesn\u0026rsquo;t know where the 1,000,000th record starts, so even with an index, it still needs to calculate from the beginning. In most cases, this performance issue is caused by lazy programming.\nIn scenarios such as frontend data browsing or exporting large data in batches, you can use the maximum value of the previous page as a parameter for querying. The SQL can be redesigned as follows:\n1 2 3 4 5 6 7 SELECT * FROM operation WHERE type = \u0026#39;SQLStats\u0026#39; AND name = \u0026#39;SlowLog\u0026#39; AND create_time \u0026gt; \u0026#39;2017-03-16 14:00:00\u0026#39; ORDER BY create_time LIMIT 10; With this new design, the query time remains constant and does not change with the increasing data volume.\nImplicit Conversion Another common mistake in SQL statements is when the types of query variables and field definitions do not match. Take the following statement as an example:\n1 2 3 4 5 6 mysql\u0026gt; explain extended SELECT * \u0026gt; FROM my_balance b \u0026gt; WHERE b.bpn = 14000000123 \u0026gt; AND b.isverified IS NULL ; mysql\u0026gt; show warnings; | Warning | 1739 | Cannot use ref access on index \u0026#39;bpn\u0026#39; due to type or collation conversion on field \u0026#39;bpn\u0026#39; In this case, the field bpn is defined as varchar(20), and MySQL\u0026rsquo;s strategy is to convert the string to a number before comparing. This causes the function to be applied to the table field, rendering the index ineffective.\nSuch cases may be caused by parameters automatically filled in by the application framework, rather than the programmer\u0026rsquo;s intention. Nowadays, application frameworks are often complex, and while they provide convenience, they can also create pitfalls.\nJoin Updates and Deletions Although MySQL 5.6 introduced materialization, it only optimizes SELECT statements. For UPDATE or DELETE statements, you need to manually rewrite them using JOIN.\nFor example, consider the following UPDATE statement. MySQL actually performs a loop/nested subquery (DEPENDENT SUBQUERY), and you can imagine the execution time.\n1 2 3 4 5 6 7 8 9 10 11 UPDATE operation o SET status = \u0026#39;applying\u0026#39; WHERE o.id IN (SELECT id FROM (SELECT o.id, o.status FROM operation o WHERE o.group = 123 AND o.status NOT IN ( \u0026#39;done\u0026#39; ) ORDER BY o.parent, o.id LIMIT 1) t); The execution plan is as follows:\n1 2 3 4 5 6 7 +----+--------------------+-------+-------+---------------+---------+---------+-------+------+-----------------------------------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+--------------------+-------+-------+---------------+---------+---------+-------+------+-----------------------------------------------------+ | 1 | PRIMARY | o | index | | PRIMARY | 8 | | 24 | Using where; Using temporary | | 2 | DEPENDENT SUBQUERY | | | | | | | | Impossible WHERE noticed after reading const tables | | 3 | DERIVED | o | ref | idx_2,idx_5 | idx_5 | 8 | const | 1 | Using where; Using filesort | +----+--------------------+-------+-------+---------------+---------+---------+-------+------+-----------------------------------------------------+ After rewriting it as a JOIN, the subquery\u0026rsquo;s select type changes from DEPENDENT SUBQUERY to DERIVED, significantly speeding up the execution time from 7 seconds to 2 milliseconds.\n1 2 3 4 5 6 7 8 9 10 11 UPDATE operation o JOIN (SELECT o.id, o.status FROM operation o WHERE o.group = 123 AND o.status NOT IN ( \u0026#39;done\u0026#39; ) ORDER BY o.parent, o.id LIMIT 1) t ON o.id = t.id SET status = \u0026#39;applying\u0026#39;; The simplified execution plan is as follows:\n1 2 3 4 5 6 +----+-------------+-------+------+---------------+-------+---------+-------+------+-----------------------------------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+------+---------------+-------+---------+-------+------+-----------------------------------------------------+ | 1 | PRIMARY | | | | | | | | Impossible WHERE noticed after reading const tables | | 2 | DERIVED | o | ref | idx_2,idx_5 | idx_5 | 8 | const | 1 | Using where; Using filesort | +----+-------------+-------+------+---------------+-------+---------+-------+------+-----------------------------------------------------+ Mixed Sorting MySQL cannot utilize indexes for mixed sorting. However, in certain scenarios, there are still opportunities to improve performance using special methods.\n1 2 3 4 5 6 SELECT * FROM my_order o INNER JOIN my_appraise a ON a.orderid = o.id ORDER BY a.is_reply ASC, a.appraise_time DESC LIMIT 0, 20; The execution plan shows a full table scan:\n1 2 3 4 5 6 +----+-------------+-------+--------+-------------+---------+---------+---------------+---------+-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra +----+-------------+-------+--------+-------------+---------+---------+---------------+---------+-+ | 1 | SIMPLE | a | ALL | idx_orderid | NULL | NULL | NULL | 1967647 | Using filesort | | 1 | SIMPLE | o | eq_ref | PRIMARY | PRIMARY | 122 | a.orderid | 1 | NULL | +----+-------------+-------+--------+---------+---------+---------+-----------------+---------+-+ Since is_reply only has two states, 0 and 1, we can rewrite it as follows, reducing the execution time from 1.58 seconds to 2 milliseconds:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 SELECT * FROM ((SELECT * FROM my_order o INNER JOIN my_appraise a ON a.orderid = o.id AND is_reply = 0 ORDER BY appraise_time DESC LIMIT 0, 20) UNION ALL (SELECT * FROM my_order o INNER JOIN my_appraise a ON a.orderid = o.id AND is_reply = 1 ORDER BY appraise_time DESC LIMIT 0, 20)) t ORDER BY is_reply ASC, appraisetime DESC LIMIT 20; EXISTS Statement When dealing with EXISTS clauses, MySQL still uses nested subqueries for execution. Take the following SQL statement as an example:\n1 2 3 4 5 6 7 8 9 10 11 SELECT * FROM my_neighbor n LEFT JOIN my_neighbor_apply sra ON n.id = sra.neighbor_id AND sra.user_id = \u0026#39;xxx\u0026#39; WHERE n.topic_status \u0026lt; 4 AND EXISTS(SELECT 1 FROM message_info m WHERE n.id = m.neighbor_id AND m.inuser = \u0026#39;xxx\u0026#39;) AND n.topic_type \u0026lt;\u0026gt; 5; 1 2 3 4 5 6 7 +----+--------------------+-------+------+-----+------------------------------------------+---------+-------+---------+ -----+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra +----+--------------------+-------+------+ -----+------------------------------------------+---------+-------+---------+ -----+ | 1 | PRIMARY | n | ALL | | NULL | NULL | NULL | 1086041 | Using where | | 1 | PRIMARY | sra | ref | | idx_user_id | 123 | const | 1 | Using where | | 2 | DEPENDENT SUBQUERY | m | ref | | idx_message_info | 122 | const | 1 | Using index condition; Using where | +----+--------------------+-------+------+ -----+------------------------------------------+---------+-------+---------+ -----+ By removing the EXISTS clause and changing it to a JOIN, we can avoid nested subqueries and reduce the execution time from 1.93 seconds to 1 millisecond.\n1 2 3 4 5 6 7 8 9 10 SELECT * FROM my_neighbor n INNER JOIN message_info m ON n.id = m.neighbor_id AND m.inuser = \u0026#39;xxx\u0026#39; LEFT JOIN my_neighbor_apply sra ON n.id = sra.neighbor_id AND sra.user_id = \u0026#39;xxx\u0026#39; WHERE n.topic_status \u0026lt; 4 AND n.topic_type \u0026lt;\u0026gt; 5; The new execution plan is as follows:\n1 2 3 4 5 6 7 +----+-------------+-------+--------+ -----+------------------------------------------+---------+ -----+------+ -----+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+--------+ -----+------------------------------------------+---------+ -----+------+ -----+ | 1 | SIMPLE | m | ref | | idx_message_info | 122 | const | 1 | Using index condition | | 1 | SIMPLE | n | eq_ref | | PRIMARY | 122 | ighbor_id | 1 | Using where | | 1 | SIMPLE | sra | ref | | idx_user_id | 123 | const | 1 | Using where | +----+-------------+-------+--------+ -----+------------------------------------------+---------+ -----+------+ -----+ Condition Pushdown There are cases where external query conditions cannot be pushed down to complex views or subqueries:\nAggregated subqueries Subqueries with LIMIT UNION or UNION ALL subqueries Subqueries in output fields Consider the following statement, where the condition affects the aggregated subquery:\n1 2 3 4 5 6 SELECT * FROM (SELECT target, Count(*) FROM operation GROUP BY target) t WHERE target = \u0026#39;rm-xxxx\u0026#39;; 1 2 3 4 5 6 7 +----+-------------+------------+-------+---------------+-------------+---------+-------+------+-------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+------------+-------+---------------+-------------+---------+-------+------+-------------+ | 1 | PRIMARY | n | ALL | NULL | NULL | NULL | NULL | 1086041 | Using where | | 1 | PRIMARY | sra | ref | NULL | idx_user_id | 123 | const | 1 | Using where | | 2 | DEPENDENT SUBQUERY | m | ref | NULL | idx_message_info | 122 | const | 1 | Using index condition; Using where | +----+-------------+------------+-------+---------------+-------------+---------+-------+------+-------------+ By removing the EXISTS clause and changing it to a JOIN, we can avoid nested subqueries and reduce the execution time from 1.93 seconds to 1 millisecond.\n1 2 3 4 5 6 7 8 9 10 SELECT * FROM my_neighbor n INNER JOIN message_info m ON n.id = m.neighbor_id AND m.inuser = \u0026#39;xxx\u0026#39; LEFT JOIN my_neighbor_apply sra ON n.id = sra.neighbor_id AND sra.user_id = \u0026#39;xxx\u0026#39; WHERE n.topic_status \u0026lt; 4 AND n.topic_type \u0026lt;\u0026gt; 5; The new execution plan is as follows:\n1 2 3 4 5 6 7 +----+-------------+-------+--------+ -----+------------------------------------------+---------+ -----+------+ -----+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-------+--------+ -----+------------------------------------------+---------+ -----+------+ -----+ | 1 | SIMPLE | m | ref | | idx_message_info | 122 | const | 1 | Using index condition | | 1 | SIMPLE | n | eq_ref | | PRIMARY | 122 | ighbor_id | 1 | Using where | | 1 | SIMPLE | sra | ref | | idx_user_id | 123 | const | 1 | Using where | +----+-------------+-------+--------+ -----+------------------------------------------+---------+ -----+------+ -----+ Narrowing the Scope in Advance Let\u0026rsquo;s take a look at the following partially optimized example (main table in the left join acts as a primary query condition):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 SELECT a.*, c.allocated FROM ( SELECT resourceid FROM my_distribute d WHERE isdelete = 0 AND cusmanagercode = \u0026#39;1234567\u0026#39; ORDER BY salecode limit 20) a LEFT JOIN ( SELECT resourcesid， sum(ifnull(allocation, 0) * 12345) allocated FROM my_resources GROUP BY resourcesid) c ON a.resourceid = c.resourcesid; Does this statement still have other issues? It is clear that subquery c is an aggregate query on the entire table, which can cause performance degradation when dealing with a large number of tables.\nIn fact, for subquery c, the left join result set only cares about the data that can be matched with the primary table\u0026rsquo;s resourceid. Therefore, we can rewrite the statement as follows, reducing the execution time from 2 seconds to 2 milliseconds:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 SELECT a.*, c.allocated FROM ( SELECT resourceid FROM my_distribute d WHERE isdelete = 0 AND cusmanagercode = \u0026#39;1234567\u0026#39; ORDER BY salecode limit 20) a LEFT JOIN ( SELECT resourcesid， sum(ifnull(allocation, 0) * 12345) allocated FROM my_resources r, ( SELECT resourceid FROM my_distribute d WHERE isdelete = 0 AND cusmanagercode = \u0026#39;1234567\u0026#39; ORDER BY salecode limit 20) a WHERE r.resourcesid = a.resourcesid GROUP BY resourcesid) c ON a.resourceid = c.resourcesid; However, the subquery a appears multiple times in our SQL statement. This approach not only incurs additional costs but also makes the statement more complex. We can simplify it using the WITH statement:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 WITH a AS ( SELECT resourceid FROM my_distribute d WHERE isdelete = 0 AND cusmanagercode = \u0026#39;1234567\u0026#39; ORDER BY salecode limit 20) SELECT a.*, c.allocated FROM a LEFT JOIN ( SELECT resourcesid， sum(ifnull(allocation, 0) * 12345) allocated FROM my_resources r, a WHERE r.resourcesid = a.resourcesid GROUP BY resourcesid) c ON a.resourceid = c.resourcesid; Conclusion The database compiler generates execution plans that determine how SQL statements are actually executed. However, compilers can only do their best to serve, and no database compiler is perfect. The scenarios mentioned above also exist in other databases. Understanding the characteristics of the database compiler allows us to work around its limitations and write high-performance SQL statements.\nWhen designing data models and writing SQL statements, it is important to bring algorithmic thinking or awareness into the process. Developing the habit of using the WITH statement when writing complex SQL statements can simplify them and reduce the burden on the database.\nFinally, here is the execution order of SQL statements:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 FROM \u0026lt;left_table\u0026gt; ON \u0026lt;join_condition\u0026gt; \u0026lt;join_type\u0026gt; JOIN \u0026lt;right_table\u0026gt; WHERE \u0026lt;where_condition\u0026gt; GROUP BY \u0026lt;group_by_list\u0026gt; HAVING \u0026lt;having_condition\u0026gt; SELECT DISTINCT \u0026lt;select_list\u0026gt; ORDER BY \u0026lt;order_by_condition\u0026gt; LIMIT \u0026lt;limit_number\u0026gt; ","date":"2024-05-11T20:19:00Z","permalink":"https://example.com/p/8-common-sql-slow-query-statements-and-how-to-optimize-them/","title":"8 Common SQL Slow Query Statements and How to Optimize Them"},{"content":"Smart Go compiler: Slimming 1. Experiment: Which Functions are Included in the Final Executable? This article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nLet\u0026rsquo;s conduct an experiment to determine which functions are included in the final executable! We\u0026rsquo;ll create a demo1 with the following directory structure and code snippets:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // dead-code-elimination/demo1 $ tree -F . . ├── go.mod ├── main.go └── pkga/ └── pkga.go // main.go package main import ( \u0026#34;fmt\u0026#34; \u0026#34;demo/pkga\u0026#34; ) func main() { result := pkga.Foo() fmt.Println(result) } // pkga/pkga.go package pkga import ( \u0026#34;fmt\u0026#34; ) func Foo() string { return \u0026#34;Hello from Foo!\u0026#34; } func Bar() { fmt.Println(\u0026#34;This is Bar.\u0026#34;) } The example is very simple! The main function calls the exported function Foo from the pkga package, which also contains the Bar function (although it is not called by any other function). Now let\u0026rsquo;s compile this module and examine the functions from the pkga package included in the compiled executable file! (This article uses Go version 1.22.0)\n1 2 $ go build $ go tool nm demo | grep demo Surprisingly, we didn\u0026rsquo;t find any symbol information related to pkga in the output of the executable file. This might be due to Go\u0026rsquo;s optimization. Let\u0026rsquo;s disable the optimization of the Go compiler and try again:\n1 2 3 $ go build -gcflags \u0026#39;-l -N\u0026#39; $ go tool nm demo | grep demo 108ca80 T demo/pkga.Foo After disabling inlining optimization, we can see that pkga.Foo appears in the final executable file demo, but the unused Bar function is not included.\nNow let\u0026rsquo;s look at an example with indirect dependencies:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // dead-code-elimination/demo2 $ tree . . ├── go.mod ├── main.go ├── pkga │ └── pkga.go └── pkgb └── pkgb.go // pkga/pkga.go package pkga import ( \u0026#34;demo/pkgb\u0026#34; \u0026#34;fmt\u0026#34; ) func Foo() string { pkgb.Zoo() return \u0026#34;Hello from Foo!\u0026#34; } func Bar() { fmt.Println(\u0026#34;This is Bar.\u0026#34;) } In this example, we call a new function Zoo from the pkgb package within the pkga.Foo function. Let\u0026rsquo;s compile this new example and see which functions are included in the final executable:\n1 2 3 4 $ go build -gcflags=\u0026#39;-l -N\u0026#39; $ go tool nm demo | grep demo 1093b40 T demo/pkga.Foo 1093aa0 T demo/pkgb.Zoo We can observe that only the functions reachable through the program execution path are included in the final executable!\nIn more complex examples, we can use the go build -ldflags='-dumpdep' command to view the call dependency relationship (using demo2 as an example):\n1 2 3 4 5 6 7 8 9 10 $ go build -ldflags=\u0026#39;-dumpdep\u0026#39; -gcflags=\u0026#39;-l -N\u0026#39; \u0026gt; deps.txt 2\u0026gt;\u0026amp;1 $ grep demo deps.txt # demo main.main -\u0026gt; demo/pkga.Foo demo/pkga.Foo -\u0026gt; demo/pkgb.Zoo demo/pkga.Foo -\u0026gt; go:string.\u0026#34;Hello from Foo!\u0026#34; demo/pkgb.Zoo -\u0026gt; math/rand.Int31n demo/pkgb.Zoo -\u0026gt; demo/pkgb..stmp_0 demo/pkgb..stmp_0 -\u0026gt; go:string.\u0026#34;Zoo in pkgb\u0026#34; From this, we can conclude that Go ensures that only the code that is actually used enters the final executable file, even if some code (such as pkga.Bar) and the code that is actually used (such as pkga.Foo) are in the same package. This mechanism also ensures that the final executable file size remains within a manageable range.\nNext, let\u0026rsquo;s explore this mechanism in Go.\n2. Dead Code Elimination Let\u0026rsquo;s review the build process of go build. The following steps outline the go build command:\nRead go.mod and go.sum: If the current directory contains a go.mod file, go build reads it to determine the project\u0026rsquo;s dependencies. It also verifies the integrity of the dependencies based on checksums in the go.sum file. Calculate the package dependency graph: go build analyzes the import statements in the packages being built and their dependencies to construct a dependency graph. This graph represents the relationships between packages, enabling the compiler to determine the build order of packages. Determine the packages to build: Based on the build cache and the dependency graph, go build determines which packages need to be built. It checks the build cache to see if the compiled packages are up to date. If any package or its dependencies have changed since the last build, go build will rebuild those packages. Invoke the compiler (go tool compile): For each package that needs to be built, go build invokes the Go compiler (go tool compile). The compiler converts the Go source code into machine code specific to the target platform and generates object files (.o files). Invoke the linker (go tool link): After compiling all the necessary packages, go build invokes the Go linker (go tool link). The linker merges the object files generated by the compiler into an executable binary file or a package archive file. It resolves symbols and references between packages, performs necessary relocations, and generates the final output. The entire build process can be represented by the following diagram:\nDuring the build process, go build performs various optimizations, such as dead code elimination and inlining, to improve the performance and reduce the size of the generated binary files. Dead code elimination is an important mechanism that ensures the controllable size of the final executable file in Go.\nThe implementation of the dead code detection algorithm can be found in the $GOROOT/src/cmd/link/internal/ld/deadcode.go file. The algorithm operates by traversing the graph and follows these steps:\nStart from the entry point of the system and mark all symbols reachable through relocations. Relocation represents the dependency relationship between two symbols. By traversing the relocation relationships, the algorithm marks all symbols that can be accessed from the entry point. For example, if the function pkga.Foo is called in the main function main.main, there will be a relocation entry for this function in main.main. After marking is complete, the algorithm marks all unmarked symbols as unreachable and dead code. These unmarked symbols represent the code that cannot be accessed by the entry point or any other reachable symbols. However, there is a special syntax element to note, which is types with methods. Whether the methods of a type are included in the final executable depends on different scenarios. In deadcode.go, the function implementation for marking reachable symbols distinguishes three cases of method invocation for reachable types:\nDirect invocation Invocation through reachable interface types Invocation through reflection: reflect.Value.Method (or MethodByName) or reflect.Type.Method (or MethodByName) In the first case, the invoked method is marked as reachable. In the second case, all reachable interface types are decomposed into method signatures. Each encountered method is compared with the interface method signatures, and if there is a match, it is marked as reachable. This method is conservative but simple and correct.\nIn the third case, the algorithm handles methods by looking for functions marked as REFLECTMETHOD by the compiler. The presence of REFLECTMETHOD on a function F means that F uses reflection for method lookup, but the compiler cannot determine the method name during static analysis. Therefore, all functions that call reflect.Value.Method or reflect.Type.Method are marked as REFLECTMETHOD. Functions that call reflect.Value.MethodByName or reflect.Type.MethodByName with non-constant arguments are also considered REFLECTMETHOD. If a REFLECTMETHOD is found, static analysis is abandoned, and all exported methods of reachable types are marked as reachable.\nHere is an example from the reference material:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // dead-code-elimination/demo3/main.go type X struct{} type Y struct{} func (*X) One() { fmt.Println(\u0026#34;hello 1\u0026#34;) } func (*X) Two() { fmt.Println(\u0026#34;hello 2\u0026#34;) } func (*X) Three() { fmt.Println(\u0026#34;hello 3\u0026#34;) } func (*Y) Four() { fmt.Println(\u0026#34;hello 4\u0026#34;) } func (*Y) Five() { fmt.Println(\u0026#34;hello 5\u0026#34;) } func main() { var name string fmt.Scanf(\u0026#34;%s\u0026#34;, \u0026amp;name) reflect.ValueOf(\u0026amp;X{}).MethodByName(name).Call(nil) var y Y y.Five() } In this example, type *X has three methods, and type *Y has two methods. In the main function, we call the methods of an X instance through reflection and directly call a method of a Y instance. Let\u0026rsquo;s see which methods of X and Y are included in the final executable:\n1 2 3 4 5 6 7 8 9 10 $ go build -gcflags=\u0026#39;-l -N\u0026#39; $ go tool nm ./demo | grep main 11d59c0 D go:main.inittasks 10d4500 T main.(*X).One 10d4640 T main.(*X).Three 10d45a0 T main.(*X).Two 10d46e0 T main.(*Y).Five 10d4780 T main.main ... ... We can observe that only the directly called method Five of the reachable type Y is included in the final executable, while all methods of the reachable type X through reflection are present! This aligns with the third case mentioned earlier.\n3. Summary This article introduced the dead code elimination and executable file size reduction mechanisms in the Go language. Through experiments, we verified that only the functions called on the program execution path are included in the final executable, and unused functions are eliminated.\nThe article explained the Go build process, including package dependency graph calculation, compilation, and linking steps, and highlighted dead code elimination as an important optimization strategy. The specific dead code elimination algorithm is implemented through graph traversal, where reachable symbols are marked and unmarked symbols are considered unused. The article also mentioned the handling of type methods.\nWith this dead code elimination mechanism, Go controls the size of the final executable file, achieving executable file size reduction.\nThe source code mentioned in this article can be downloaded here.\n4. References Getting the most out of Dead Code elimination all: binaries too big and growing aarzilli/whydeadcode ","date":"2024-05-11T09:10:00Z","permalink":"https://example.com/p/smart-go-compiler-slimming/","title":"Smart Go compiler: Slimming"},{"content":"Generate By DELLA-3\nWith the increasing popularity of open-source products, a backend engineer must be able to clearly identify whether an abnormal machine has been compromised. Based on my personal work experience, I have compiled several common scenarios of machines being hacked for reference.\nBackground : The following scenarios are observed on CentOS systems and are similar for other Linux distributions.\n1. Intruders May Delete Machine Logs Check if log information still exists or has been cleared using the following commands:\n2. Intruders May Create a New File for Storing Usernames and Passwords Check /etc/passwd and /etc/shadow files for any alterations using the following commands:\n3. Intruders May Modify Usernames and Passwords Examine the contents of /etc/passwd and /etc/shadow files for any changes using the following commands:\n4. Check Recent Successful and Last Unsuccessful Login Events on the Machine Refer to the log “/var/log/lastlog” using the following commands:\n5. Use who to View All Currently Logged-in Users on the Machine Refer to the log file “/var/run/utmp”:\n6. Use last To view Users Logged in Since Machine Creation Refer to the log file “/var/log/wtmp”:\n7. Use ac to View Connection Time (in Hours) for All Users on the Machine Refer to the log file “/var/log/wtmp”:\n8. If Abnormal Traffic is Detected Use tcpdump to capture network packets or iperf to check traffic.\n9. Review the /var/log/secure Log File Attempt to identify information about intruders using the following commands:\n10. Identify Scripts Executed by Abnormal Processes 1. Use the top command to view the PID of abnormal processes: 1. Search for the executable file of the process in the virtual file system directory: 11. File Recovery After Confirming Intrusion and Deletion of Important Files When a process opens a file, even if it’s deleted, it remains on the disk as long as the process keeps it open. To recover such files, use lsof the /proc directory. Most lsof information is stored in directories named after the process\u0026rsquo;s PID, such as /proc/1234, containing information for PID 1234. Each process directory contains various files providing insight into the process\u0026rsquo;s memory space, file descriptor list, symbolic links to files on disk, and other system information. lsof uses this and other kernel internal state information to generate its output. Using the information above, you can retrieve the data by examining /proc/\u0026lt;PID\u0026gt;/fd/\u0026lt;descriptor\u0026gt;.\nFor example, to recover /var/log/secure, follow these steps:\na. Check /var/log/secure, confirming its absence:\nb. Use lsof to check if any process is currently accessing /var/log/secure:\nc. From the information above, PID 1264 (rsyslogd) has opened the file with a file descriptor of 4. It’s marked as deleted. Therefore, you can check the corresponding information in /proc/1264/fd/4:\nd. You can recover the data by redirecting it to a file using I/O redirection:\ne. Confirm the existence of /var/log/secure it again. This method is particularly useful for many applications, especially log files and databases.\nThe above is the method I summarized for dealing with Linux intrusion. It can generally handle most problems. If you encounter an unresolved issue, it is best to seek advice from a professional IT operations and maintenance engineer.\nI may not have written it completely correctly, so if you have different opinions, please leave a comment and let me know.\n","date":"2024-05-10T01:00:00Z","permalink":"https://example.com/p/11-tips-for-detecting-and-responding-to-intrusions-on-linux/","title":"11 Tips for Detecting and Responding to Intrusions on Linux"},{"content":"\nI’ve spent the past few years building my second brain, and here are some lessons I’ve learned along the way.\nFrequent Switching of Note-Taking Software/Blogging Systems I’ve tried EverNote, WizNote, VNote, CSDN blog, Google blogspot, WordPress, only to end up scattering my blogs across multiple corners of the internet. The solution? Going all in one. I’ve now settled on Obsidian.\nConstantly Switching Note Formats I’ve experimented with txt, orgmode, markdown, rich text, and more, only to find myself frustrated with format conversions. Similar to the first point, each note system may not be universally compatible. The reason I chose Obsidian is for its markdown syntax. If need be, I can easily migrate it to any note system.\nMixing Fleeting Thoughts with Truly Useful Notes Fleeting thoughts serve to capture a moment of inspiration, but it’s only meaningful if you revisit them within a day or two and turn them into useful, relevant notes. Without timely review, good ideas drown in a sea of whims. Most of our daily thoughts are insignificant and should be discarded, while those with potential significance must be identified.\nMixing Project Notes with Knowledge Notes Only recording notes relevant to specific projects leads to the loss of interesting insights or ideas during the project. The correct approach is to extract universal knowledge from projects. I recommend using the P.A.R.A method to organize notes. For more information on P.A.R.A, you can refer to this page\nObsessive Note Organization A large pile of notes leads to an urge to organize knowledge, but too many attempts can affect the confidence to keep recording. The solution is to define the areas of interest and responsibility and not completely adopt a bottom-up knowledge management approach. When reaching the point of psychological pressure, use the MOCS (Maps of Content) method to organize notes (bidirectional linking is definitely worth trying). The most important aspect of a knowledge management system is to record your insights in one place, with the same format, and consistent standards.\n","date":"2024-05-08T23:46:56Z","permalink":"https://example.com/p/crafting-your-second-brain-lessons-learned-from-my-note-taking-journey/","title":"Crafting Your Second Brain Lessons Learned from My Note-Taking Journey"},{"content":"Last month’s hot topic in IT circles was Google laying off many developers from its Python core team and flutter/dart team, purportedly for a city-wide reorganization.\nhttps://news.ycombinator.com/item?id=40171125\nReportedly, those laid off were mostly core members responsible for important Python maintenance.\nAs a gopher, I pondered: will Google abandon Go? And if so, what would become of Go?\nWhat does Google offer to Go? Based on our past understanding clarified by @Lance Taylor and descriptions from various sources, we can estimate what Go has likely received from Google.\nJob Positions: Details regarding job positions of members of the Go core team, including compensation, benefits, and other remuneration. Software and Hardware Resources: Information on Go-related resources such as intellectual property, servers, domain names, and module management mirrors required by the community. Offline Activities: Possibility of reduced or scaled-down Go conferences worldwide in terms of funding and endorsement. Internal Resources of Big Corporations: Gradual loss of exposure to advanced projects and opportunities for Go’s adoption due to the absence of resources within Google. Promotion and Feedback Channels: Slower discovery and response to significant issues and features in Go as Google’s internal demands historically take precedence. Potential Scenarios What might happen if Google dissolves the Go core team and ceases all infrastructure support?\nDissolution of the Go core team, leading members may retire or seek employment elsewhere. If Google decides to cease all investment in Go, maintenance of Go could become more complex as it relies heavily on infrastructure. In such a scenario, Go might transition from Google to an external foundation, resulting in noticeable maintenance fluctuations. If Google chooses to continue investing in Go through other internal teams, the worst-case scenario could involve Google flexing its ownership of intellectual property, possibly leading to Go being rebranded. CNCF might take over Google’s mantle, organizing the future development of Go. Among CNCF projects, the Go language enjoys the widest adoption. Probability of Occurrence Currently, Go belongs to Google Cloud. Considering Go’s current trend focusing on customer success, the likelihood of Google Cloud shutting down Go is low. But who knows? I consulted gemini on this question.\ngenerated by Gemini\nConclusion Drawing from the example of Rust, which transitioned from Mozilla’s core to an independent foundation, Go could potentially thrive even more. A nonprofit organization will probably form around Go (or it may directly join CNCF), with enough support from major companies, at least for a period.\nReferences https://ajmani.net/2024/02/23/go-2019-2022-becoming-a-cloud-team/ https://www.reddit.com/r/golang/comments/1cft7mc/if_google_decided_to_part_with_the_core_go_team/ ","date":"2024-05-07T11:31:12Z","permalink":"https://example.com/p/if-google-no-longer-supports-golang/","title":"If Google No Longer supports Golang"},{"content":" source ：https://tonybai.com/2024/04/22/gopher-rust-first-lesson-all-about-rust\nTo talk about which backend programming language has been the hottest in the past two years,If Rust claims to be second , and no one dares to say first . Rust has topped the Stack Overflow\u0026rsquo;s most admired programming languages for 8 consecutive years, and it has even been referred to as the \u0026ldquo;perfect programming language\u0026rdquo; by Jack Dorsey, the co-founder of Twitter:\nhttps://twitter.com/jack/status/1474263588651126788\nrust is a perfect programming language\n\u0026mdash; jack (@jack) December 24, 2021 1. Why should I learn Rust? Learning Rust isn\u0026rsquo;t about riding the hype train; it\u0026rsquo;s about practical development needs. In certain situations, Go\u0026rsquo;s performance isn\u0026rsquo;t particularly good due to issues like STW. From benchmarks, Rust\u0026rsquo;s performance is three times better than Go\u0026rsquo;s: The Computer Language Benchmarks Game Visualization In our own business, after rewriting a gateway service in Rust, performance improved by about 70%. This is a significant improvement for our business scenario. Moreover, Rust\u0026rsquo;s widespread use in the Linux kernel validates its reliability, making Rust worth trying.\nHowever, the rise of Rust in certain domains has indeed sparked some dissatisfaction and controversy in other programming language communities. Particularly, the proposition from some in the Rust community to \u0026ldquo;Rewrite Everything in Rust\u0026rdquo; has made many programming language communities, especially the C++ community, quite uneasy. The Go community, on the other hand, is relatively more open and friendly. The mainstream view is that Go and Rust can complement each other, with each language playing its role in its respective areas of strength. Through cooperation rather than confrontation, developers can be provided with better choices. For more details, you can refer to an article co-authored by Steve Francia, former product manager of the Go team and author of Hugo, Rust vs. Go: Why They’re Better Together.\nIn other words, Go is still my primary language, but considering the requirements of my work, I need to systematically learn Rust. To avoid the \u0026ldquo;from learning to giving up\u0026rdquo; scenario, I plan to learn Rust while outputting. On one hand, this can motivate me to learn, and on the other hand, I hope to interact with readers promptly and correct any misunderstandings in learning.\nI\u0026rsquo;ve always believed that when you start learning a new language, you must understand its history and current status. This way, you can build an overall understanding of the language and anticipate its future direction. Moreover, it can establish a sense of \u0026ldquo;security\u0026rdquo; in learning, believing that it can bring you enough value and returns, thus enabling you to learn more confidently.\nIn this article, I\u0026rsquo;ll first explore the history of Rust\u0026rsquo;s development and its current state, as well as its unique design philosophy. I\u0026rsquo;ll also make a simple comparison with Go, hoping to provide myself and the readers with a preliminary understanding of Rust.\n2. Rust\u0026rsquo;s History and Current Status 2.1 The Birth and Evolution of Rust Rust was born in 2006, a year earlier than the \u0026ldquo;conspiracy\u0026rdquo; of the three Google giants to create the Go language. However, compared to the three founders of Go: Ken Thompson, Turing Award winner, co-inventor of C syntax, and father of Unix; Rob Pike, leader of the Plan 9 operating system and original designer of UTF-8 encoding; and Robert Griesemer, one of the designers of the HotSpot virtual machine for Java and the JavaScript V8 engine for the Chrome browser, the identity and status of Rust\u0026rsquo;s father, Graydon Hoare, were not so \u0026ldquo;prominent\u0026rdquo;. At that time, he was just a Canadian developer under 30 years old working at Mozilla Research:\nThe birth of a new generation programming language often comes with a story, such as the founders of Go frequently encountering long compile times in C++ projects at Google. Whenever they started compiling a C++ project, they had to wait for a long time, during which they could drink several cups of coffee. This deeply impressed them and made them realize the need to design a new language with faster compilation speed, thus Go was born. Similar to the \u0026ldquo;drink coffee, wait for C++ project to compile\u0026rdquo; scenario, the birth of Rust also has a story:\nIn 2006, 29-year-old Hoare returned home to Vancouver one day, only to find that the elevator was broken, and the elevator software crashed! He had to climb the stairs back to his 21st-floor apartment. As he climbed the stairs, he felt very annoyed. He thought, \u0026ldquo;We programmers actually can\u0026rsquo;t create an elevator that can work properly without crashing!\u0026rdquo; Hoare knew that many such crashes were caused by issues with memory usage in programs. The software inside devices like elevators is usually written in C++ or C, languages known for allowing programmers to write code that runs very fast and is quite compact. The problem is that these languages also make it easy to inadvertently introduce memory errors, which can lead to crashes. Hoare decided to do something about it. So he opened his laptop and started designing a new programming language, one that he hoped could write small and fast code without memory errors, and he named it Rust.\nThis story is obviously unverifiable. But what can be confirmed is that for several years starting from 2006, the personal language project Rust created by Hoare was not actually used to improve elevator systems, but was sponsored by Mozilla and used in the continuous development of Mozilla\u0026rsquo;s browser engine Servo. Mozilla officially announced the project in 2010, and Hoare also introduced the Rust language for the first time in a speech in 2010:\nThe first line of Rust code was also open-sourced in 2010:\nIn addition, the initial Rust compiler was implemented in OCaml, and in 2011, the Rust team re-implemented the compiler in Rust based on LLVM and achieved bootstrapping. That same year, Rust also got its own logo, inspired by a bicycle gear:\nIn 2012, Graydon Hoare was interviewed by InfoQ and talked about leading the Rust team in developing Rust, a systems programming language at Mozilla, including Rust\u0026rsquo;s features, advantages and differences compared to C/C++/Java/Go, and Rust 1.0 release plans.\nHowever, in the following year, in 2013, Graydon Hoare resigned as leader of the Rust team due to exhaustion, leaving his own Rust team and distancing himself from Rust development. Hoare\u0026rsquo;s departure was a significant loss for the Rust team and the language itself, but the Rust community and team took proactive measures to ensure the continued development and evolution of Rust.\nIn November 2014, Rust officially announced cargo and crates.io. Cargo is Rust\u0026rsquo;s project build manager, while crates.io is the central package repository for Rust code maintained by the Rust team. With cargo, developers can easily build and publish packages to crates.io or pull dependencies of Rust code from crates.io.\nOn May 15, 2015, Rust reached a milestone moment: Rust 1.0 was officially released!, which was 3 years later than the release of Go 1.0. However, as the official blog stated, \u0026ldquo;the release of version 1.0 marks the end of the chaos. This version is the formal beginning of our stability commitment, providing a solid foundation for building applications and libraries. From now on, major changes are essentially out of scope (some minor warnings apply, such as compiler errors).\u0026rdquo;\nAfter the release of Rust 1.0, the release cycle and rhythm of Rust were determined, with a stable version released every 6 weeks. Following this rhythm, Rust 1.1 Beta was released simultaneously with Rust 1.0. After six weeks of testing, Rust 1.1 Beta became Rust 1.1 stable, and Rust 1.2 Beta was released, and so on. Of course, Rust also has a nightly build version, which contains the latest but unstable features. Compared to the Go community and developers who can only \u0026ldquo;get high\u0026rdquo; twice a year, Rust developers and the community are more fortunate to \u0026ldquo;get high\u0026rdquo; every six weeks!\nRust\u0026rsquo;s evolution is driven by RFCs (Request For Comments), and this measure was established based on RFCs before the release of Rust 1.0. This is similar to the Go Proposal process, but it feels more standardized and rigorous, which of course is related to the composition and rules of governance structures of the two languages.\nHowever, the\nevolution and development of Rust is not as smooth and perfect as one might expect. For example, at the Rust 2018 edition launch event, Alex Crichton, one of the core members of the Rust team, mentioned in the keynote speech that in the evolution of the Rust language, some past design decisions may not have been optimal, which sometimes hinders the language\u0026rsquo;s progress. One example is the error handling mechanism. However, Rust\u0026rsquo;s development team has proven that they are good at introspection and adjustment. For example, the aforementioned error handling mechanism was greatly improved with the introduction of the \u0026ldquo;Try\u0026rdquo; trait in Rust 1.26, and with the release of Rust 1.31, the Rust 2018 edition resolved many issues with Rust\u0026rsquo;s design.\n2.2 Rust\u0026rsquo;s Current Status In the past decade, Rust has gradually developed from a personal project of Graydon Hoare to a popular systems programming language with a large and active community. According to GitHub\u0026rsquo;s 2020 Octoverse report, Rust was the fastest-growing programming language in the GitHub open-source community, with 235% more contributors than in 2019, making it the most loved language in the Stack Overflow Developer Survey for five consecutive years from 2016 to 2020.\nHowever, it should be pointed out that Rust is still not widely used in large-scale commercial projects, especially compared to C/C++/Java/Go. The main reason is that Rust\u0026rsquo;s ecosystem and maturity are still not comparable to those languages. For example, the Alibaba e-commerce team mentioned in 2017 that they have 100,000+ Java developers and have invested heavily in Java and the JVM ecosystem. They also mentioned that in recent years, Alibaba has considered using Rust to solve some distributed computing problems, but has not yet considered using Rust in large-scale commercial projects due to concerns about the immaturity of Rust\u0026rsquo;s ecosystem.\nHowever, Rust is gradually being adopted and applied in some fields and areas where C/C++ is traditionally dominant, such as systems programming, game development, and blockchain systems. For example, in the blockchain field, in addition to the Solana project I mentioned earlier, the Parity project, which is known for the development of the Polkadot blockchain framework, is also developed in Rust. In addition, in the blockchain field, some small and medium-sized blockchain projects also use Rust to develop blockchain systems. For example, the Bitcoin Light client library BTC-Rust implemented by the Bitcoin-NG team is developed in Rust.\n3. The Design Philosophy of Rust The design philosophy of a programming language is like the values of a person—it dictates its behavior. If you don\u0026rsquo;t agree with someone\u0026rsquo;s values, it\u0026rsquo;s hard to maintain a continuous relationship, as they say, \u0026ldquo;different paths do not converge.\u0026rdquo; Similarly, if you don\u0026rsquo;t agree with the design philosophy of a programming language, you\u0026rsquo;ll likely encounter the problems mentioned earlier in your subsequent language learning, which may dampen your motivation to continue learning. Therefore, before diving into Rust syntax and coding, let\u0026rsquo;s first understand Rust\u0026rsquo;s design philosophy. After understanding these, you\u0026rsquo;ll have a deeper understanding of why you\u0026rsquo;re learning Rust.\n3.1 Core Values of Rust In June 2019, Rust core team member Stephen Klabnik delivered a speech titled How Rust Views Tradeoffs at QCon London, where he outlined his personal understanding of Rust\u0026rsquo;s core values, which are the points Rust\u0026rsquo;s team refuses to compromise on when making design decisions, including memory safety, execution speed, and productivity:\nAccording to Stephen Klabnik, these three core values are ordered, with memory safety being paramount, followed by high performance, and finally productivity. When conflicts arise among them, decisions are made according to the highest value!\nThis is consistent with the official description of Rust:\nThe \u0026ldquo;Reliable\u0026rdquo; corresponds to memory safety, while \u0026ldquo;efficient\u0026rdquo; has two meanings: runtime efficiency and high productivity during development.\nThese three values are the design goals of the Rust language and the essence of its characteristics and advantages. After losing Graydon Hoare, the father of the language, these values became the fundamental basis for the Rust core team to determine the direction of language evolution.\nMemory Safety\nMemory safety is the most important value for Rust. It means that Rust programs won\u0026rsquo;t suffer from memory leaks, buffer overflows, dangling pointers, and other memory-related errors at runtime (without using unsafe code). These errors not only lead to program crashes but can also result in security vulnerabilities. Rust ensures memory safety by features like ownership, lifetimes, and borrowing, which are thoroughly checked at compile time. Rust\u0026rsquo;s memory safety mechanism not only enhances program stability and reliability but also reduces development and maintenance complexity. With Rust\u0026rsquo;s ability to detect memory errors at compile time, developers don\u0026rsquo;t have to spend a lot of time and effort searching for and fixing these errors.\nHigh Performance High performance is the second core value of Rust, closely following memory safety. One of Rust\u0026rsquo;s design goals is to be a high-performance systems programming language. Through features like zero-cost abstractions, move semantics, and generic programming, Rust enables programs to achieve performance comparable to traditional systems programming languages like C and C++ at runtime.\nRust\u0026rsquo;s high-performance mechanism not only improves program execution speed but also reduces hardware costs. Because Rust can better utilize hardware resources, Rust programs typically outperform programs in other languages with the same hardware conditions and resource overhead.\nProductivity Productivity is the third core value of Rust. One of Rust\u0026rsquo;s design goals is to be a language that enhances developer productivity. With features like the Cargo package manager, smart editor support, rich library ecosystem, and detailed system documentation, Rust makes it easier for developers to write, debug, and maintain Rust programs.\n3.2 Secondary Values of Rust Stephen Klabnik also summarized three secondary values of Rust:\nWe see that Rust\u0026rsquo;s secondary values include ergonomics, compile times, and correctness. These values are also design goals of the Rust language, but unlike the primary core values mentioned above, they are subject to compromise.\nErgonomics refers to the ease of use of the Rust language, which is an important design goal of Rust. Rust aims to make it easier for developers to write Rust programs through simple syntax and a rich library ecosystem.\nCompile Times refer to the compilation time of the Rust compiler. Rust\u0026rsquo;s compiler is slow, which is a problem that the Rust team is actively working to optimize. However, the Rust team is more concerned about the final execution speed of the binary than making the compiler faster, hence why compile time is a secondary value.\nCorrectness refers to the correctness of Rust programs. Rust really cares about whether your program is correct and aims to ensure the correctness of Rust programs as much as possible through a powerful type system and static checks. However, Rust is not willing to rely entirely on types and proof assistants to prove the correctness of your code.\n3.3 Comparison with Go\u0026rsquo;s Values Let\u0026rsquo;s compare Go\u0026rsquo;s official introduction to Go\u0026rsquo;s implicit values (design philosophy):\nIn the official description of Go, there are three keywords: Simple, Secure, and Scalable.\nSimple is the primary design principle of Go. The designers of Go hope that Go can be simple and easy to use, enabling developers to learn and use Go more quickly to rapidly develop production capabilities. Go has a simple and understandable syntax, and it removes many complex features found in other programming languages, such as type hierarchies and inheritance, making Go more concise, easy to learn, read, use, and maintain.\nSecure is about making Go more secure and reliable, avoiding common security vulnerabilities found in many other programming languages. Go achieves this by automatically managing memory through garbage collection, avoiding common memory leaks and buffer overflow issues found in many other programming languages. Additionally, Go provides lightweight goroutines and channels, making it easier for developers to implement concurrent programming. With data race detection tools, Go also prevents common data race issues in concurrent programming. Furthermore, Go provides a simple and easy-to-use explicit error handling mechanism, ensuring no error handling is missed by developers.\nScalable is reflected in Go\u0026rsquo;s engineering orientation, built-in concurrency, and a philosophy that emphasizes composition. The designers of Go hope that Go can better support scalability, enabling Go programs to better adapt to different organizational scales, workloads, and hardware environments. Go achieves this through simple syntax, module-based reproducible build management, extremely fast compilation speed, high-quality standard library, practical toolchain, powerful built-in concurrency mechanism, and interface-oriented programming, making Go programs more scalable and productive.\n4 Conclusion In summary, Rust prioritizes safety, low-level control, and optimal performance, while Go emphasizes simplicity, security, scalability, and engineering efficiency. There are differences in their positioning and design philosophies, but they also share some common characteristics, such as modern toolchains and active communities.\nIn this article, we\u0026rsquo;ve learned about the birth, current development, and unique design philosophy of Rust. By comparing it with Go, we can see some differences in their backgrounds, goals, and design philosophies.\nAs software systems become increasingly complex, the demand for security, performance, and concurrency is also rising. As a new language focused on low-level systems programming and performance optimization, Rust is attracting more and more developers\u0026rsquo; attention. I believe that through comprehensive and systematic learning of Rust later on, we will all gain a deeper understanding and mastery of Rust.\nIf you find Rust\u0026rsquo;s values align with yours and you agree with Rust\u0026rsquo;s future development, stay tuned for the next article, where we\u0026rsquo;ll start hands-on learning Rust!\n5 References Rust Wikipedia How Rust went from a side project to the world’s most-loved programming language 2022 Review | The adoption of Rust in Business How Rust Views Tradeoffs Unofficial Rust mascot Ferris ","date":"2024-04-26T09:43:00Z","permalink":"https://example.com/p/rust-lesson-1-a-gophers-view-on-rust/","title":"Rust Lesson 1： A Gopher's View on Rust"},{"content":"As an ordinary person, when you browse the web, you may not realize that the web pages sent to you by the server are actually compressed.\nIf you like a programmer, press F12 in the browser, you\u0026rsquo;ll find something like this:\nIt means: In order to save bandwidth and provide speed, I (the server) compressed the content using gzip, and you (the browser) need to decompress it to view it!\nIn HTTP compression, besides gzip, there are also algorithms like compress, deflate, br, etc., which can be dazzling.\nHowever, all these compression algorithms have an ancestor: LZ algorithm.\nLZ comes from the names of two people: Abraham Lempel and Jacob Ziv.\nBoth of them passed away in 2023, living a long life, with Lempel living to be 86 years old and Ziv living to be 91 years old.\nOrigin Data compression can be divided into two types: lossy compression, such as MP3, JPEG, where some unimportant data is deleted during compression, and lossless compression, where binary bits magically disappear, making files significantly smaller, facilitating storage and transmission.\nIn 1948, after Claude Shannon founded information theory, everyone has been working on one thing: how to find the optimal coding to compress a piece of information.\nShannon and Fano were the first to propose the Shannon-Fano coding.\nIt constructs a binary tree from top to bottom by grouping symbols.\nHowever, this method is not the optimal solution and the encoding is not a prefix code, making it prone to ambiguity.\nLater, while teaching information theory at MIT, Fano challenged his students: either take the final exam or improve existing data compression algorithms.\nA graduate student named Huffman didn\u0026rsquo;t like exams, so he chose the latter path.\nHuffman didn\u0026rsquo;t know that even the famous Shannon struggled with this problem. He researched for several months, developed various methods, but none worked.\nJust as he was about to give up and throw his notes into the trash, a wonderful and elegant algorithm crossed his mind: build a binary tree from bottom to top based on the frequency of characters, which is the famous Huffman algorithm.\nHuffman\u0026rsquo;s algorithm is called \u0026ldquo;optimal coding\u0026rdquo; and achieves two goals:\n(1) No character encoding is a prefix of another character encoding.\n(2) The total length of the information encoding is minimized.\nAlthough the Huffman algorithm is excellent, it has a huge limitation: it requires obtaining the probability of each character appearing first, and then compression encoding can be done, which is often impossible in many cases.\nIn the 1970s, with the emergence of the Internet, this problem became more prominent.\nIs it possible to compress data while reading it?\nBreakthrough Ziv and Lempel from the Technion-Israel Institute of Technology jointly challenged this problem.\nThe two were a good team, with Ziv being good at statistics and information theory, while Lempel excelled in Boolean algebra and computer science.\nIn 1977, they both came to Bell Labs for academic sabbaticals.\nAcademic sabbatical, also known as \u0026ldquo;intellectual leave,\u0026rdquo; gives you a long period of\nleave (like six months) after working for a few years, during which you can do whatever you want, and it\u0026rsquo;s paid.\nThe sabbaticals of the big shots are interesting. For example, Ken Thompson, the inventor of Unix, returned to his alma mater, Berkeley, during his sabbatical and spread Unix there, inspiring Bill Joy and others to develop BSD.\nZiv and Lempel were similar. They went to Bell Labs in the United States for academic sabbaticals and co-authored a paper during their \u0026ldquo;sabbatical\u0026rdquo;: \u0026ldquo;A Universal Algorithm for Sequential Data Compression,\u0026rdquo; proposing an algorithm based on a \u0026ldquo;sliding window,\u0026rdquo; which does not directly consider character frequencies but instead finds repeated data blocks (such as strings, byte sequences, etc.) and references the positions where these data blocks appeared previously.\nThis algorithm is LZ77, which is applicable to any type of data, requires no preprocessing (statistical character appearance probabilities), and achieves extremely high compression ratios with just one pass.\nThe following year, they continued their efforts and improved LZ77 to become LZ78, which could perfectly reconstruct data from compressed data and was more efficient than previous algorithms.\nChaos An invaluable treasure like the LZ algorithm remained in the theoretical realm for several years without widespread use.\nIt wasn\u0026rsquo;t until 1984, when Terry Welch of DEC created the LZW algorithm based on LZ, which was used in Unix\u0026rsquo;s compress program.\nWith the widespread dissemination of Unix, the LZ algorithm began to enter the fast lane of rapid development.\nHowever, it also entered an era of chaotic competition.\nIn 1985, Thom Henderson, while downloading files from BBS, found it tedious to download one by one, as dial-up internet was too slow. So he wrote a software called ARC, which could compress multiple files into one, making it much more convenient.\nIn 1986, Phillip Katz discovered ARC, liked it, but felt that the compression speed was too slow. So he rolled up his sleeves, rewrote the key compression and decompression parts in assembly language, and created PKARC, which he started selling.\nWhen Thom Henderson saw his business being snatched away, he sued Phillip Katz, and the reasons were sufficient: the comments and spelling errors in your PKARC are the same as my ARC, you\u0026rsquo;re plagiarizing! Also, while my ARC is open source, the protocol specifies that you can only view it, not modify it!\nIn the end, ARC won the lawsuit, and Phillip Katz paid tens of thousands of dollars in damages.\nGenius Phillip Katz was naturally not satisfied. He studied the LZ77 algorithm and the Huffman algorithm, combined them, and created a new compression algorithm (deflate) and a new file format (zip), as well as the new software PKZIP.\nPKZIP quickly outperformed ARC in both compression ratio and decompression speed, and quickly dominated the DOS era.\nSince the ZIP format was open, the open-source info-zip group also released the open-source, free unzip and zip, implementing the deflate algorithm.\nLater, Jean-loup Gailly and Mark Adler developed the famous gzip (file format + utility) based on deflate, replacing compress on Unix.\ngzip is the HTTP compression format seen at the beginning of the article.\nIn 1991, Nico Mak felt dissatisfied with the command line of PKZIP, so he developed a front-end for Windows 3.1 based on PKZIP (later replaced by the open-source info-zip), allowing people to compress files using a graphical interface. This is the famous WinZip.\nDespite WinZip\u0026rsquo;s success, it was still \u0026ldquo;parasitic\u0026rdquo; on the Windows platform.\nUsers find that WinZip has an exquisite interface and is user-friendly. There is no need to remember those annoying parameters and compression can be completed with a few clicks of the mouse.\nWinZip quickly took over all PCs and became one of the most popular shareware programs in the 1990s.\nWindows intervened and simply integrated Zip functionality into the operating system, ending everything.\nConclusion From LZ77 to LZW, compress, Deflate, gzip\u0026hellip; Lossless compression algorithms have been continuously patched and gradually formed into a huge family. However, no matter how they change, their principles and ideas are not much different from the original LZ algorithm.\nThese algorithms help us compress images, compress text, compress content transmitted over the Internet, and have become an indispensable part of our daily lives.\nIt\u0026rsquo;s no exaggeration to say that the LZ algorithm and its descendants have dominated the world.\n","date":"2024-04-25T18:47:00Z","permalink":"https://example.com/p/the-magical-algorithms-written-by-two-old-men-dominating-the-world/","title":"The Magical Algorithms Written by Two Old Men, Dominating the World!"},{"content":"Go is a statically typed compiled language designed to be concise and efficient. While Go is not a purely object-oriented language, we can still use design patterns to improve code readability and maintainability. Today, I will introduce a common design pattern: the Decorator pattern.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nWhat is the Decorator Pattern? The Decorator pattern is a design pattern that allows us to dynamically add behavior to an object at runtime without altering its implementation. This is achieved by creating a wrapper object or decorator that contains the original object and provides an enhanced interface to add new behavior.\nIn Go, we can use functions as decorators because Go supports higher-order functions, which means functions can be passed as parameters and returned as values.\nAn Example To better understand the Decorator pattern, let\u0026rsquo;s see how we can implement it in Go through an example.\nFirst, we define a function type Foo and a decorator type FooDecorator:\n1 2 3 type Foo func(string) string type FooDecorator func(Foo) Foo Then, we can create a decorator that takes a function of type Foo and returns a new function of type Foo which adds some behavior before and after calling the original function:\n1 2 3 4 5 6 7 8 func WithLog(decorated Foo) Foo { return func(s string) string { fmt.Println(\u0026#34;Before calling the decorated function\u0026#34;) result := decorated(s) fmt.Println(\u0026#34;After calling the decorated function\u0026#34;) return result } } Now, we can create a Foo function and enhance it using the decorator:\n1 2 3 4 5 6 7 8 9 10 func main() { foo := func(s string) string { fmt.Println(\u0026#34;Foo function called\u0026#34;) return s } foo = WithLog(foo) foo(\u0026#34;Hello, world!\u0026#34;) } In this example, we create a Foo function and use the WithLog decorator to enhance it. When we call the enhanced function, it first prints a message, then calls the original Foo function, and finally prints another message.\nThis is the Decorator pattern in Go. By using decorators, we can dynamically add new behavior without modifying the original function.\nAn HTTP-related Example Next, let\u0026rsquo;s look at an example related to handling HTTP requests. First, we\u0026rsquo;ll start with a simple HTTP server code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strings\u0026#34; ) func WithServerHeader(h http.HandlerFunc) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { log.Println(\u0026#34;---\u0026gt;WithServerHeader()\u0026#34;) w.Header().Set(\u0026#34;Server\u0026#34;, \u0026#34;HelloServer v0.0.1\u0026#34;) h(w, r) } } func hello(w http.ResponseWriter, r *http.Request) { log.Printf(\u0026#34;Received Request %s from %s\\n\u0026#34;, r.URL.Path, r.RemoteAddr) fmt.Fprintf(w, \u0026#34;Hello, World! \u0026#34;+r.URL.Path) } func main() { http.HandleFunc(\u0026#34;/v1/hello\u0026#34;, WithServerHeader(hello)) err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) if err != nil { log.Fatal(\u0026#34;ListenAndServe: \u0026#34;, err) } } In this code, we use the Decorator pattern. The WithServerHeader() function acts as a decorator that takes an http.HandlerFunc and returns a modified version. This example is relatively simple, as we only add a response header using WithServerHeader(). However, we can create many more functions like this, such as writing authentication cookies, checking authentication cookies, and logging.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strings\u0026#34; ) func WithServerHeader(h http.HandlerFunc) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { log.Println(\u0026#34;---\u0026gt;WithServerHeader()\u0026#34;) w.Header().Set(\u0026#34;Server\u0026#34;, \u0026#34;HelloServer v0.0.1\u0026#34;) h(w, r) } } func WithAuthCookie(h http.HandlerFunc) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { log.Println(\u0026#34;---\u0026gt;WithAuthCookie()\u0026#34;) cookie := \u0026amp;http.Cookie{Name: \u0026#34;Auth\u0026#34;, Value: \u0026#34;Pass\u0026#34;, Path: \u0026#34;/\u0026#34;} http.SetCookie(w, cookie) h(w, r) } } func WithBasicAuth(h http.HandlerFunc) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { log.Println(\u0026#34;---\u0026gt;WithBasicAuth()\u0026#34;) cookie, err := r.Cookie(\u0026#34;Auth\u0026#34;) if err != nil || cookie.Value != \u0026#34;Pass\u0026#34; { w.WriteHeader(http.StatusForbidden) return } h(w, r) } } func WithDebugLog(h http.HandlerFunc) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { log.Println(\u0026#34;---\u0026gt;WithDebugLog\u0026#34;) r.ParseForm() log.Println(r.Form) log.Println(\u0026#34;path\u0026#34;, r.URL.Path) log.Println(\u0026#34;scheme\u0026#34;, r.URL.Scheme) log.Println(r.Form[\u0026#34;url_long\u0026#34;]) for k, v := range r.Form { log.Println(\u0026#34;key:\u0026#34;, k) log.Println(\u0026#34;val:\u0026#34;, strings.Join(v, \u0026#34;\u0026#34;)) } h(w, r) } } func hello(w http.ResponseWriter, r *http.Request) { log.Printf(\u0026#34;Received Request %s from %s\\n\u0026#34;, r.URL.Path, r.RemoteAddr) fmt.Fprintf(w, \u0026#34;Hello, World! \u0026#34;+r.URL.Path) } func main() { http.HandleFunc(\u0026#34;/v1/hello\u0026#34;, WithServerHeader(WithAuthCookie(hello))) http.HandleFunc(\u0026#34;/v2/hello\u0026#34;, WithServerHeader(WithBasicAuth(hello))) http.HandleFunc(\u0026#34;/v3/hello\u0026#34;, WithServerHeader(WithBasicAuth(WithDebugLog(hello)))) err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) if err != nil { log.Fatal(\u0026#34;ListenAndServe: \u0026#34;, err) } } Pipeline of Multiple Decorators When using multiple decorators, the code can become less visually appealing as we need to nest functions layer by layer. However, we can refactor the code to make it cleaner. To do this, we first write a utility function that iterates through and calls each decorator:\n1 2 3 4 5 6 7 8 9 type HttpHandlerDecorator func(http.HandlerFunc) http.HandlerFunc func Handler(h http.HandlerFunc, decors ...HttpHandlerDecorator) http.HandlerFunc { for i := range decors { d := decors[len(decors)-1-i] // iterate in reverse h = d(h) } return h } Then, we can use it like this:\n1 2 http.HandleFunc(\u0026#34;/v4/hello\u0026#34;, Handler(hello, WithServerHeader, WithBasicAuth, WithDebugLog)) Conclusion In this article, I demonstrated the Decorator pattern using two examples. However, since Go does not support annotations as a syntactic sugar, using decorators can be a bit cumbersome. Nevertheless, the concept is still important, and we can apply this way of thinking to write higher-quality code in our daily development.\n","date":"2024-04-23T14:33:00Z","permalink":"https://example.com/p/go-program-pattern-05-decorations/","title":"Go Program Pattern 05: Decorations"},{"content":"In the previous article, I briefly introduced the composite pattern in Go, which was explained in a simple manner. We understood that Go can achieve polymorphism in object-oriented programming through composition.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nIn this article, let\u0026rsquo;s learn about Inversion of Control (IoC). Inversion of Control is a software design method that involves separating control logic from business logic. Instead of writing control logic within the business logic, which creates a dependency of control logic on business logic, IoC reverses this relationship and makes the business logic dependent on the control logic.\nInversion of Control Let\u0026rsquo;s consider an example where we want to implement a functionality to record the existence of numbers. We can easily implement the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 type IntSet struct { data map[int]struct{} } func NewIntSet() IntSet { return IntSet{make(map[int]struct{})} } func (set *IntSet) Add(x int) { set.data[x] = struct{}{} } func (set *IntSet) Delete(x int) { delete(set.data, x) } func (set *IntSet) Contains(x int) bool { _, ok := set.data[x] return ok } The above code uses a map to store numbers and provides functionalities for adding, deleting, and checking the existence of numbers. Everything seems perfect.\nNow, suppose we want to add an undo feature to this functionality. How can we do that? With a little thought, we can write clear code by wrapping IntSet into UndoableIntSet. Here\u0026rsquo;s the code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 type UndoableIntSet struct { // Poor style IntSet // Embedding (delegation) functions []func() } func NewUndoableIntSet() UndoableIntSet { return UndoableIntSet{NewIntSet(), nil} } func (set *UndoableIntSet) Add(x int) { // Override if !set.Contains(x) { set.data[x] = true set.functions = append(set.functions, func() { set.Delete(x) }) } else { set.functions = append(set.functions, nil) } } func (set *UndoableIntSet) Delete(x int) { // Override if set.Contains(x) { delete(set.data, x) set.functions = append(set.functions, func() { set.Add(x) }) } else { set.functions = append(set.functions, nil) } } func (set *UndoableIntSet) Undo() error { if len(set.functions) == 0 { return errors.New(\u0026#34;No functions to undo\u0026#34;) } // invert the order of calls index := len(set.functions) - 1 if function := set.functions[index]; function != nil { function() } set.functions = set.functions[:index] return nil } This approach is a good choice for extending existing code with new functionalities. It allows for a balance between reusing the existing code and adding new features. However, the main issue with this approach is that the Undo operation is actually a form of control logic, not business logic. The Undo feature cannot be reused because it contains a lot of business logic related to IntSet.\nDependency Inversion Let\u0026rsquo;s explore another implementation approach where we extract the undo feature and make IntSet depend on it:\n1 2 3 4 5 6 7 8 9 10 11 12 type Undo []func() func (undo *Undo) Add(u func()) { *undo = append(*undo, u) } func (undo *Undo) Undo() { if len(*undo) == 0 { return } index := len(*undo) - 1 (*undo)[index]() *undo = (*undo)[:index] } Next, we embed Undo in IntSet:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 type IntSet struct { data map[int]struct{} undo Undo } func NewIntSet() IntSet { return IntSet{make(map[int]struct{}), make(Undo, 0)} } func (set *IntSet) Undo() { set.undo.Undo() } func (set *IntSet) Add(x int) { if set.Contains(x) { return } else { set.undo.Add(func() { set.Delete(x) }) set.data[x] = struct{}{} } } func (set *IntSet) Delete(x int) { if !set.Contains(x) { return } else { set.undo.Add(func() { set.Add(x) }) delete(set.data, x) } } func (set *IntSet) Contains(x int) bool { _, ok := set.data[x] return ok } In our application, we can use it as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 func main() { set := NewIntSet() set.Add(1) set.Add(2) fmt.Println(set.Contains(2)) set.Undo() fmt.Println(set.Contains(2)) set.Delete(1) fmt.Println(set.Contains(1)) set.Undo() fmt.Println(set.Contains(1)) } Output:\n1 2 3 4 5 /Users/hxzhouh/Library/Caches/JetBrains/GoLand2023.3/tmp/GoLand/___go_build_github_com_hxzhouh_go_example_pattern_ioc true false false true This is Inversion of Control, where the control logic Undo no longer depends on the business logic IntSet, but rather the business logic IntSet depends on Undo. Now, the Undo feature can be easily used by other business logics.\n","date":"2024-04-23T14:32:00Z","permalink":"https://example.com/p/go-program-pattern-03-inversion-of-control/","title":"Go Program Pattern 03: Inversion of Control"},{"content":"Map-Reduce is a programming paradigm used for processing large-scale datasets. It helps simplify the process of parallel computation and improves computational efficiency.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nFirst, let\u0026rsquo;s understand the concepts of Map and Reduce.\nMap: In the Map phase, the input dataset is divided into a series of key-value pairs, and the same operation is applied to each key-value pair. This operation can be a function or a code block used to process each key-value pair and generate intermediate results. Reduce: In the Reduce phase, the intermediate results generated in the Map phase are combined and processed to obtain the final output result. In the Reduce phase, we can aggregate, summarize, or perform other operations on intermediate results with the same key. The core idea of the Map-Reduce programming paradigm is \u0026ldquo;divide and conquer.\u0026rdquo; It allows us to break down complex computational tasks into multiple independent subtasks, process these subtasks in parallel, and then merge the results to obtain the final result.\nBasic Example Here is a simple example demonstrating the workflow of Map-Reduce:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 func MapFunction(arr []string, fn func(string) string) \u0026lt;-chan string { ch := make(chan string) go func() { for _, v := range arr { ch \u0026lt;- fn(v) } close(ch) }() return ch } func ReduceFunction(ch \u0026lt;-chan string, fn func(string, string) string) string { var res string for v := range ch { res = fn(res, v) } return res } func main() { // generate 10 random strings arr := []string{\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;, \u0026#34;f\u0026#34;, \u0026#34;g\u0026#34;, \u0026#34;h\u0026#34;, \u0026#34;i\u0026#34;} // map ch := MapFunction(arr, func(s string) string { return strings.ToUpper(s) }) // reduce res := ReduceFunction(ch, func(s1, s2 string) string { return s1 + s2 }) fmt.Println(res) } go.dev\nIn this example, we define a MapFunction that takes a string array and converts each element to uppercase using a custom function fn, returning a channel. The ReduceFunction takes a channel and a custom function fn to concatenate the results and print them out.\nThe following image provides a metaphor that vividly illustrates the business semantics of Map-Reduce, which is very useful in data processing.\nYou may understand that Map/Reduce is just a control logic, and the real business logic is defined by the data and the function passed to them. Yes, this is a classic programming pattern of separating \u0026ldquo;business logic\u0026rdquo; from \u0026ldquo;control logic.\u0026rdquo; Now let\u0026rsquo;s take a look at a code example with meaningful business logic to reinforce the understanding of separating \u0026ldquo;control logic\u0026rdquo; and \u0026ldquo;business logic.\u0026rdquo;\nBusiness Example Employee Information\nFirst, we have an employee object and some data:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type Employee struct { Name string Age int Vacation int Salary int } var list = []Employee{ {\u0026#34;Hao\u0026#34;, 44, 0, 8000}, {\u0026#34;Bob\u0026#34;, 34, 10, 5000}, {\u0026#34;Alice\u0026#34;, 23, 5, 9000}, {\u0026#34;Jack\u0026#34;, 26, 0, 4000}, {\u0026#34;Tom\u0026#34;, 48, 9, 7500}, {\u0026#34;Marry\u0026#34;, 29, 0, 6000}, {\u0026#34;Mike\u0026#34;, 32, 8, 4000}, } Related Reduce/Filter Functions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 func EmployeeCountIf(list []Employee, fn func(e *Employee) bool) int { count := 0 for i, _ := range list { if fn(\u0026amp;list[i]) { count += 1 } } return count } func EmployeeFilterIn(list []Employee, fn func(e *Employee) bool) []Employee { var newList []Employee for i, _ := range list { if fn(\u0026amp;list[i]) { newList = append(newList, list[i]) } } return newList } func EmployeeSumIf(list []Employee, fn func(e *Employee) int) int { var sum = 0 for i, _ := range list { sum += fn(\u0026amp;list[i]) } return sum } Here\u0026rsquo;s a brief explanation:\nEmployeeCountIf and EmployeeSumIf are used to count the number of employees or calculate the total based on a certain condition. They represent the semantics of Filter + Reduce. EmployeeFilterIn filters the employees based on a certain condition. It represents the semantics of Filter. Now we can have the following code:\n1) Count the number of employees over 40 years old:\n1 2 3 4 5 old := EmployeeCountIf(list, func(e *Employee) bool { return e.Age \u0026gt; 40 }) fmt.Printf(\u0026#34;Old people: %d\\n\u0026#34;, old) //Old people: 2 2) Count the number of employees with a salary greater than 6000:\n1 2 3 4 5 highPay := EmployeeCountIf(list, func(e *Employee) bool { return e.Salary \u0026gt;= 6000 }) fmt.Printf(\u0026#34;High Salary people: %d\\n\u0026#34;, highPay) //High Salary people: 4 3) List employees who have not taken any vacation:\n1 2 3 4 noVacation := EmployeeFilterIn(list, func(e *Employee) bool { return e.Vacation == 0 }) fmt.Printf(\u0026#34;People with no vacation: %v\\n\u0026#34;, noVacation) The Map-Reduce programming paradigm divides the computational task into Map and Reduce phases. Although writing single-machine code may not be faster than a simple for loop and may appear complex, in the era of cloud-native computing, we can leverage parallel computation and shared data access to improve computational efficiency. It is a powerful tool suitable for handling large-scale data and parallel computing scenarios, such as the original Google PageRank algorithm. The main purpose of learning it is to understand its mindset.\n","date":"2024-04-23T14:32:00Z","permalink":"https://example.com/p/go-program-pattern-04-map-reduce/","title":"Go Program Pattern 04: Map-Reduce"},{"content":"In the previous tutorial, I have already introduced that Go language, unlike object-oriented programming languages such as Java and PHP, does not support keywords like class to define classes. Instead, it uses the type keyword combined with basic types or structures to define the type system. Additionally, it does not support explicitly defining inheritance relationships between types using the extends keyword.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nStrictly speaking, Go language is not an object-oriented programming language, at least not the best choice for object-oriented programming (Java is the most established one). However, we can simulate object-oriented programming based on some features provided by Go.\nTo implement object-oriented programming, we must implement the three major features of object-oriented programming: encapsulation, inheritance, and polymorphism.\nInheritance Next is inheritance. Although Go does not directly provide syntax for inheritance, we can indirectly achieve similar functionality through composition. Composition means embedding one type into another type to build a new type structure.\nIn traditional object-oriented programming, explicitly defining inheritance relationships has two drawbacks: one is that it leads to increasingly complex class hierarchies, and the other is that it affects the extensibility of classes. Many software design patterns advocate using composition instead of inheritance to improve class extensibility.\nLet\u0026rsquo;s take an example. Suppose we want to create a UI component library. We have a Widget structure type with two properties, x and y, representing the length and width of the component.\nIf we want to define a class representing Label, we can do it like this:\n1 2 3 4 type Label struct { Widget text string } Here, Label inherits all the properties of Widget and adds a new property text. Similarly, we can define the Button and ListBox classes:\n1 2 3 4 5 6 7 8 type Button struct { Label } type ListBox struct { Widget text []string index int } Polymorphism First, we define two interfaces, Painter for painting and Clicker for clicking:\n1 2 3 4 5 6 type Painter interface { Paint() } type Clicker interface { Click() } Then, the components implement these interfaces:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func (label Label) Paint() { // display label fmt.Printf(\u0026#34;%p:Label.Paint(%q)\\n\u0026#34;, \u0026amp;label, label.text) } func (button Button) Paint() { // display button fmt.Printf(\u0026#34;Button.Paint(%q)\\n\u0026#34;, button.text) } func (button Button) Click() { // click button fmt.Printf(\u0026#34;Button.Click(%q)\\n\u0026#34;, button.text) } func (listBox ListBox) Paint() { // display listBox fmt.Printf(\u0026#34;ListBox.Paint(%q)\\n\u0026#34;, listBox.text) } Label implements Painter, and Button and ListBox implement both Painter and Clicker.\nAt the application level, we can use these components like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 label := Label{Widget{10, 10}, \u0026#34;State:\u0026#34;} button1 := Button{Label{Widget{10, 70}, \u0026#34;OK\u0026#34;}} button2 := NewButton(50, 70, \u0026#34;Cancel\u0026#34;) listBox := ListBox{Widget{10, 40}, []string{\u0026#34;AL\u0026#34;, \u0026#34;AK\u0026#34;, \u0026#34;AZ\u0026#34;, \u0026#34;AR\u0026#34;}, 0} for _, painter := range []Painter{label, listBox, button1, button2} { painter.Paint() } fmt.Println(\u0026#34;=========================================\u0026#34;) for _, clicker := range []Clicker{listBox, button1, button2} { clicker.Click() } fmt.Println(\u0026#34;=========================================\u0026#34;) for _, widget := range []interface{}{label, listBox, button1, button2} { widget.(Painter).Paint() if clicker, ok := widget.(Clicker); ok { clicker.Click() } } Go language is different from object-oriented programming languages like Java and PHP in that it does not provide keywords specifically for referencing parent class instances (such as super, parent, etc.). In Go language, the design philosophy is simplicity, without any unnecessary keywords. All calls are straightforward.\nSummary Let\u0026rsquo;s summarize briefly. In Go language, the concept of classes in traditional object-oriented programming is intentionally weakened, which is in line with Go\u0026rsquo;s philosophy of simplicity. The \u0026ldquo;classes\u0026rdquo; defined based on structures are just ordinary data types, similar to built-in data types. Built-in data types can also be transformed into \u0026ldquo;classes\u0026rdquo; that can contain custom member methods using the type keyword.\nAll methods associated with a data type collectively form the method set of that type. Like other object-oriented programming languages, methods within the same method set cannot have the same name. Additionally, if they belong to a structure type, their names cannot overlap with any field names in that type.\nReferences How to pass a \u0026lsquo;child\u0026rsquo; struct into a function accepting \u0026lsquo;parent\u0026rsquo; struct? Check if a struct has struct embedding at run time GO编程模式：委托和反转控制 ","date":"2024-04-23T14:30:00Z","permalink":"https://example.com/p/go-program-pattern-02-implementing-class-inheritance-and-method-overriding-through-composition/","title":"Go Program Pattern 02: Implementing Class Inheritance and Method Overriding through Composition"},{"content":"Go is not a fully object-oriented language, and some object-oriented patterns are not well-suited for it. However, over the years, Go has developed its own set of patterns. Today, I would like to introduce a common pattern: the Functional Options Pattern.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nWhat is the Functional Options Pattern? Go does not have constructors like other languages. Instead, it typically uses a New function to act as a constructor. However, when a structure has many fields that need to be initialized, there are multiple ways to do so. One preferred way is to use the Functional Options Pattern.\nThe Functional Options Pattern is a pattern for constructing structs in Go. It involves designing a set of expressive and flexible APIs to help configure and initialize the struct.\nThe Go Language Specification by Uber mentions this pattern:\nFunctional options are a pattern in which you declare an opaque Option type that records information in some internal structure. You accept these variable numbers of options and operate on the complete information recorded by the options on the internal structure.\nUse this pattern for optional parameters in constructors and other public APIs where you expect these parameters to be extended, especially when there are already three or more parameters on these functions.\nAn Example To better understand this pattern, let\u0026rsquo;s walk through an example.\nLet\u0026rsquo;s define a Server struct:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main type Server struct { host string port int } func New(host string, port int) *Server { return \u0026amp;Server{ host, port, } } func (s *Server) Start() error { return nil } How do we use it?\n1 2 3 4 5 6 func main() { svr := New(\u0026#34;localhost\u0026#34;, 1234) if err := svr.Start(); err != nil { log.Fatal(err) } } But what if we want to extend the configuration options for the Server? There are generally three approaches:\nDeclare a new constructor function for each different configuration option. Define a new Config struct to store the configuration information. Use the Functional Options Pattern. Approach 1: Declare a new constructor function for each different configuration option This approach involves defining dedicated constructor functions for different options. Let\u0026rsquo;s say we added two fields to the Server struct:\n1 2 3 4 5 6 type Server struct { host string port int timeout time.Duration maxConn int } Typically, host and port are required fields, while timeout and maxConn are optional. We can keep the original constructor function and assign default values to these two fields:\n1 2 3 4 5 6 7 8 func New(host string, port int) *Server { return \u0026amp;Server{ host, port, time.Minute, 100, } } Then, we can provide two additional constructor functions for timeout and maxConn:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func NewWithTimeout(host string, port int, timeout time.Duration) *Server { return \u0026amp;Server{ host, port, timeout, 100, } } func NewWithTimeoutAndMaxConn(host string, port int, timeout time.Duration, maxConn int) *Server { return \u0026amp;Server{ host, port, timeout, maxConn, } } This approach works well for configurations that are unlikely to change frequently. Otherwise, you would need to create new constructor functions every time you need to add a new configuration. This approach is used in the Go standard library, such as the Dial and DialTimeout functions in the net package:\n1 2 func Dial(network, address string) (Conn, error) func DialTimeout(network, address string, timeout time.Duration) (Conn, error) Approach 2: Use a dedicated configuration struct This approach is also common, especially when there are many configuration options. Typically, you create a Config struct that contains all the configuration options for the Server. This approach allows for easy extension without breaking the API of the Server, even when adding more configuration options in the future.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type Server struct { cfg Config } type Config struct { Host string Port int Timeout time.Duration MaxConn int } func New(cfg Config) *Server { return \u0026amp;Server{ cfg, } } When using this approach, you need to construct a Config instance first, which brings us back to the original problem of configuring the Server. If you modify the fields in Config, you may need to define a constructor function for Config if the fields are changed to private.\nApproach 3: Use the Functional Options Pattern A better solution is to use the Functional Options Pattern.\nIn this pattern, we define an Option function type:\n1 type Option func(*Server) The Option type is a function type that takes a *Server parameter. Then, the constructor function for Server accepts a variable number of Option types as parameters:\n1 2 3 4 5 6 7 func New(options ...Option) *Server { svr := \u0026amp;Server{} for _, f := range options { f(svr) } return svr } How do the options work? We need to define a series of related functions that return Option:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 func WithHost(host string) Option { return func(s *Server) { s.host = host } } func WithPort(port int) Option { return func(s *Server) { s.port = port } } func WithTimeout(timeout time.Duration) Option { return func(s *Server) { s.timeout = timeout } } func WithMaxConn(maxConn int) Option { return func(s *Server) { s.maxConn = maxConn } } To use this pattern, the client code would look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package main import ( \u0026#34;log\u0026#34; \u0026#34;server\u0026#34; ) func main() { svr := New( WithHost(\u0026#34;localhost\u0026#34;), WithPort(8080), WithTimeout(time.Minute), WithMaxConn(120), ) if err := svr.Start(); err != nil { log.Fatal(err) } } Adding new options in the future only requires adding corresponding WithXXX functions.\nThis pattern is widely used in third-party libraries, such as github.com/gocolly/colly:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type Collector struct { // ... } func NewCollector(options ...CollectorOption) *Collector // Defines a series of CollectorOptions type CollectorOption struct { // ... } func AllowURLRevisit() CollectorOption func AllowedDomains(domains ...string) CollectorOption ... However, when Uber\u0026rsquo;s Go Programming Style Guide mentions this pattern, it suggests defining an Option interface instead of an Option function type. This Option interface has an unexported method, and the options are recorded in an unexported options struct.\nCan you understand Uber\u0026rsquo;s example?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 type options struct { cache bool logger *zap.Logger } type Option interface { apply(*options) } type cacheOption bool func (c cacheOption) apply(opts *options) { opts.cache = bool(c) } func WithCache(c bool) Option { return cacheOption(c) } type loggerOption struct { Log *zap.Logger } func (l loggerOption) apply(opts *options) { opts.logger = l.Log } func WithLogger(log *zap.Logger) Option { return loggerOption{Log: log} } // Open creates a connection. func Open( addr string, opts ...Option, ) (*Connection, error) { options := options{ cache: defaultCache, logger: zap.NewNop(), } for _, o := range opts { o.apply(\u0026amp;options) } // ... } Summary In real-world projects, when dealing with a large number of options or options from different sources (e.g., from files or environment variables), consider using the Functional Options Pattern.\nNote that in actual work, we should not rigidly apply the pattern as described above. For example, in Uber\u0026rsquo;s example, the Open function does not only accept a variable number of Option parameters because the addr parameter is required. Therefore, the Functional Options Pattern is more suitable for cases with many configurations and optional parameters.\nReferences:\nhttps://golang.cafe/blog/golang-functional-options-pattern.html https://github.com/uber-go/guide/blob/master/style.md#functional-options If you found my article enjoyable, feel free to follow me and give it a 👏. Your support would be greatly appreciated.\n","date":"2024-04-23T14:29:00Z","permalink":"https://example.com/p/go-program-pattern-01-functional-options-pattern/","title":"Go Program Pattern 01: Functional Options Pattern"},{"content":" This article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nGo language has three types of pointers. In the normal development process, we only encounter the ordinary pointer. However, in the low-level source code of Go language, there are a lot of operations involving three types of pointer conversion and manipulation. Let\u0026rsquo;s clarify these points first.\nIn the C language, pointers are crucial. Although pointers make operations highly flexible and efficient, there are many security risks associated with accessing memory through pointer operations, such as accessing memory out of bounds and compromising the atomicity of types in the type system. Here are some examples of incorrect usage:\n1 2 3 4 5 6 7 8 9 10 // Example 1 int arr[2]; *(arr+2) = 1;\t// Accessing memory address out of bounds // Example 2 int a = 4;\tint* ap = \u0026amp;a;\t// Taking the starting address of variable a (4 bytes) *(short*)ap = 2; // Modifying the first 2 bytes of the 4-byte variable a directly through type casting, thus breaking the atomicity of the int variable // The code in Example 2 may occur in certain scenarios, but it has portability issues on machines with different endianness The reason for these security risks in the C language is that it supports pointer operations and pointer type conversions. Therefore, in Go language, the most commonly used ordinary pointers, which have types, have eliminated pointer arithmetic and type conversion operations to ensure type safety. Here\u0026rsquo;s an example:\n1 2 3 4 5 var a int32 = 10 var ap *int32 = \u0026amp;a\t// Ordinary pointer with type ap++ // Illegal, pointer arithmetic is not allowed p := (*int16)(ap)\t// Illegal, *int32 cannot be directly converted to *int16 This ensures that pointers always point to valid addresses with allocated memory and preserves type independence and atomicity.\nIn addition to ordinary pointers, Go language also retains two other types of pointers that allow bypassing the type system and achieving the same level of memory manipulation as in C language. The other two types of pointers are:\nunsafe.Pointer. uintptr.\nTo understand these two, we need to establish a concept: a pointer is essentially a number that stores a memory address. The addressing space is 32 bits for a 32-bit machine and 64 bits for a 64-bit machine, so the size of a pointer is equal to the number of bits in the machine. uintptr is straightforward; it is simply a number that stores a memory address. It is equivalent to uint32 on a 32-bit machine and uint64 on a 64-bit machine. Since it is a number, it naturally supports arithmetic operations, which allows it to represent any memory location. However, the problem is that a data cannot be operated solely based on its memory address; you also need to know its size. In other words, we cannot manipulate data solely based on a uintptr pointer. On the other hand, an ordinary typed pointer not only provides the address but also informs the compiler about the size of the data pointed to. For example, *int32 and *int64 pointers tell the compiler that they operate on 4-byte and 8-byte data, respectively.\nNow that we have explained ordinary pointers and uintptr pointers in Go language, what is this additional unsafe.Pointer compared to C language?\nunsafe.Pointer is a generic pointer that, like uintptr, only keeps the memory address without concerning itself with the type. However, the difference between unsafe.Pointer and uintptr is that the former refers to an object that will be referenced by the garbage collector (GC), so it will not be collected as garbage by the GC. In contrast, the latter only represents the memory address as a number, which means that if a data address is saved by uintptr, it will be mercilessly collected by the garbage collector.\nSummary of the three types of pointers in Go language:\nOrdinary pointer: Does not support pointer arithmetic, saves the address and type information, and the data it points to will not be garbage collected by the GC. unsafe.Pointer: Does not support pointer arithmetic, saves the address but not the type information, and the data it points to will not be garbage collected by the GC. uintptr: Supports address arithmetic, saves the address but not the type information, and the data it points to will be garbage collected by the GC. In practical usage, uintptr cannot be directly converted to an ordinary pointer, and both must be first converted to unsafe.Pointer as an intermediate step before further conversion.\nHere\u0026rsquo;s a simple example:\n1 2 3 4 5 6 7 8 type Foo struct{ a int32 b int32 } foo := \u0026amp;Foo{} bp := uintptr(unsafe.Pointer(foo)) + 4\t// Add 4 to the address of foo to locate foo.b *(*int32)(unsafe.Pointer(bp)) = 1\t// Convert to *int32 ordinary pointer and modify the value fmt.Println(foo.b)\t// foo.b = 1 ","date":"2024-04-23T14:25:00Z","permalink":"https://example.com/p/decryption-go-understand-go-language-pointer/","title":"Decryption go: understand go language pointer"},{"content":"\nHandling time is a common task for programmers. In Go, the standard library time provides the necessary capabilities.\nThis article will introduce some important functions and methods in the time package, hoping to help those who often need to deal with time-related issues in Go.\nHandle Time Zones In programming, we often encounter the issue of an eight-hour time difference. This is caused by differences in time zones. To better handle them, we need to understand several time definition standards.\nGMT (Greenwich Mean Time) is based on the Earth\u0026rsquo;s rotation and revolution to calculate time. It defines noon as the time when the sun passes through the Royal Greenwich Observatory in the suburbs of London, UK. GMT was the former world standard time.\nUTC (Coordinated Universal Time) is more precise than GMT, calculated based on atomic clocks. In situations where precision to the second is not required, UTC can be considered equivalent to GMT. UTC is the current world standard time.\nFrom the Prime Meridian at Greenwich, going east is positive, going west is negative. The globe is divided into 24 standard time zones, with neighboring time zones differing by one hour.\n1 2 3 4 5 6 7 8 9 10 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { fmt.Println(time.Now()) } In mainland China, the standard time used is in the GMT+8 time zone, known as China Standard Time (CST).\n1 2 $ go run main.go 2022-07-17 16:37:31.186043 +0800 CST m=+0.000066647 This is the result under the default time zone, and +0800 CST is indicated in the time.Now() printout.\nSuppose we are in the Los Angeles time zone in the United States, what result do we get?\n1 2 $ TZ=\u0026#34;America/Los_Angeles\u0026#34; go run main.go 2022-07-17 01:39:12.391505 -0700 PDT m=+0.000069514 As seen, the result at this time is -0700 PDT, which is Pacific Daylight Time. Due to the time zone difference, the results of the two executions differ by 15 hours.\nNote that when using Docker containers, the system\u0026rsquo;s default time zone is UTC time (0 time zone), which is eight hours behind Beijing time as we need, leading to the classic scenario of the eight-hour time difference problem.\nStrategies for dealing with time zone issues can be found in detail in the loading logic of the initLocal() function in src/time/zoneinfo_unix.go. For example, you can solve it by specifying the environment variable TZ or modifying the /etc/localtime file.\nBecause time zone issues are very important, they are discussed in the first part of the article. Let\u0026rsquo;s now move on to the usage of the time package.\nTime Instant time.Time The core object in the time package is the time.Time struct. Its definition, used to represent a specific moment in time, is as follows:\n1 2 3 4 5 6 7 8 type Time struct { // wall and ext encode the wall time seconds, // wall time nanoseconds, and optional monotonic // clock reading in nanoseconds. wall uint64 ext int64 loc *Location } In computer time handling, two types of clocks are mainly involved:\nWall clock, also known as clock time, used to represent specific dates and times. Monotonic clocks, which always guarantee that time moves forward without the issue of wall clock rollback, making them suitable for measuring durations. The wall and ext fields are used to record wall clock and monotonic clock times, with nanosecond precision. The bits of these fields are associated with specific information such as years, months, days, hours, minutes, and seconds.\nThe loc field records the time zone location. When loc is nil, it defaults to UTC time.\nBecause time.Time is used to represent time instants with nanosecond precision, it is typically stored and passed as a value in programs, rather than a pointer.\nThat is, in time variables or struct fields, we should use time.Time rather than *time.Time.\nGetting time.Time We can get the current local time using the Now function:\n1 func Now() Time {} Or, using the Date function, we can get a specified time based on the year, month, day, and other parameters, along with the time zone:\n1 func Date(year int, month Month, day, hour, min, sec, nsec int, loc *Location) Time {} Converting Timestamps In the computer world, UTC time 0 on January 1, 1970, is considered Unix time 0. To convert a time instant into a Unix timestamp, we calculate the number of seconds, microseconds, etc., elapsed from Unix time 0 to the specified instant.\n1 2 3 4 func (t Time) Unix() int64 // Seconds since Unix time 0 func (t Time) UnixMicro() int64 // Microseconds since Unix time 0 func (t Time) UnixMilli() int64 // Milliseconds since Unix time 0 func (t Time) UnixNano() int64 // Nanoseconds since Unix time 0 Getting Basic Fields 1 2 3 4 5 6 7 8 9 10 11 12 13 t := time.Now() fmt.Println(t.Date()) // 2022 July 17 fmt.Println(t.Year()) // 2022 fmt.Println(t.Month()) // July fmt.Println(t.ISOWeek()) // 2022 28 fmt.Println(t.Clock()) // 22 21 56 fmt.Println(t.Day()) // 17 fmt.Println(t.Weekday()) // Sunday fmt.Println(t.Hour()) // 22 fmt.Println(t.Minute()) // 21 fmt.Println(t.Second()) // 56 fmt.Println(t.Nanosecond()) // 494313000 fmt.Println(t.YearDay()) // 198 Duration time.Duration time.Duration represents the time elapsed between two time.Time instants. It uses an int64 to represent the count of nanoseconds, allowing for approximately 290 years of representation.\n1 2 3 4 // A Duration represents the elapsed time between two instants // as an int64 nanosecond count. The representation limits the // largest representable duration to approximately 290 years. type Duration int64 In Go, time.Duration is simply a number in nanoseconds. If a duration is equal to 1000000000, it represents 1 second, or 1000 milliseconds, or 1000000 microseconds, or 1000000000 nanoseconds.\nFor example, the duration between two time instants separated by 1 hour can be calculated as:\n1 1*60*60*1000*1000*1000 The time package defines constant values for these durations:\n1 2 3 4 5 6 7 8 9 10 const ( Nanosecond Duration = 1 Microsecond = 1000 * Nanosecond Millisecond = 1000 * Microsecond Second = 1000 * Millisecond Minute = 60 * Second Hour = 60 * Minute ) Additionally, time.Duration provides methods to get values at various time granularities:\n1 2 3 4 5 6 func (d Duration) Nanoseconds() int64 // Nanoseconds func (d Duration) Microseconds() int64 // Microseconds func (d Duration) Milliseconds() int64 // Milliseconds func (d Duration) Seconds() float64 // Seconds func (d Duration) Minutes() float64 // Minutes func (d Duration) Hours() float64 // Hours Time Calculation After learning about time instants and durations, let\u0026rsquo;s see how to perform time calculations.\n1 func (t Time) Add(d Duration) Time {} Add adds or subtracts (positive d means addition, negative d means subtraction) a time.Duration to a time.Time. We can add or subtract durations of specific nanosecond levels to a specific instant in time. 1 func (t Time) Sub(u Time) Duration {} Sub returns the duration between two time instants. 1 func (t Time) AddDate(years int, months int, days int) Time {} AddDate adds or subtracts values based on the year, month, and day dimensions to a time.Time. Of course, calculating based on the current time instant time.Now() is the most common requirement. Therefore, the time package also provides the following convenient time calculation functions:\n1 func Since(t Time) Duration {} Since is a shortcut for time.Now().Sub(t).\n1 func Until(t Time) Duration {} Until is a shortcut for t.Sub(time.Now()).\nUsage Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 t := time.Now() fmt.Println(t) // 2022-07-17 22:41:06.001567 +0800 CST m=+0.000057466 // Adding 1 hour to the time fmt.Println(t.Add(time.Hour * 1)) // 2022-07-17 23:41:06.001567 +0800 CST m=+3600.000057466 // Adding 15 minutes fmt.Println(t.Add(time.Minute * 15)) // 2022-07-17 22:56:06.001567 +0800 CST m=+900.000057466 // Adding 10 seconds fmt.Println(t.Add(time.Second * 10)) // 2022-07-17 22:41:16.001567 +0800 CST m=+10.000057466 // Subtracting 1 hour fmt.Println(t.Add(-time.Hour * 1)) // 2022-07-17 21:41:06.001567 +0800 CST m=-3599.999942534 // Subtracting 15 minutes fmt.Println(t.Add(-time.Minute * 15)) // 2022-07-17 22:26:06.001567 +0800 CST m=-899.999942534 // Subtracting 10 seconds fmt.Println(t.Add(-time.Second * 10)) // 2022-07-17 22:40:56.001567 +0800 CST m=-9.999942534 time.Sleep(time.Second * 5) t2 := time.Now() // Calculating the duration from t to t2 fmt.Println(t2.Sub(t)) // 5.004318874s // Time after 1 year t3 := t2.AddDate(1, 0, 0) // Calculating the duration from t to the current time fmt.Println(time.Since(t)) // 5.004442316s // Calculating the duration from now to next year fmt.Println(time.Until(t3)) // 8759h59m59.999864s Formatting Time In other languages, a universal time template is typically used to format time. For example, in Python, %Y represents year, %m represents month, %d represents day, and so on.\nHowever, Go is different. It uses a fixed time (it\u0026rsquo;s important to note that using other times is not allowed) as the layout template, and this fixed time is the birth time of the Go language.\n1 Mon Jan 2 15:04:05 MST 2006 Formatting time involves two conversion functions:\n1 func Parse(layout, value string) (Time, error) {} Parse converts a time string to a time.Time object based on the layout it can correspond to. 1 func (t Time) Format(layout string) string {} Format converts a time.Time object to a time string based on the given layout. Example 1 2 3 4 5 6 7 8 const ( layoutISO = \u0026#34;2006-01-02\u0026#34; layoutUS = \u0026#34;January 2, 2006\u0026#34; ) date := \u0026#34;2012-08-09\u0026#34; t, _ := time.Parse(layoutISO, date) fmt.Println(t) // 2012-08-09 00:00:00 +0000 UTC fmt.Println(t.Format(layoutUS)) // August 9, 2012 In the time package, Go provides some predefined layout template constants that can be directly used.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 const ( Layout = \u0026#34;01/02 03:04:05PM \u0026#39;06 -0700\u0026#34; // The reference time, in numerical order. ANSIC = \u0026#34;Mon Jan _2 15:04:05 2006\u0026#34; UnixDate = \u0026#34;Mon Jan _2 15:04:05 MST 2006\u0026#34; RubyDate = \u0026#34;Mon Jan 02 15:04:05 -0700 2006\u0026#34; RFC822 = \u0026#34;02 Jan 06 15:04 MST\u0026#34; RFC822Z = \u0026#34;02 Jan 06 15:04 -0700\u0026#34; // RFC822 with numeric zone RFC850 = \u0026#34;Monday, 02-Jan-06 15:04:05 MST\u0026#34; RFC1123 = \u0026#34;Mon, 02 Jan 2006 15:04:05 MST\u0026#34; RFC1123Z = \u0026#34;Mon, 02 Jan 2006 15:04:05 -0700\u0026#34; // RFC1123 with numeric zone RFC3339 = \u0026#34;2006-01-02T15:04:05Z07:00\u0026#34; RFC3339Nano = \u0026#34;2006-01-02T15:04:05.999999999Z07:00\u0026#34; Kitchen = \u0026#34;3:04PM\u0026#34; // Handy time stamps. Stamp = \u0026#34;Jan _2 15:04:05\u0026#34; StampMilli = \u0026#34;Jan _2 15:04:05.000\u0026#34; StampMicro = \u0026#34;Jan _2 15:04:05.000000\u0026#34; StampNano = \u0026#34;Jan _2 15:04:05.000000000\u0026#34; ) Here\u0026rsquo;s a table of optional layout parameters:\n1 2 3 4 5 6 7 8 9 10 11 12 13 Year 06/2006 Month 01/1/Jan/January Day 02/2/_2 Weekday Mon/Monday Hour 03/3/15 Minute 04/4 Second 05/5 Milliseconds .000/.999 Microseconds .000000/.999999 Nanoseconds .000000000/.999999999 am/pm PM/pm Timezone MST Timezone offset -0700/-07/-07:00/Z0700/Z07:00 Timezone Conversion At the beginning of the article, we discussed timezone issues. If in your code, you need to get the result of the same time.Time in different time zones, you can use its In method.\n1 func (t Time) In(loc *Location) Time {} It\u0026rsquo;s straightforward to use. Let\u0026rsquo;s see an example code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 now := time.Now() fmt.Println(now) // 2022-07-18 21:19:59.9636 +0800 CST m=+0.000069242 loc, _ := time.LoadLocation(\u0026#34;UTC\u0026#34;) fmt.Println(now.In(loc)) // 2022-07-18 13:19:59.9636 +0000 UTC loc, _ = time.LoadLocation(\u0026#34;Europe/Berlin\u0026#34;) fmt.Println(now.In(loc)) // 2022-07-18 15:19:59.9636 +0200 CEST loc, _ = time.LoadLocation(\u0026#34;America/New_York\u0026#34;) fmt.Println(now.In(loc)) // 2022-07-18 09:19:59.9636 -0400 EDT loc, _ = time.LoadLocation(\u0026#34;Asia/Dubai\u0026#34;) fmt.Println(now.In(loc)) // 2022-07-18 17:19:59.9636 +0400 +04 Conclusion In general, the functions and methods provided by the time package for time processing meet our usage needs.\nInterestingly, Go\u0026rsquo;s time formatting conversion must adopt Go\u0026rsquo;s birth time. It\u0026rsquo;s quite self-centered.\n","date":"2024-04-19T18:43:00Z","permalink":"https://example.com/p/learn-how-to-handle-time-in-golang/","title":"Learn How to Handle Time In Golang"},{"content":" This article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nThe inaugural v2 version of the standard library in Go, hails from the esteemed math/rand/v2 repository. It is set to make its grand debut with the official release of Go1.22, poised to serve as a reliable and production-ready resource.\nReasons The original math/rand library in the standard package had numerous deficiencies and areas for improvement. These included outdated generators, slow algorithms (performance), and unfortunate conflicts with crypto/rand.Read, among other issues. There is a plan in place to upgrade the v2 versions of standard libraries. Starting with math allows for the accumulation of experience and resolving tooling ecosystem challenges (such as support from tools like gopls and goimports for v2 packages). Subsequent iterations can then address higher-risk packages, like sync/v2 or encoding/json/v2. Go1 requires compatibility guarantees, making it impractical to directly modify the original library. The issues with math/rand are also more prominent and evident. change list Removed Rand.Read and the top-level Read function. Removed Source.Seed, Rand.Seed, and the top-level Seed function (meaning that top-level functions like Int will always use random seeding). Removed Source64, as Source now provides the Uint64 method, making the original methods unnecessary. Utilized a more direct implementation for Float32 and Float64. For example, in the case of Float64, the original implementation used float64(r.Int63()) / (1\u0026lt;\u0026lt;63). However, this had a problem of occasionally rounding to 1.0, while Float64 should never round. The improvement involves changing it to float64(r.Int63n(1\u0026lt;\u0026lt;53)) / (1\u0026lt;\u0026lt;53), which avoids the rounding issue. Implemented Rand.Perm using Rand.Shuffle. This improves efficiency and ensures only one implementation. Renamed Int31, Int31n, Int63, and Int64n to Int32, Int32n, Int64, and Int64n, respectively. These names were unnecessary and confusing. Added Uint32, Uint32n, Uint64, Uint64n, Uint, and Uintn as top-level functions and methods on Rand. Utilized Lemire\u0026rsquo;s algorithm in Intn, Uintn, Int32n, Uint32n, Int64n, and Uint64n, resulting in improved performance. Introduced a new implementation of Source called PCG-DXSM, including related APIs like NewPCG. Removed the Mitchell \u0026amp; Reeds LFSR generator and NewSource. example Read \u0026amp; Seed The functions Read and Seed have been removed. It is recommended to use crypto/rand\u0026lsquo;s Read function instead.\n1 2 3 4 5 6 7 8 9 10 11 12 13 import ( \u0026#34;crypto/rand\u0026#34; \u0026#34;fmt\u0026#34; ) func main() { b := make([]byte, 3) _, err := rand.Read(b) if err != nil { panic(err) } fmt.Printf(\u0026#34;hxzhouh: %v\\n\u0026#34;, b) } output：\n1 hxzhouh: [48 71 122] For the Seed function, it is advised to call New(NewSource(seed)) in order to reinitialize the random number generator.\ninternal The functions N, IntN, and UintN now utilize a novel implementation algorithm. Interested individuals are encouraged to allocate extra time to examine it in detail:A fast alternative to the modulo reduction\nThe functions Intn, Int31, Int31n, Int63, and Int64n have been renamed as follows: IntN, Int32, Int32N, Int64, and Int64N, respectively.\nAdditionally, new functions Uint32, Uint32N, Uint64, Uint64N, Uint, and UintN have been introduced to generate random unsigned integers. They have also been added as corresponding functions within the Rand structure.\nThe newly added function N generates random numbers of arbitrary integer types. This function is implemented using generics, and the following integer types are its type parameters:\nint int8 int16 int32 int64 Summary: Today, we have shared and further described the new math/rand/v2 library, highlighting key changes including performance optimization (algorithm rewrite), standardization, and additions of new random generators.\nGiven the substantial amount of information covered, we have selected and presented only the aspects that are essential for understanding and using the library. However, for those who are interested in delving deeper, it is recommended to refer to the full documentation ofhttps://pkg.go.dev/math/rand/v2@master\n","date":"2024-04-18T17:05:00Z","permalink":"https://example.com/p/go1.22-add-a-new-math-lib/","title":"Go1.22 Add a new math lib"},{"content":" This article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nIn the previous article, we used defer to recover from panics. In the practical work of a gopher, defer acts like a loyal and reliable teammate, silently helping us with the clean-up work behind the scenes. For example:\n1 2 3 4 5 6 7 8 9 10 wg.Add(goroutines) for i := 0; i \u0026lt; goroutines; i++ { go func() { defer wg.Done() for j := 0; j \u0026lt; count/goroutines; j++ { atomic.AddInt64(\u0026amp;sum, 1) } }() } wg.Wait() defer is used to release locks or any other resources.\nIn Go, defer can only be used inside functions and methods.\nThe defer keyword must be followed by a function or method, which are referred to as deferred functions.\ndefer registers these functions into a stack data structure specific to the goroutine in which it is executed. The deferred functions are then scheduled to be executed in a last-in, first-out (LIFO) order before the function containing the defer statement exits.\nRegardless of whether the function reaches the end of its body and returns, explicitly calls return in an error handling branch, or encounters a panic, the functions stored in the deferred function stack will be scheduled for execution. Thus, deferred functions provide a convenient way to perform clean-up tasks for a function in any scenario.\nSeveral Use Cases for defer Capturing panics: Since deferred functions are always executed in any scenario, we can handle exceptions within defer (although it is not recommended to use panic for general errors unless necessary). Resource release: defer allows for graceful resource release, such as file descriptors or locks. Delayed execution: defer can be used to record the execution time of a function, for example: 1 2 3 go func(s time.Time) { fmt.Println(time.Now().Sub(s)) }(time.Now()) Performance Overhead of defer defer makes resource release (like file descriptors or locks) more elegant and less error-prone. However, in performance-sensitive programs, Gophers must be aware of and consider the performance burden introduced by defer.\nIn the following benchmark test, we can observe the performance difference between a version with defer and a version without defer:\n1 2 3 4 5 6 7 8 9 10 11 12 hxzhouh  atomic  ➜ ( main  1)  ♥ 20:16  go test -bench=BenchmarkFooWithDefer 10000000 goos: darwin goarch: arm64 pkg: github.com/hxzhouh/go-example/atomic BenchmarkFooWithDefer-10 189423524 6.353 ns/op PASS ok github.com/hxzhouh/go-example/atomic 3.631s hxzhouh  atomic  ➜ ( main  1)  ♥ 21:05  go test -bench=BenchmarkFooWithoutDefer BenchmarkFooWithoutDefer-10 273232389 4.397 ns/op PASS ok github.com/hxzhouh/go-example/atomic 2.875s In this test, the non-deferred version is approximately 7 times faster than the version with defer in Go 1.12. After optimization in versions 1.13 and 1.14, the performance of defer has significantly improved. On my computer, the non-deferred version still has a performance advantage of about 50%.\nConclusion In most cases, our programs are not highly sensitive to performance. I recommend using defer whenever possible. However, it is important to understand how defer works, as well as a few things to avoid.\n","date":"2024-04-18T17:02:00Z","permalink":"https://example.com/p/go-defermakes-the-function-simpler-and-more-robust./","title":"Go defer：makes the function simpler and more robust."},{"content":"\nDuring KubeCon EU 2024, CNCF launched its first Cloud-Native AI Whitepaper. This article provides an in-depth analysis of the content of this whitepaper.\nIn March 2024, during KubeCon EU, the Cloud-Native Computing Foundation (CNCF) released its first detailed whitepaper on Cloud-Native Artificial Intelligence (CNAI) 1. This report extensively explores the current state, challenges, and future development directions of integrating cloud-native technologies with artificial intelligence. This article will delve into the core content of this whitepaper.\nWhat is Cloud-Native AI? Cloud-Native AI refers to building and deploying artificial intelligence applications and workloads using cloud-native technology principles. This includes leveraging microservices, containerization, declarative APIs, and continuous integration/continuous deployment (CI/CD) among other cloud-native technologies to enhance AI applications\u0026rsquo; scalability, reusability, and operability.\nThe following diagram illustrates the architecture of Cloud-Native AI, redrawn based on the whitepaper.\nRelationship between Cloud-Native AI and Cloud-Native Technologies Cloud-native technologies provide a flexible, scalable platform that makes the development and operation of AI applications more efficient. Through containerization and microservices architecture, developers can iterate and deploy AI models quickly while ensuring high availability and scalability of the system. Kuuch as resource scheduling, automatic scaling, and service discovery.\nThe whitepaper provides two examples to illustrate the relationship between Cloud-Native AI and cloud-native technologies, namely running AI on cloud-native infrastructure:\nHugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure2 OpenAI Scaling Kubernetes to 7,500 nodes3 Challenges of Cloud-Native AI Despite providing a solid foundation for AI applications, there are still challenges when integrating AI workloads with cloud-native platforms. These challenges include data preparation complexity, model training resource requirements, and maintaining model security and isolation in multi-tenant environments. Additionally, resource management and scheduling in cloud-native environments are crucial for large-scale AI applications and need further optimization to support efficient model training and inference.\nDevelopment Path of Cloud-Native AI The whitepaper proposes several development paths for Cloud-Native AI, including improving resource scheduling algorithms to better support AI workloads, developing new service mesh technologies to enhance the performance and security of AI applications, and promoting innovation and standardization of Cloud-Native AI technology through open-source projects and community collaboration.\nCloud-Native AI Technology Landscape Cloud-Native AI involves various technologies, ranging from containers and microservices to service mesh and serverless computing. Kubernetes plays a central role in deploying and managing AI applications, while service mesh technologies such as Istio and Envoy provide robust traffic management and security features. Additionally, monitoring tools like Prometheus and Grafana are crucial for maintaining the performance and reliability of AI applications.\nBelow is the Cloud-Native AI landscape diagram provided in the whitepaper.\nKubernetes Volcano Armada Kuberay Nvidia NeMo Yunikorn Kueue Flame Distributed Training Kubeflow Training Operator Pytorch DDP TensorFlow Distributed Open MPI DeepSpeed Megatron Horovod Apla … ML Serving Kserve Seldon VLLM TGT Skypilot … CI/CD — Delivery Kubeflow Pipelines Mlflow TFX BentoML MLRun … Data Science Jupyter Kubeflow Notebooks PyTorch TensorFlow Apache Zeppelin Workload Observability Prometheus Influxdb Grafana Weights and Biases (wandb) OpenTelemetry … AutoML Hyperopt Optuna Kubeflow Katib NNI … Governance \u0026amp; Policy Kyverno Kyverno-JSON OPA/Gatekeeper StackRox Minder … Data Architecture ClickHouse Apache Pinot Apache Druid Cassandra ScyllaDB Hadoop HDFS Apache HBase Presto Trino Apache Spark Apache Flink Kafka Pulsar Fluid Memcached Redis Alluxio Apache Superset … Vector Databases Chroma Weaviate Quadrant Pinecone Extensions Redis Postgres SQL ElasticSearch … Model/LLM Observability • Trulens Langfuse Deepchecks OpenLLMetry … Conclusion Finally, the following key points are summarized:\nRole of Open Source Community: The whitepaper indicates the role of the open-source community in advancing Cloud-Native AI, including accelerating innovation and reducing costs through open-source projects and extensive collaboration. Importance of Cloud-Native Technologies: Cloud-Native AI, built according to cloud-native principles, emphasizes the importance of repeatability and scalability. Cloud-native technologies provide an efficient development and operation environment for AI applications, especially in resource scheduling and service scalability. Existing Challenges: Despite bringing many advantages, Cloud-Native AI still faces challenges in data preparation, model training resource requirements, and model security and isolation. Future Development Directions: The whitepaper proposes development paths including optimizing resource scheduling algorithms to support AI workloads, developing new service mesh technologies to enhance performance and security, and promoting technology innovation and standardization through open-source projects and community collaboration. Key Technological Components: Key technologies involved in Cloud-Native AI include containers, microservices, service mesh, and serverless computing, among others. Kubernetes plays a central role in deploying and managing AI applications, while service mesh technologies like Istio and Envoy provide necessary traffic management and security. For more details, please download the Cloud-Native AI whitepaper 4.\nReference Links Whitepaper:\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI Scaling Kubernetes to 7,500 nodes:\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCloud-Native AI Whitepaper: \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-04-17T10:14:00Z","permalink":"https://example.com/p/a-deep-dive-into-cncfs-cloud-native-ai-whitepaper/","title":"A Deep Dive into CNCF’s Cloud-Native AI Whitepaper"},{"content":"\nNote: Non-members can read the full story in this link.\nIntroduction In the previous article, we verified the context switch overhead of Linux processes and threads experimentally, which was approximately between 3-5 microseconds. This overhead is not significant, but for massively concurrent internet servers and typical computer programs, the characteristics are as follows:\nHigh concurrency: Thousands to tens of thousands of user requests need to be processed per second. Short cycles: The processing time per user should be as short as possible, often in the millisecond range. High network I/O: Often requires network I/O from other machines, such as Redis, MySQL, etc. Low computation: General CPU-intensive operations are not frequent. Even with a context switch overhead of 3-5 microseconds, it can still appear somewhat performance-degrading if the context switch volume is particularly high. For example, the Apache web server, which was the software product under this model, suffered from this. (In fact, when Linux operating system was designed, its goal was to be a general-purpose operating system rather than specifically designed for high-concurrency server-side applications.)\nTo avoid frequent context switches, there is another asynchronous non-blocking development model. That is to use a process or thread to handle a large number of user requests and then improve performance through IO multiplexing (processes or threads do not block, saving the overhead of context switches). Nginx and Node.js are typical representatives of this model. Frankly speaking, in terms of program execution efficiency, this model is the most machine-friendly, with the highest efficiency (better than the coroutine development model mentioned below). Therefore, Nginx has replaced Apache as the preferred web server. However, the problem with this programming model lies in its unfriendliness to development, which is overly mechanized and deviates from the original intention of abstracting the concept of processes. Normal linear thinking of humans is disrupted, and application layer developers are forced to write code with non-human-like thinking, making code debugging extremely difficult.\nSo, some smart heads continued to brainstorm at the application layer and designed \u0026ldquo;threads\u0026rdquo; that do not require process/thread context switching, called coroutines. Using coroutines to handle high-concurrency application scenarios can not only meet the original intention of processes but also allow developers to use normal linear thinking to handle their business, while also eliminating the expensive overhead of process/thread context switches. Therefore, it can be said that coroutines are a good patch for the process model in the scenario of processing massive requests on Linux.\nWith the background introduced, what I want to say is that although coroutine encapsulation is lightweight, it still incurs some additional costs. So, let\u0026rsquo;s take a look at how small these additional costs are.\nCoroutine Overhead Test This article is based on go 1.22.1.\n1. Coroutine Context Switch CPU Overhead\nThe test process involves continuously yielding the CPU between coroutines. The core code is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;time\u0026#34; ) func cal() { for i := 0; i \u0026lt; 1000000; i++ { runtime.Gosched() } } func main() { runtime.GOMAXPROCS(1) currentTime := time.Now() fmt.Println(currentTime) go cal() for i := 0; i \u0026lt; 1000000; i++ { runtime.Gosched() } fmt.Println(time.Now().Sub(currentTime) / 2000000) } Compilation and execution\n1 2 3 4 ➜ trace git:(main) ✗ go run main.go 2024-03-20 19:52:24.772579 +0800 CST m=+0.000114834 54ns ➜ trace git:(main) ✗ The average overhead of each coroutine switch is 54ns, which is approximately 1/70 of the context switch overhead measured in the previous article, about 3.5 microseconds, and is approximately 70 times lower than the overhead caused by system calls.\nCoroutine Memory Overhead\nIn terms of space, when coroutines are initialized and created, a stack of 2KB is allocated for them. The stack of threads is much larger than this number, which can be checked through the ulimit command, usually in several megabytes. On my Mac, it\u0026rsquo;s 8MB. If a coroutine is created for each user to handle, 2GB of memory is sufficient for handling 1 million concurrent user requests, while the thread model would require 8TB.\n1 2 3 4 5 6 7 8 9 10 ➜ trace git:(main) ✗ ulimit -a -t: cpu time (seconds) unlimited -f: file size (blocks) unlimited -d: data seg size (kbytes) unlimited -s: stack size (kbytes) 8176 -c: core file size (blocks) 0 -v: address space (kbytes) unlimited -l: locked-in-memory size (kbytes) unlimited -u: processes 2666 -n: file descriptors 12544 Conclusion Since coroutines complete context switches in user space, the switch time is only slightly over 50ns, which is 70 times higher than process switches. The stack memory required by a single coroutine is also quite small, only requiring 2KB. Therefore, coroutines have shined in high-concurrency scenarios in backend internet applications in recent years.\nWhether in terms of space or time performance, they are much better than processes (threads). Then why doesn\u0026rsquo;t Linus implement them in the operating system? For the sake of better real-time performance, the operating system may preempt the CPU of processes with higher priorities. However, coroutines cannot achieve this and still rely on the coroutines currently using the CPU to release it actively, which is not consistent with the implementation purpose of the operating system. Therefore, the efficiency of coroutines comes at the cost of sacrificing preemption.\nCoroutines ultimately execute attached to operating system threads.\nA question we need to consider is:\nDoes using coroutines mean threads no longer switch? The frequency of thread switches basically depends on the number of threads. When using coroutines, you need to specify tasks for each thread. For the same workload, the number of threads required by coroutines should always be higher than that of automatically allocated thread pools.\nTherefore:\nUsing threads = thread switch overhead (low)\nUsing coroutines = thread switch overhead (high) + coroutine switch overhead\nThen CPU overhead:\nInstruction cycles of threads = interrupt detection + instruction execution (including fetch, decode, and execute)\nInstruction cycles of coroutines = interrupt detection + instruction execution + interrupt detection + coroutine signal detection\nSo, I have the following conclusion:\nIn terms of performance, IO multiplexing + thread pool completely outperforms coroutines; but in terms of convenience, coroutines are still easier to use.\nBecause calling coroutines in Go is so convenient, some Go programmers use the go keyword casually. It should be noted that before switching to a coroutine, the coroutine must be created first. Once created, plus the scheduling overhead, it increases to 400ns, which is almost equivalent to the time consumed by a system call. Although coroutines are efficient, they should not be used casually,\nIf you like my articles, consider to :\nDrop me a follow -\u0026gt; huizhou92 Leave a clap👏 (50 👏👏👏👏would be the best) and a comment if you want to interact with me. Receive an email every time I post on Medium -\u0026gt; Click here! If you find my article helpful to you, please buy me a cup of coffee ","date":"2024-03-20T16:32:00Z","permalink":"https://example.com/p/performance-analysis-of-goroutine-switching/","title":"Performance analysis of goroutine switching"},{"content":"Process is one of the great inventions of the operating system, which shields application programs from hardware details such as CPU scheduling and memory management, abstracting the concept of a process, allowing applications to focus on implementing their own business logic, and \u0026ldquo;simultaneously\u0026rdquo; performing many tasks on a limited CPU. However, while it brings convenience to users, it also introduces some additional overhead. As shown in the figure below, during the running time of a process, although the CPU is busy, it does not complete any user work, which is the additional overhead brought by the process mechanism.\nThis article is first published in the medium MPP plan. If you are a medium user, please follow me in medium. Thank you very much.\nDuring the process switch from process A to process B, first save the context of process A so that when A resumes running, it knows what the next instruction of process A is. Then, restore the context of process B to the register. This process is called a context switch. The context switch overhead is not significant in scenarios with few processes and infrequent switches. However, now the Linux operating system is used in high-concurrency network backend servers. When a single machine supports thousands of user requests, this overhead needs to be addressed. Because user processes are blocked by network I/O such as Redis, Mysql data, or when the process time slice is up, it will trigger a context switch.\nA Simple Experiment on Process Context Switch Overhead Without further ado, let\u0026rsquo;s conduct an experiment to see how long it takes for a context switch! The experimental method is to create two processes and transfer a token between them. One process will be blocked when reading the token, and the other process will be blocked when waiting for its return. After back-and-forth transmission for a certain number of times, we can then calculate their average single-switch time overhead.\ntest04\n1 2 3 4 # gcc main.c -o main # ./main./main Before Context Switch Time1565352257 s, 774767 us After Context SWitch Time1565352257 s, 842852 us The time for each execution may vary, but the average time for each context switch is around 3.5 microseconds. Of course, this number varies depending on the machine, and it is recommended to test it on real hardware.\nWhen we tested system calls earlier, the minimum value was 200 nanoseconds. It can be seen that the context switch overhead is greater than the system call overhead. While a system call only switches from user mode to kernel mode within the process and then switches back, a context switch directly switches from process A to process B. Obviously, this context switch requires more work to be done.\nTypes of Overhead in Process Context Switching So what specific CPU overheads are involved in context switching? Overheads can be divided into two types: direct overhead and indirect overhead.\nDirect overhead includes tasks that the CPU must perform during the switch, including:\nSwitching the page table global directory. Switching the kernel stack. Switching hardware contexts (all data to be loaded into registers before the process resumes, collectively referred to as hardware context) ip(instruction pointer): points to the next instruction being executed bp(base pointer): used to store the stack bottom address of the executing function\u0026rsquo;s stack frame sp(stack pointer): used to store the stack top address of the executing function\u0026rsquo;s stack frame cr3: Page Directory Base Register, stores the physical address of the page directory table \u0026hellip;\u0026hellip; Refreshing TLB. Execution of the system scheduler\u0026rsquo;s code. Indirect overhead mainly refers to the fact that when switching to a new process, due to various caches not being hot, the speed of execution will be slower. If the process is always scheduled on the same CPU, it\u0026rsquo;s somewhat better, but if it crosses CPUs, the previously warmed TLB, L1, L2, L3 caches become useless because the running process has changed, leading to more memory IO penetrations for the new process. In fact, our previous experiment did not measure this situation well, so the actual context switch overhead may be greater than 3.5 microseconds.\nFor students who want to understand the detailed operation process, please refer to Chapter 3 and Chapter 9 of \u0026ldquo;Understanding the Linux Kernel.\u0026rdquo;\nA More Professional Testing Tool - lmbench lmbench is an open-source benchmark for evaluating system performance on multiple platforms, which can test various aspects of performance including document reading and writing, memory operations, process creation and destruction overhead, and networking. The usage is simple, but it runs a bit slow. Interested students can try it out themselves.\nThe advantage of this tool is that it conducts multiple experiments, each with 2 processes, 8 processes, and 16 processes. The size of data used by each process also varies, fully simulating the impact of cache misses. I used it for testing and the results are as follows:\n1 2 3 4 5 ------------------------------------------------------------------------- Host OS 2p/0K 2p/16K 2p/64K 8p/16K 8p/64K 16p/16K 16p/64K ctxsw ctxsw ctxsw ctxsw ctxsw ctxsw ctxsw --------- ------------- ------ ------ ------ ------ ------ ------- ------- bjzw_46_7 Linux 2.6.32- 2.7800 2.7800 2.7000 4.3800 4.0400 4.75000 5.48000 lmbench shows that the process context switch time ranges from 2.7 microseconds to 5.48 microseconds.\nThread Context Switching Time Previously, we tested the overhead of process context switching, now let\u0026rsquo;s continue to test threads in Linux. Let\u0026rsquo;s see if threads can be faster than processes, and if so, how much faster.\nIn Linux, there are actually no threads, but just to cater to developers\u0026rsquo; taste, a lightweight process was created and called a thread. Like processes, lightweight processes also have their own independent task_struct process descriptors and their own independent PIDs. From the perspective of the operating system, there is no difference in scheduling between threads and processes; they are just selecting a task_struct from the waiting queue to switch to the running state. The only difference between lightweight processes and regular processes is that lightweight processes can share the same memory address space, code segment, global variables, and the same set of open files.\nFor threads in the same process, the PID seen by getpid() is actually the same, but there is a tgid field in the task_struct. For multi-threaded programs, what getpid() system call actually gets is this tgid, so multiple threads belonging to\nthe same process appear to have the same PID.\nWe\u0026rsquo;ll use an experiment to test this test06. The principle is similar to the process test. We create 20 threads and use a pipe to pass signals between them. When a signal is received, it will wake up, then pass the signal to the next thread, and sleep by itself. In this experiment, we separately considered the additional overhead of passing signals through the pipe and calculated it in the first step.\n1 2 3 # gcc -lpthread main.c -o main 0.508250 4.363495 The results may vary each time, and the above results are averages of multiple runs. The approximate time for each thread switch is around 3.8 microseconds. From the perspective of context switch time, Linux threads (lightweight processes) are actually not much different from processes.\nLinux Related Commands Now that we know the CPU time consumed by context switching, how can we check how many switches are happening in Linux? If context switches are affecting the overall system performance, is there a way to identify problematic processes and optimize them?\n1 2 3 4 5 6 7 8 # vmstat 1 procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 2 0 0 595504 5724 190884 0 0 295 297 0 0 14 6 75 0 4 5 0 0 593016 5732 193288 0 0 0 92 19889 29104 20 6 67 0 7 3 0 0 591292 5732 195476 0 0 0 0 20151 28487 20 6 66 0 8 4 0 0 589296 5732 196800 0 0 116 384 19326 27693 20 7 67 0 7 4 0 0 586956 5740 199496 0 0 216 24 18321 24018 22 8 62 0 8 or\n1 2 3 4 5 6 7 8 9 10 11 12 13 # sar -w 1 proc/s Total number of tasks created per second. cswch/s Total number of context switches per second. 11:19:20 AM proc/s cswch/s 11:19:21 AM 110.28 23468.22 11:19:22 AM 128.85 33910.58 11:19:23 AM 47.52 40733.66 11:19:24 AM 35.85 30972.64 11:19:25 AM 47.62 24951.43 11:19:26 AM 47.52 42950.50 ...... The above environment is a production machine with a configuration of 8 cores and 8GB of KVM virtual machine, running nginx+fpm. There are about 100 user interface requests processed per second on average. The cs column in the above output indicates the number of context switches that occurred in 1 second, and it\u0026rsquo;s about 40,000 times per second. Roughly estimating, each core needs to switch about 5,000 times per second, which means nearly 20 milliseconds are spent on context switching per second. Considering this is a virtual machine, there are some additional overheads in virtualization, and it also consumes CPU for user interface logic processing, system call kernel logic processing, networking, and soft interrupt processing, so a 20ms overhead is not low.\nSo, further, let\u0026rsquo;s see which processes are causing frequent context switches?\n1 2 3 4 5 6 # pidstat -w 1 11:07:56 AM PID cswch/s nvcswch/s Command 11:07:56 AM 32316 4.00 0.00 php-fpm 11:07:56 AM 32508 160.00 34.00 php-fpm 11:07:56 AM 32726 131.00 8.00 php-fpm ...... Because fpm operates in synchronous blocking mode, most of the switches are voluntary, with fewer involuntary switches occurring only when the time slice expires.\nIf you want to see the total context switch situation of a specific process, you can directly check it under the /proc interface, but this is the total value.\n1 2 3 grep ctxt /proc/32583/status voluntary_ctxt_switches: 573066 nonvoluntary_ctxt_switches: 89260 Conclusion We don\u0026rsquo;t need to remember exactly what the context switch does, just remember one conclusion: the context switch overhead on my working machine is about 2.7-5.48 microseconds, you can test it on your own machine using the code or tools I provided. You can use vmstat sar and other tools to view the context switches of processes and then locate performance issues. lmbench is relatively more accurate because it considers the additional overhead caused by cache misses after the switch. ","date":"2024-03-15T12:48:15.391Z","permalink":"https://example.com/p/the-time-in-the-computers-context-switching/","title":"The Time In The Computers, Context Switching"},{"content":"Decrypt Go: Understand the Three types of pointers in the Go Go language has three types of pointers. In the normal development process, we only encounter the ordinary pointer. However, in the low-level source code of the Go language, there are a lot of operations involving three types of pointer conversion and manipulation. Let’s clarify these points first.\nIn the C language, pointers are crucial. Although pointers make operations highly flexible and efficient, there are many security risks associated with accessing memory through pointer operations, such as accessing memory out of bounds and compromising the atomicity of types in the type system. Here are some examples of incorrect usage:\n1 2 3 4 5 6 7 8 9 10 // Example 1 int arr[2]; *(arr+2) = 1; // Accessing memory address out of bounds ​ // Example 2 int a = 4; int* ap = \u0026amp;a; // Taking the starting address of variable a (4 bytes) *(short*)ap = 2; // Modifying the first 2 bytes of the 4-byte variable a directly through type casting, thus breaking the atomicity of the int variable ​ // The code in Example 2 may occur in certain scenarios, but it has portability issues on machines with different endianness The reason for these security risks in the C language is that it supports pointer operations and pointer type conversions. Therefore, in Go language, the most commonly used ordinary pointers, which have types, have eliminated pointer arithmetic and type conversion operations to ensure type safety. Here’s an example:\n1 2 3 4 5 var a int32 = 10 var ap *int32 = \u0026amp;a // Ordinary pointer with type ​ ap++ // Illegal, pointer arithmetic is not allowed p := (*int16)(ap) // Illegal, *int32 cannot be directly converted to *int16 This ensures that pointers always point to valid addresses with allocated memory and preserves type independence and atomicity.\nIn addition to ordinary pointers, Go language also retains two other types of pointers that allow bypassing the type system and achieving the same level of memory manipulation as in C language. The other two types of pointers are:\nunsafe.Pointer\nuintptr\nTo understand these two, we need to establish a concept: a pointer is essentially a number that stores a memory address. The addressing space is 32 bits for a 32-bit machine and 64 bits for a 64-bit machine, so the size of a pointer is equal to the number of bits in the machine.\nuintptr is straightforward; it is simply a number that stores a memory address. It is equivalent to uint32 a 32-bit machine and uint64 on a 64-bit machine. Since it is a number, it naturally supports arithmetic operations, which allows it to represent any memory location. However, the problem is that data cannot be operated solely based on its memory address; you also need to know its size. In other words, we cannot manipulate data solely based on a uintptr pointer. On the other hand, an ordinary typed pointer not only provides the address but also informs the compiler about the size of the data pointed to. For example, *int32 and *int64 pointers tell the compiler that they operate on 4-byte and 8-byte data, respectively.\nNow that we have explained ordinary pointers and uintptr pointers in Go language, what is this additional unsafe.Pointer compared to C language?\nunsafe.Pointer is a generic pointer that, like uintptr, only keeps the memory address without concerning itself with the type. However, the difference between unsafe.Pointer and uintptr is that the former refers to an object that will be referenced by the garbage collector (GC), so it will not be collected as garbage by the GC. In contrast, the latter only represents the memory address as a number, which means that if a data address is saved by uintptr, it will be mercilessly collected by the garbage collector.\nSummary of the three types of pointers in Go language:\nOrdinary pointer: This does not support pointer arithmetic, saves the address and type information, and the data it points to will not be garbage collected by the GC. unsafe.Pointer: Does not support pointer arithmetic, saves the address but not the type information, and the data it points to will not be garbage collected by the GC. uintptr: Supports address arithmetic, saves the address but not the type information, and the data it points to will be garbage collected by the GC. In practical usage, uintptr cannot be directly converted to an ordinary pointer, and both must be first converted to unsafe.Pointer as an intermediate step before further conversion.\nHere’s a simple example:\n1 2 3 4 5 6 7 8 type Foo struct{ a int32 b int32 } foo := \u0026amp;Foo{} bp := uintptr(unsafe.Pointer(foo)) + 4 // Add 4 to the address of foo to locate foo.b *(*int32)(unsafe.Pointer(bp)) = 1 // Convert to *int32 ordinary pointer and modify the value fmt.Println(foo.b) // foo.b = 1 ","date":"2024-02-25T12:32:00Z","permalink":"https://example.com/p/decrypt-go-understand-the-three-pointers-in-the-go/","title":"Decrypt Go:  Understand the Three Pointers in the Go"},{"content":"\nMechanical hard disk drives (HDD) and solid-state drives (SSD) are two of the most common types of hard drives. As external storage for computers, it takes a long time for the CPU to access the data stored on them. According to the table below, accessing 4KB of data randomly in an SSD takes 1,500 times longer than accessing main memory, while the seek time for a mechanical disk is 100,000 times longer than accessing main memory:\nLatency Numbers Every Programmer Should Know https://gist.github.com/jboner/2841832\nAlthough the seek time for a disk is only 10 ms, it is already a very long time for the CPU. When we scale up the times mentioned above proportionally, we can intuitively feel the performance differences. For example, if accessing L1 cache takes 1 second for the CPU, accessing main memory would take 3 minutes, reading data randomly from an SSD would take 3.4 days, disk seek would take 2 months, and network transmission could take over a year.\nIn computer architecture, hard disks are common input/output devices, and the operating system does not necessarily need a hard disk to start. It can start through a hard disk, a network device, or an external device. Therefore, a hard disk is not a necessary condition for computer operation.\nInput/Output Devices\nAs an external input/output device, compared to CPU cache and memory, the slow read and write speed of a hard disk is reasonable. However, the several thousand to several hundred thousand times difference in speed does make it difficult to imagine or accept. In this article, we will analyze why accessing a hard disk is very slow for the CPU:\nThe process of CPU accessing data from a hard disk is complex. It first reads the data from the disk into memory through I/O operations and then accesses the data stored in memory. Mechanical hard disks rely on mechanical structures to access the data in the disk, which requires moving the mechanical arm of the disk. I/O Operations In order for the CPU to access data from the disk, it must first read the data from the disk into memory through I/O operations, and then access the data stored in memory. There are three common types of I/O operations in computers: Programmed I/O, Interrupt-driven I/O, and Direct Memory Access (DMA). We will introduce these operations one by one:\nThe simplest form of performing I/O operations is using Programmed I/O. When using Programmed I/O, the CPU is responsible for all the work. For example, if we want to output “Hello World” on the screen, the CPU will write a new character to the I/O device each time, and after writing, it will poll the device’s status and wait for it to complete its work before writing a new character. This method is simple but it occupies all the CPU resources, which can cause serious waste of computing resources in some complex systems.\nInterrupt-driven I/O is a more efficient way to perform I/O operations. In Programmed I/O, the CPU actively retrieves the device’s status and waits for the device to become idle. However, if Interrupt-driven I/O is used, the device will actively initiate an interrupt when it is idle, pause the current process, and save the context. The operating system will then execute the interrupt handler for the I/O device:\nIf there are no characters to be printed at the moment, the interrupt handler stops and resumes the paused process. If there are characters to be printed, the next character is copied to the device and the paused process is resumed. Using Interrupt-driven I/O allows the CPU to handle other tasks when the device is busy, thus maximizing CPU utilization and avoiding wasting precious computing resources. Compared to Programmed I/O, Interrupt-driven I/O delegates some work to the I/O device, thereby improving resource utilization.\nDirect Memory Access (DMA) uses a DMA controller to perform I/O operations. Interrupt-driven I/O requires triggering an operating system interrupt for each character, which consumes CPU time. When we use a DMA controller, the CPU reads all the data from the buffer into the DMA controller at once, and the DMA controller is responsible for writing the data to the I/O device character by character。 Although the DMA controller can free up the CPU and reduce the number of interrupts, its execution speed is much slower than the CPU. If the DMA controller cannot drive the I/O device quickly, the CPU may have to wait for the DMA controller to trigger an interrupt. In this case, Interrupt-driven I/O or even Programmed I/O can provide faster access speed.\nBy default, we use a DMA controller to perform I/O tasks. However, Programmed I/O and Interrupt-driven I/O are also acceptable options. When the CPU frequently needs to wait for the DMA controller to perform I/O tasks, using Interrupt-driven I/O or even polling Programmed I/O can achieve higher throughput. However, regardless of the method used, I/O is a complex and time-consuming operation in the program.\nMechanical Hard Disk A mechanical hard disk drive (HDD) is an electronic, non-volatile mechanical data storage device. It uses magnetic storage to store and retrieve data on the disk. During the process of reading and writing data, the disk head connected to the mechanical arm of the hard disk reads and writes bits on the surface of the disk 4.\nBecause the disk has a complex mechanical structure, reading and writing data on the disk takes a lot of time. The read and write performance of databases also depends on the performance of the disk. If we randomly query a piece of data in a database using a mechanical hard disk, it may trigger random I/O on the disk. However, it requires a significant cost to read data from the disk into memory. Loading data from a regular disk (non-SSD) involves processes such as queueing, seeking, rotating, and transferring data, which takes about 10 ms.\nWhen estimating the query performance of a database, we can use the order of magnitude of 10 ms to estimate the time occupied by random I/O. It is worth mentioning that random I/O has a significant impact on the query performance of databases. On the other hand, reading data sequentially from a disk can achieve a speed of up to 40 MB/s, which is several orders of magnitude faster. Therefore, we should try to minimize the number of random I/O operations in order to improve performance.\nA solid-state drive (SSD) is a computer storage device that uses flash memory as persistent storage. Unlike mechanical hard disks, SSDs do not contain any mechanical structures. When we read or write data using an SSD, no mechanical structures are involved because everything is done by circuits. Therefore, the read and write speed of an SSD is much faster than that of an HDD.\nFigure 5 — HDD and SSD Prices\nSince their inception, the prices of both mechanical hard disks and SSDs have been continuously decreasing. Mechanical hard disks are the main external storage used in data centers today. Most general-purpose commercial servers use mechanical hard disks as their main external storage. However, because the read and write speed of SSDs is tens of times faster than that of mechanical hard disks, more and more servers, especially databases, use SSDs as their external storage. However, as an external storage device with mechanical structures, it is susceptible to external interference when subjected to vibration.\nConclusion Hard disks are external storage devices in computers that can store large amounts of data persistently. However, the CPU cannot directly access the data on the hard disk. When a computer starts, the operating system loads the necessary data from the disk into memory for CPU access. But if the data the CPU wants to access is not in memory, it takes several thousand to several hundred thousand times longer to read the data. This is mainly due to the following two reasons:\nCPU needs to access data in external storage through I/O operations. The three methods of Programmed I/O, Interrupt-driven I/O, and DMA all incur additional overhead and consume a significant amount of CPU time. Mechanical hard disks access the data in the disk through mechanical structures. Each random I/O operation on the hard disk requires several processes such as queueing, seeking, rotating, and transferring data, which takes about 10 ms. As mentioned in the article, a hard disk is not a necessary hardware device for computer operation. A computer can load the necessary data for startup from any external storage device such as a disk or CD-ROM into memory and start normally. However, hard disks are currently the most common external storage devices. In the end, let’s look at some open-ended questions related to the topic. Interested readers can carefully consider the following questions:\nIs data written to a hard disk always persistently stored without loss? Why is the data in memory cleared after a power outage and restart? If you have any questions about the content of the article or want to learn more about the reasons behind some design decisions in software engineering, you can leave a comment below the blog post. The author will reply to related questions in a timely manner and select suitable topics for future content based on them\n","date":"2024-02-19T03:02:00Z","permalink":"https://example.com/p/why-the-design-why-is-the-cpu-slow-to-access-the-hard-disk/","title":"Why The Design :Why is the CPU slow to access the hard disk?"},{"content":"Today I read a paper about real-world go concurrency error bugs, and here’s a transcript of what I read as a start to learning about go concurrency programming.\nLink\nRecently, I came across a paper in one of the newsletters I subscribe to. The paper, from the University of Pennsylvania, presents the first systematic study of concurrency-related bugs in several major open-source Golang software projects. The researchers examined the commit histories of the following software: Docker, Kubernetes, etcd, gRPC, CockroachDB, and BoltDB, and drew several interesting conclusions.\nResearch Method The focus of this study was on concurrency-related bugs. The researchers conducted their study by examining the commit histories of these projects. They searched for keywords such as “race,” “deadlock,” “synchronization,” and Golang-specific synchronization primitives like “context,” “once,” and “WaitGroup.” They identified fixes for synchronization bugs and even performed some bug replay and reproduction. The bugs were classified as either “blocking” or “non-blocking.”\nNumber and types of bugs in different projects\nBlocking Bugs Blocking bugs refer to bugs where one or more Goroutines are blocked, leading to partial or global deadlock. These bugs typically arise from circular dependencies. Here’s an example:\n1 2 3 4 5 6 7 8 9 10 // goroutine 1 func goroutine1() { m.Lock() - ch \u0026lt;- request // blocks + select { + case ch \u0026lt;- request + default: + } m.Unlock() }// goroutine 1 func goroutine1() { m.Lock() - ch \u0026lt;- request // blocks + select { + case ch \u0026lt;- request + default: + } m.Unlock() goroutine 2 func goroutine2() { for { m.Lock() // blocks m.Unlock() request \u0026lt;- ch } 1 2 3 4 5 6 7 8 // goroutine 2 func goroutine2() { for { m.Lock() // blocks m.Unlock() request \u0026lt;- ch } } In the author’s study, it was found that the proportion of bugs caused by message passing was even higher than those caused by traditional mutexes. Furthermore, there are currently no mature detection methods for such bugs.\nOverall, we found that there are around 42% blocking bugs caused by errors in protecting shared memory, and 58% are caused by errors in message passing. Considering that shared memory primitives are used more frequently than message passing ones (Section 3.2), message passing operations are even more likely to cause blocking bugs.\nPersonal opinion: The reasons for these issues may include the unfamiliarity with new channel synchronization primitives or the overreliance on channels, leading to a relaxed attitude towards bugs. In summary, channels are powerful, but they are not a panacea for solving all synchronization problems.\nNon-blocking Bugs Non-blocking bugs refer to data race issues caused by inadequate memory protection, as well as Goroutine leaks due to delayed sending or receiving on Goroutine channels, which are unique to Go.\nHere’s an example where the original code uses select-case, resulting in the default case being unintentionally executed multiple times. The fix completely replaces the original code with Once:\n1 2 3 4 5 6 7 8 9 10 11 12 // when multiple goroutines execute the following code, default // can execute multiple times, closing the channel more than once, // which leads to panic in Go runtime - select { - case \u0026lt;- c.closed: // do something - default: + Once.Do(func() { close(c.closed) + }) - }// when multiple goroutines execute the following code, default // can execute multiple times, closing the channel more than once, // which leads to panic in Go runtime - select { - case \u0026lt;- c.closed: // do something - default: + Once.Do(func() { close(c.closed) + }) - } Additionally, the paper explores different methods for modifying different types of bugs and provides recommendations for the development of future bug detection tools.\nConclusions and Reflections Go channels provide powerful concurrency patterns, but they are not a panacea. The study found that message passing can cause a higher proportion of blocking bugs. Currently, there are no well-established detection methods. Concurrency issues are inherently complex, and while language features may reduce complexity, they cannot effortlessly solve all problems through casual analysis. In Go programs, synchronization of shared memory (e.g., classic lock/unlock) still accounts for a higher proportion. Furthermore, misuse of channels can lead to performance issues ( see this article). The author observes that many Go bugs exhibit similar patterns, which suggests the possibility of developing more static analysis tools dedicated to analyzing specific types of problems. Personal opinion: When writing Go code, whether using synchronization locks or channels, it is advisable to keep the synchronized code concise, clear, and easy to verify and inspect in order to reduce bugs. The Go compiler includes deadlock and data race detection, but it cannot detect many situations. More in-depth bug research could consider using debuggers like gdb and runtime profiling tools like pprof ( see this article and official documentation). There is further discussion of this paper on HackerNews ( link). Commit messages are not only helpful for tracing bugs, but also for systematically analyzing historical bugs in the future. (Remember not to write “asdfasdf” in Git commit messages, kids!) ","date":"2024-02-04T21:32:27Z","permalink":"https://example.com/p/a-study-of-concurrency-bugs-in-real-world/","title":"A Study of Concurrency Bugs In Real-World"},{"content":"Last time we shared the method of debugging Go code using assembly language. Assembly language allows us to easily trace low-level knowledge of the Go runtime and other underlying details. In this article, we will introduce the powerful debugging tool called “go tool”, which, when mastered, can elevate your development skills to the next level.\nThis article focuses on practical techniques for debugging in Golang and the effective usage of related tools, so you no longer need to worry about how to debug Golang code. Golang, as a modern language, comes with built-in debugging capabilities from the very beginning:\nGolang tools are directly integrated into the language tools, supporting memory analysis, CPU analysis, and blocking lock analysis, among others. Delve and GDB are the most commonly used debug tools, allowing you to dive deeper into program debugging. Delve is currently the most user-friendly Golang debugging program, and IDE debugging is essentially calling dlv, such as in Goland. Unit testing is deeply integrated into the language design, making it very convenient to execute unit tests and generate code coverage. Golang tools Golang integrates a variety of useful tools at the language level, which are the essence of the experience accumulated by Robert Griesemer, Rob Pike, Ken Thompson, and other experts. After installing Golang, you can see all the built-in tools by executing go tool.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 hxzhouh@hxzhouhdeMacBook-Pro ~\u0026gt; go tool addr2line asm buildid cgo compile covdata cover dist distpack doc fix link nm objdump pack pprof test2json trace vet Here, I will focus on selecting several commonly used debug tools:\nnm: view the symbol table (equivalent to the system nm command). objdump: disassembly tool, used to analyze binary files (equivalent to the system objdump command). pprof: metric and performance analysis tool. cover: code coverage generation. trace: sampling over a period of time, metric tracking and analysis tool. compile: code assembly. Now, I will demonstrate the usage of these tools with an example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 package main ​ import ( \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; ) ​ var helloCount int ​ func main() { listener, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:50050\u0026#34;) if err != nil { log.Fatal(err) } defer listener.Close() ​ for { conn, err := listener.Accept() if err != nil { log.Fatal(err) } tcpConn := conn.(*net.TCPConn) err = tcpConn.SetNoDelay(true) if err != nil { log.Println(err) } go handleConnection(conn) } } ​ func handleConnection(conn net.Conn) { defer conn.Close() helloCount++ resp := []byte(\u0026#34;Hello count: \u0026#34;) resp = append(resp, []byte(string(rune(helloCount)))...) resp = append(resp, \u0026#39;\\n\u0026#39;) conn.Write(resp) } // go build main.go nm The nm command is used to view the symbol table, which is equivalent to the system nm command and is very useful. When setting breakpoints, if you don\u0026rsquo;t know the function symbol of the breakpoint, you can use this command to find out (this command operates on binary program files).\ncompile Assemble a specific file:\n1 go tool compile -N -l -S main.go You will be able to see the assembly code corresponding to your Go code (please note that this command operates on Go code text), which is cool.\nobjdump The objdump tool is used to disassemble binaries, equivalent to the system objdump (please note that this command parses binary program files).\n1 2 go tool objdump main.o go tool objdump -s DoFunc main.o // Disassembling specific functions Assembly code may not be needed in 90% of scenarios, but if you have experience working with C programs, in certain special situations, inferring application behavior by disassembling a piece of logic may be your only way out. This is because the code running in production usually has optimization enabled, which can cause your code to not match. Furthermore, you cannot attach to processes at will in production environments. Many times, you only have a core file to troubleshoot.\npprof pprof supports four types of analysis:\nCPU: CPU analysis, sampling calls that consume CPU resources, which is generally used to locate and troubleshoot areas of high computational resource consumption in programs. Memory: Memory analysis, which is generally used to troubleshoot memory usage, memory leaks, and other issues. Block: Blocking analysis, which samples the blocking calls in the program. Mutex: Mutex analysis, which samples the competition for mutex locks. For more information about pprof, you can refer to this article.\ntrace Program trace debugging:\n1 go tool trace -http=\u0026#34;:6060\u0026#34; ./ssd_336959_20190704_105540_trace The trace command allows you to trace and collect information over a period of time, then dump it to a file, and finally analyze the dump file using go tool trace and open it in a web format.\nUnit Testing The importance of unit testing is self-evident. In Golang, files ending with _test.go are considered test files. As a modern language, Golang supports unit testing at the language tool level.\nRunning Unit Tests There are two ways to execute unit tests:\nRun go test directly, which is the simplest method. Compile the test files first, then run them. This method provides more flexibility. Running go test 1 2 3 4 5 6 // Run go test directly in your project directory. go test . // Specify the running function. go test -run=TestPutAndGetKeyValue // Print details. go test -v Compilation and Execution Essentially, running Golang unit tests involves compiling *_test.go files into binaries and then running these binaries. When you execute go test, the tool handles these actions for you, but you can also perform them separately.\nCompile the test files to generate the test executable:\n1 2 3 4 // Compile the .test file first. go test -c -coverpkg=. -covermode=atomic -o 1TestSwapInt32_in_sync_atomic.test sync/atomic // Specify running a file. ./1TestSwapInt32_in_sync_atomic.test -test.timeout=10m0s -test.v=true -test.run=TestSwapInt32 This method is usually used in the following scenarios:\nCompile on one machine and run tests on another. Debugging test programs. Code Coverage Analysis Golang’s code coverage is based on unit tests, which serve as the starting point for measuring code coverage of your business code. The operation is simple:\nAdd the -coverprofile parameter when running tests to record code coverage. Use the go tool cover command to analyze and generate coverage reports. 1 2 go test -coverprofile=coverage.out go tool cover -func=coverage.out The output will be similar to the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 hxzhouh@hxzhouhdeMacBook-Pro ~/w/g/s/sync\u0026gt; go tool cover -func=coverage.out heads/go1.21.4? sync/cond.go:47: NewCond 100.0% sync/cond.go:66: Wait 100.0% sync/cond.go:81: Signal 100.0% sync/cond.go:90: Broadcast 100.0% sync/cond.go:98: check 100.0% sync/cond.go:116: Lock 0.0% sync/cond.go:117: Unlock 0.0% sync/map.go:104: newEntry 100.0% sync/map.go:110: loadReadOnly 100.0% sync/map.go:120: Load 100.0% sync/map.go:145: load 100.0% ....... This way, you can see the code coverage for each function.\nProgram Debugging Program debugging mainly relies on two tools:\n1 2 1. dlv 2. gdb Here, I recommend dlv because GDB’s functionality is limited. GDB does not understand Golang’s specific types such as channels, maps, and slices. GDB’s native support for goroutines is limited since it only understands threads. However, GDB has one irreplaceable feature, which is the gcore command.\ndlv Debugging Debugging Binaries 1 dlv exec \u0026lt;path/to/binary\u0026gt; [flags] For example:\n1 dlv exec ./main Debugging binaries with dlv and passing arguments:\n1 dlv exec ./main -- --audit=./d Debugging Processes 1 dlv attach ${pid} [executable] [flags] The process ID is mandatory. For example:\n1 dlv attach 12808 ./main Debugging Core Files Debugging core files with dlv and redirecting standard output to a file:\n1 dlv core \u0026lt;executable\u0026gt; \u0026lt;core\u0026gt; [flags] 1 dlv core ./main core.277282 Common Debugging Syntax System Summary Program Execution\ncall: call a function (note that this will cause the entire program to run). continue: resume execution. next: step over. restart: restart. step: step into a function. step-instruction: step into a specific assembly instruction. stepout: step out of the current function. Breakpoint-related break (alias: b): set a breakpoint. breakpoints (alias: bp): print all breakpoint information. clear: clear a breakpoint. clearall: clear all breakpoints. condition (alias: cond): set a conditional breakpoint. on: set a command to be executed when a breakpoint is hit. trace (alias: t): set a tracepoint, which is also a breakpoint but does not stop the program when hit; it only prints a line of information. This command is useful in certain scenarios where stopping the program affects logic (e.g., business timeouts), and you only want to print a specific variable. Information Printing args: print function arguments. examinemem (alias: x): a powerful command for examining memory, similar to gdb’s x command. locals: print local variables. print (alias: p): print an expression or variable. regs: print register information. set: set variable value. vars: print global variables (package variables). whatis: print type information. Goroutine-related goroutine (alias: gr): print information of a specific goroutine. goroutines (alias: grs): list all goroutines. thread (alias: tr): switch to a specific thread. threads: print information of all threads. Stack-related deferred: execute commands in the context of a defer function. down: move up the stack. frame: jump to a specific stack frame. stack (alias: bt): print stack information. up: move down the stack. Other Commands config: modify configurations. disassemble (alias: disass): disassemble. edit (alias: ed): omitted. exit (alias: quit | q): omitted. funcs: print all function symbols. libraries: print all loaded dynamic libraries. list (alias: ls | l): display source code. source: load commands. sources: print source code. types: print all type information. The above commands cover the complete set of commands supported by dlv, which meet our debugging needs (some commands are only applicable during development and debugging, as it is not possible to single-step debug on production code in most cases).\nApplication Examples\nPrint Global Variables\n1 (dlv) vars This is very useful for inspecting global variables.\nConditional Breakpoints\n1 2 3 4 5 6 # Set a breakpoint first. (dlv) b # Check breakpoint information. (dlv) bp # Customize the condition. (dlv) condition 2 i==2 \u0026amp;\u0026amp; j==7 \u0026amp;\u0026amp; z==32 Inspecting the Stack\n1 2 3 4 # Show all stacks. (dlv) goroutines # Expand all stacks. (dlv) goroutines -t Examining Memory\n1 (dlv) x -fmt hex -len 20 0xc00008af38 The x command is the same as gdb\u0026rsquo;s x command.\ngdb Debugging GDB’s support for Golang debugging is achieved through a Python script called src/runtime/runtime-gdb.py, so its functionality is limited. GDB can only perform basic variable printing and cannot understand some of Golang\u0026rsquo;s specific types such as channels, maps, and slices. GDB cannot directly debug goroutines because it only understands threads. However, GDB has one feature that cannot be replaced, which is the gcore command.\ndlv Debugging Example Debugging a Binary\n1 dlv exec ./main Debugging a Process\n1 dlv attach 12808 ./main Debugging a Core File\n1 dlv core ./main core.277282 gdb Debugging Example 1 gdb ./main Print Global Variables (note the single quotation marks)\n1 (gdb) p \u0026#39;runtime.firstmoduledata\u0026#39; Due to GDB’s limited understanding of Golang’s type system, sometimes it may not be able to print variables, so please pay attention to this.\nPrint Array Length\n1 (gdb) p $len(xxx) Therefore, I usually only use GDB to generate core files.\nTips and Tricks Don’t know how to set breakpoints in functions? Sometimes you don’t know how to set breakpoints in a function. You can use nm to query the function and then set a breakpoint, which will ensure that you hit the breakpoint.\nDon’t know the calling context? Add the following line in your code:\n1 debug.PrintStack() This will print the stack trace at the current code position, allowing you to understand the calling path of the function.\nDon’t know how to enable pprof? There are two ways to enable pprof, corresponding to two packages:\nnet/http/pprof: used in web server scenarios. runtime/pprof: used in non-server applications. These two packages are essentially the same, with net/http/pprof being a web wrapper on top of runtime/pprof.\nUsing net/http/pprof\n1 import _ \u0026#34;net/http/pprof\u0026#34; Using runtime/pprof\nThis method is usually used for performance optimization. When running a program that is not a server application, you want to find bottlenecks, so you typically use this method.\n1 2 3 4 5 6 7 8 // CPU pprof file path f, err := os.Create(\u0026#34;cpufile.pprof\u0026#34;) if err != nil { log.Fatal(err) } // Start CPU pprof pprof.StartCPUProfile(f) defer pprof.StopCPUProfile() Why does the code sometimes execute unexpectedly during single-step debugging? This situation is usually caused by compiler optimization, such as function inlining and removal of redundant logic and parameters from the compiled binary. This can cause unexpected execution during dlv single-step debugging or prevent the printing of certain variables. The solution to this problem is to disable compiler optimization.\n1 go build -gcflags \u0026#34;-N -l\u0026#34; Conclusion This article provides a systematic overview of the techniques and usage of Golang program debugging:\nThe language tool package provides built-in tools that support assembly, disassembly, pprof analysis, symbol table queries, and other practical functions. The language tool package integrates unit testing, and code coverage relies on triggering unit tests. The powerful dlv/gdb tools serve as the main debugging tools, supporting the analysis of binaries, processes, and core files. ","date":"2024-02-04T13:54:42Z","permalink":"https://example.com/p/advanced-debugging-tips-for-the-go-language/","title":"Advanced Debugging Tips for the Go Language"},{"content":"Introduction After writing tons of code and implementing hundreds of interfaces, you finally managed to deploy your application successfully. However, you soon discover that the performance is not up to par. What a nightmare! 😭\nThe Need for Performance Analysis\n![[Pasted image 20240517214930.png]]\nIntroducing PProf To optimize performance, the first thing to focus on is the toolchain provided by Go itself. In this article, we will explore and utilize the powerful features of Go’s performance profiling tool, PProf. It covers the following areas:\nruntime/pprof: Collects runtime data of non-server programs for analysis net/http/pprof: Collects runtime data of HTTP servers for analysis What is PProf? PProf is a tool used for visualizing and analyzing performance profiling data. It reads a collection of analysis samples in the profile.proto format and generates reports to visualize and analyze the data (supports both text and graphical reports).\nThe profile.proto file is a Protocol Buffer v3 descriptor file that describes a set of call stacks and symbolization information. It represents a set of sampled call stacks for statistical analysis and is a common format for stack trace configuration files.\nSupported Usage Modes Report generation: Generates reports Interactive terminal use: Supports interactive terminal-based usage Web interface: Provides a web-based interface What Can You Do with PProf? CPU Profiling: Collects CPU (including registers) usage of the monitored application at a certain frequency. It helps identify the time spent by the application in actively consuming CPU cycles. Memory Profiling: Records stack traces when heap allocations occur in the application. It monitors current and historical memory usage and helps detect memory leaks. Block Profiling: Records the locations where goroutines block and wait for synchronization (including timer channels). Mutex Profiling: Reports the competition status of mutexes. A Simple Example Let’s start with a simple example that has some performance issues. This will serve as a basic demonstration of program analysis.\nWriting the Demo Files Create a file named demo.go with the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package main ​ import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; _ \u0026#34;net/http/pprof\u0026#34; \u0026#34;github.com/EDDYCJY/go-pprof-example/data\u0026#34; ) ​ func main() { go func() { for { log.Println(data.Add(\u0026#34;https://github.com/EDDYCJY\u0026#34;)) } }() ​ http.ListenAndServe(\u0026#34;0.0.0.0:6060\u0026#34;, nil) } Create a file named data/d.go with the following content: 1 2 3 4 5 6 7 8 9 10 11 package data ​ var datas []string ​ func Add(str string) string { data := []byte(str) sData := string(data) datas = append(datas, sData) ​ return sData } When you run this file, your HTTP server will have an additional endpoint /debug/pprof for observing the application\u0026rsquo;s status.\nAnalysis 1. Using the Web Interface To view the current overview, visit http://127.0.0.1:6060/debug/pprof/ .\n1 2 3 4 5 6 7 8 9 10 /debug/pprof/ ​ profiles: 0 block 5 goroutine 3 heap 0 mutex 9 threadcreate ​ full goroutine stack dump This page contains several subpages. Let’s dive deeper to see what we can find:\ncpu (CPU Profiling): $HOST/debug/pprof/profile. This performs CPU profiling for 30 seconds by default and generates a profile file for analysis. block (Block Profiling): $HOST/debug/pprof/block. This shows the stack traces causing blocking synchronization. goroutine: $HOST/debug/pprof/goroutine. This displays the stack traces of all currently running goroutines. heap (Memory Profiling): $HOST/debug/pprof/heap. This shows the memory allocation of active objects. mutex (Mutex Profiling): $HOST/debug/pprof/mutex. This displays the stack traces of mutex contention. 2. Using the Interactive Terminal Execute the following command: go tool pprof http://localhost:6060/debug/pprof/profile?seconds=60 . 1 2 3 4 5 6 7 8 $ go tool pprof http://localhost:6060/debug/pprof/profile\\?seconds\\=60 ​ Fetching profile over HTTP from http://localhost:6060/debug/pprof/profile?seconds=60 Saved profile in /Users/eddycjy/pprof/pprof.samples.cpu.007.pb.gz Type: cpu Duration: 1mins, Total samples = 26.55s (44.15%) Entering interactive mode (type \u0026#34;help\u0026#34; for commands, \u0026#34;o\u0026#34; for options) (pprof) After executing this command, wait for 60 seconds (you can adjust the value of seconds). PProf will perform CPU profiling during this time. Once finished, it will enter the interactive command mode, allowing you to view or export the analysis results. For a list of available commands, type pprof help.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 (pprof) top10 Showing nodes accounting for 25.92s, 97.63% of 26.55s total Dropped 85 nodes (cum \u0026lt;= 0.13s) Showing top 10 nodes out of 21 flat flat% sum% cum cum% 23.28s 87.68% 87.68% 23.29s 87.72% syscall.Syscall 0.77s 2.90% 90.58% 0.77s 2.90% runtime.memmove 0.58s 2.18% 92.77% 0.58s 2.18% runtime.freedefer 0.53s 2.00% 94.76% 1.42s 5.35% runtime.scanobject 0.36s 1.36% 96.12% 0.39s 1.47% runtime.heapBitsForObject 0.35s 1.32% 97.44% 0.45s 1.69% runtime.greyobject 0.02s 0.075% 97.51% 24.96s 94.01% main.main.func1 0.01s 0.038% 97.55% 23.91s 90.06% os.(*File).Write 0.01s 0.038% 97.59% 0.19s 0.72% runtime.mallocgc 0.01s 0.038% 97.63% 23.30s 87.76% syscall.Write flat: The time spent in a given function. flat%: The percentage of CPU time spent in a given function. sum%: The cumulative percentage of CPU time spent in a given function and its callees. cum: The total time spent in a function and its callees. cum%: The cumulative percentage of CPU time spent in a given function and its callees. The last column represents the function names. In most cases, these five columns provide insights into the application’s runtime behavior, helping you optimize it. 🤔\nExecute the following command: go tool pprof http://localhost:6060/debug/pprof/heap . 1 2 3 4 5 6 7 8 9 $ go tool pprof http://localhost:6060/debug/pprof/heap Fetching profile over HTTP from http://localhost:6060/debug/pprof/heap Saved profile in /Users/eddycjy/pprof/pprof.alloc_objects.alloc_space.inuse_objects.inuse_space.008.pb.gz Type: inuse_space Entering interactive mode (type \u0026#34;help\u0026#34; for commands, \u0026#34;o\u0026#34; for options) (pprof) top Showing nodes accounting for 837.48MB, 100% of 837.48MB total flat flat% sum% cum cum% 837.48MB 100% 100% 837.48MB 100% main.main.func1 -inuse_space: Analyzes the resident memory usage of the application. -alloc_objects: Analyzes the temporary memory allocations of the application. Execute the following command: go tool pprof http://localhost:6060/debug/pprof/block Execute the following command: go tool pprof http://localhost:6060/debug/pprof/mutex 3. PProf Visualization Interface This is the exciting part! But before we proceed, we need to write a simple test case to run.\nWriting the Test Case\nCreate a file named data/d_test.go with the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package data ​ import \u0026#34;testing\u0026#34; ​ const url = \u0026#34;https://github.com/EDDYCJY\u0026#34; ​ func TestAdd(t *testing.T) { s := Add(url) if s == \u0026#34;\u0026#34; { t.Errorf(\u0026#34;Test.Add error!\u0026#34;) } } ​ func BenchmarkAdd(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { Add(url) } } Run the test case: 1 2 3 4 5 $ go test -bench=. -cpuprofile=cpu.prof pkg: github.com/EDDYCJY/go-pprof-example/data BenchmarkAdd-4 10000000 187 ns/op PASS ok github.com/EDDYCJY/go-pprof-example/data 2.300s You can also explore -memprofile.\nLaunching the PProf Visualization InterfaceMethod 1:\n1 $ go tool pprof -http=:8080 cpu.prof Method 2:\n1 2 $ go tool pprof cpu.prof $ (pprof) web If you encounter the message “Could not execute dot; may need to install graphviz,” it means you need to install graphviz (please consult your favorite search engine).\nViewing the PProf Visualization Interface\nWhen you open the PProf visualization interface, you will notice that it is more refined than the official toolchain’s PProf. Additionally, it includes a Flame Graph.\nThe Flame Graph is the highlight of this section. It is a dynamic visualization where the call sequence is represented from top to bottom (A -\u0026gt; B -\u0026gt; C -\u0026gt; D). Each block represents a function, and the larger the block, the more CPU time it consumes. It also supports drill-down analysis by clicking on the blocks!\nConclusion In this article, we provided a brief introduction to PProf, the performance profiling tool in Go.\nPProf offers great assistance in locating and analyzing performance issues in specific scenarios.\nWe hope this article has been helpful to you. We encourage you to try it out yourself and delve deeper into the various features and knowledge points it offers.\nThought Questions Congratulations on making it to the end! Here are two simple thought questions to expand your thinking:\nIs flat always greater than cum? Why? In what scenarios would cum be greater than flat? What performance issues can you identify in the demo code provided in this article? How would you address them? Now it’s your turn to share your thoughts!\n","date":"2024-02-02T04:05:00Z","permalink":"https://example.com/p/performance-profiling-with-pprof/","title":"Performance Profiling with PProf"},{"content":"TCP protocol is one of the network protocols we use in our daily lives. It is responsible for establishing and terminating connections. In the previous blog post, we analyzed why TCP requires three handshakes to establish a connection. When establishing a connection, we need to ensure the issues of historical connections and sequence numbers. Unlike the three-way handshake during connection establishment, disconnecting a TCP connection requires a four-way handshake. This article will explore why TCP disconnects require a four-way handshake instead of three or any other number.\nOverview Before delving into why a four-way handshake is necessary, let’s first understand the process of TCP connection termination. Typically, when one party in the communication decides to terminate the connection, it sends a FIN (Finish) control message to the other party, indicating that it has no more data to send. The receiving party responds with an ACK (Acknowledgment) control message to acknowledge and enters a half-closed state, indicating that it will no longer send data but can still receive data. When the other party also has no more data to send, it sends a FIN message to indicate its intention to disconnect. The receiving party then sends another ACK message to confirm, and only then will the connection be fully closed.\nDesign Why does TCP disconnect require a four-way handshake? Let’s analyze this question from several aspects:\nEnsuring Data Integrity TCP is a connection-oriented reliable transport protocol that guarantees data reliability and integrity. During connection termination, both parties may still have unsent or unacknowledged data packets. If only a three-way handshake is performed, the other party may not know whether the FIN message it sent has been received during the final handshake. This could result in the other party not fully receiving the data, leading to data loss. Therefore, by performing a four-way handshake, both parties can ensure that they receive each other’s data completely and maintain data integrity.\nHandling Network Latency and Packet Loss In a network, data packets may be delayed or lost due to network latency or packet loss. During connection termination, if only a three-way handshake is performed, the other party may not know whether the FIN message it sent has been received, which may prevent timely closure of the connection. By performing a four-way handshake, sufficient time is provided for the network to handle latency and packet loss issues, ensuring that the connection can be properly closed.\nWaiting for Unsent Data to be Sent During connection termination, both parties may still have unsent data packets. If only a three-way handshake is performed, the other party may not know whether there is any unsent data before sending the FIN message. By performing a four-way handshake, both parties have enough time to send and receive the remaining data, ensuring data integrity and correctness.\nHandling the Half-Closed State During the TCP connection termination process, one party enters a half-closed state by sending a FIN message, indicating that it will no longer send data but can still receive data. If only a three-way handshake is performed, the other party will immediately close the connection upon receiving the FIN message, which may prevent proper handling of the half-closed state. By performing a four-way handshake, both parties can ensure the correct handling of the half-closed state, avoiding data loss and confusion.\nConclusion Based on the analysis above, we can conclude that a four-way handshake is necessary for TCP disconnection to ensure data integrity, handle network latency and packet loss, wait for unsent data to be sent, and handle the half-closed state. Through the four-way handshake, both parties can better coordinate and handle the disconnection, ensuring the correct transmission of data and secure closure of the connection.\nWhen discussing TCP disconnection, we should not focus on why a four-way handshake is used, but rather understand why multiple handshakes are needed to ensure data integrity and proper connection closure. By gaining a deep understanding of TCP protocol design, we can better apply and comprehend the principles and mechanisms of network communication.\nWhether it is three handshakes or four handshakes, the first element of a tcp connection to consider is always security and data integrity, although it seems that three handshakes and four handshakes seem to be very inefficient, but the vast majority of Internet traffic is based on the tcp protocol, which is enough to prove that its reliability, on the issue of performance, there are other ways to optimize the performance of the protocol, such as udp, and later on, we will analyze how UDP and UDP-based QUIC protocol will affect the Internet in the next ten years. and how the UDP-based QUIC protocol will influence the next decade of the Internet.\nIf you found my article enjoyable, feel free to follow me and give it a 👏. Your support would be greatly appreciated.\nReferences RFC 793 — Transmission Control Protocol — IETF Tools Why do we need a 4-way handshake to terminate a TCP connection? why-tcp-connect-termination-need-4-way-handshake ","date":"2024-02-01T11:11:00Z","permalink":"https://example.com/p/why-does-it-take-four-waves-for-tcp-to-disconnect/","title":"Why does It Take Four Waves For TCP To Disconnect?"},{"content":"TCP protocol is a network protocol that we encounter almost every day. The majority of network connections are established based on the TCP protocol. People who have studied computer networks or have some understanding of the TCP protocol know that establishing a connection using TCP requires a three-way handshake.\nIf we briefly explain the process of establishing a TCP connection, many people who have prepared for interviews would be familiar with it. However, when it comes to delving into the question of “Why does TCP require a three-way handshake to establish a connection?” most people would not be able to answer this question or might provide incorrect answers. This article will discuss why we need a three-way handshake to establish a TCP connection instead of four or two.\nOverview Before analyzing the question at hand, let’s first address a common misconception that has misled many people regarding the TCP connection process. For a long time, the author of this article also believed that it provided a good explanation for why a TCP connection requires a three-way handshake:\n\u0026ndash;\u0026gt; Can you hear me?\n\u0026lt;\u0026ndash; I can hear you. Can you hear me?\n\u0026ndash;\u0026gt; I can hear you too.\nUsing analogies to explain a problem often leads to a situation where “nine out of ten analogies are wrong.” If someone uses an analogy to answer your “why” question, you need to carefully consider the flaws in their analogy. Analogies can only provide a partial similarity, and we can never find an absolutely correct analogy. Analogies are only useful when we want to present the characteristics of something in a simple and understandable way. In the rest of the article, we will explain why this analogy is flawed, and readers can read the remaining content with this question in mind.\nWhen many people try to answer or think about this question, they tend to focus on the “three” in the three-way handshake, which is indeed important. However, if we reexamine the question, do we really understand what a “connection” is? Only when we know the definition of a “connection” can we attempt to answer why TCP requires a three-way handshake.\nThe reliability and flow control mechanisms described above require that TCPs initialize and maintain certain status information for each data stream. The combination of this information, including sockets, sequence numbers, and window sizes, is called a connection.\nThe RFC 793 — Transmission Control Protocol document clearly defines what a connection is in TCP. In summary, a connection is the information used to ensure reliability and flow control mechanisms, including sockets, sequence numbers, and window sizes.\nTherefore, establishing a TCP connection means that the two parties involved in communication need to reach a consensus on the three types of information mentioned above. A pair of sockets in a connection is composed of an Internet address identifier and a port. The window size is mainly used for flow control, and the sequence number is used to track the sequence of data packets sent by the initiating party, allowing the receiving party to confirm the successful receipt of a particular data packet based on the sequence number.\nAt this point, we have transformed the original question into “Why do we need a three-way handshake to initialize sockets, window sizes, and initial sequence numbers?” Next, we will analyze and seek explanations for this refined question.\nDesign This article will mainly discuss why we need a three-way handshake to initialize sockets, window sizes, initial sequence numbers, and establish a TCP connection from the following aspects:\nA three-way handshake is required to prevent the initialization of duplicate historical connections. A three-way handshake is required to initialize the initial sequence numbers of both communicating parties. Discuss the possibility of establishing a connection with a different number of handshakes. Among these arguments, the first one is the primary reason why TCP chooses to use a three-way handshake. The other reasons are secondary in comparison. We discuss them here to provide a more comprehensive perspective and understand this interesting design decision from multiple angles.\nHistorical Connections The RFC 793 — Transmission Control Protocol clearly points out the primary reason why TCP uses a three-way handshake: to prevent confusion caused by the initiation of old duplicate connections.\nThe principle reason for the three-way handshake is to prevent old duplicate connection initiations from causing confusion.\nImagine this scenario: if the number of communications between the two parties is only two, once the sender sends a connection establishment request, it cannot retract this request. In a complex or poor network condition, if the sender continuously sends multiple connection establishment requests and TCP establishes a connection with only two communications, the receiver can only choose to accept or reject the sender’s request. The receiver is not sure whether this request is an expired connection due to network congestion.\nTherefore, TCP chooses to use a three-way handshake to establish a connection and introduces the RST control message. When the receiver receives the request, it sends the sender\u0026rsquo;s SEQ+1 as part of the ACK control message. At this point, the sender can determine whether the current connection is a historical connection:\nIf the current connection is a historical connection, meaning the SEQ has expired or timed out, the sender will directly send an RST control message to terminate this connection. If the current connection is not a historical connection, the sender will send an ACK control message, and the two parties will successfully establish a connection. By using a three-way handshake and the RST control message, the ultimate control over whether to establish a connection is given to the sender. Only the sender has enough context to determine if the current connection is erroneous or expired. This is also the primary reason why TCP uses a three-way handshake to establish a connection.\nInitial Sequence Numbers Another important reason for using a three-way handshake is that both communicating parties need to obtain an initial sequence number for sending information. As a reliable transport layer protocol, TCP needs to build a reliable transport layer in an unstable network environment. The uncertainty of the network can lead to issues such as packet loss and out-of-order delivery. Common problems may include:\nData packets being repeatedly sent by the sender, resulting in duplicate data. Data packets being lost during transmission due to routing or other network nodes. Data packets arriving at the receiver may not be in the order they were sent. To address these potential issues, the TCP protocol requires the sender to include a “sequence number” field in the data packet. With the sequence number corresponding to each data packet, we can:\nThe receiver can deduplicate repeated data packets based on the sequence number. The sender will resend the corresponding data packet until it is acknowledged. The receiver can reorder the data packets based on their sequence numbers. Sequence numbers play a crucial role in TCP connections, and the initial sequence number, as part of a TCP connection, needs to be initialized during the three-way handshake. Since both parties in a TCP connection need to obtain the initial sequence number, they need to send a SYN control message to each other, carrying their expected initial sequence number SEQ. Upon receiving the SYN message, the receiver will confirm it using the ACK control message and SEQ+1.\nAs shown in the above diagram, the two TCPs, A and B, send SYN and ACK control messages to each other. After both parties obtain their expected initial sequence numbers, they can start communication. Due to the design of the TCP message header, we can combine the two middle communications into one. TCP B can send both the ACK and SYN control messages to TCP A simultaneously, reducing the four communications to three.\nA three-way handshake is necessary because sequence numbers are not tied to a global clock in the network, and TCPs may have different mechanisms for picking the ISN’s. The receiver of the first SYN has no way of knowing whether the segment was an old delayed one or not unless it remembers the last sequence number used on the connection (which is not always possible), and so it must ask the sender to verify this SYN. The three-way handshake and the advantages of a clock-driven scheme are discussed in [3].\nFurthermore, as a distributed system, the network does not have a global clock for counting. TCP can initialize sequence numbers using different mechanisms. As the receiver of a TCP connection, we cannot determine if the initial sequence number received from the other party is expired. Therefore, we need the other party to make this determination. It is not practical for the receiver to save and verify the sequence numbers, which reinforces the point we made in the previous section — avoiding the initialization of historical wrong connections.\nNumber of Communications When discussing the number of communications required to establish a TCP connection, we often focus on why it takes three communications instead of two or four. Discussing using more communications to establish a connection is often meaningless because we can always “exchange the same information using more communications.” Therefore, it is technically possible to establish a connection using four, five, or even more communications.\nThe issue of increasing the number of communications in a TCP connection often does not require discussion. What we pursue is actually completing the information exchange with the fewest number of communications (the theoretical minimum). This is why we repeatedly emphasize in the previous sections that using a “two-way handshake” cannot establish a TCP connection, and using a three-way handshake is the minimum number of communications required to establish a connection.\nConclusion In this article, we discussed why TCP requires a three-way handshake to establish a connection. Before analyzing this question in detail, we first reconsidered what a TCP connection is. The RFC 793 — Transmission Control Protocol — IETF Tools provides a clear definition of a TCP connection — the data used for ensuring reliability and flow control mechanisms, including sockets, sequence numbers, and window sizes.\nThe three-way handshake in TCP can effectively prevent the initiation of erroneous historical connections and reduce unnecessary resource consumption for both communicating parties. The three-way handshake helps both parties obtain the initial sequence numbers, ensuring that data packets are transmitted without duplication or loss and maintaining their order. At this point, it is clear why “two-way handshake” and “four-way handshake” are not used:\n“Two-way handshake”: It cannot prevent the initialization of erroneous historical connections and wastes resources for the receiver. “Four-way handshake”: The design of the TCP protocol allows us to simultaneously transmit both the ACK and SYN control messages, reducing the number of communications. Therefore, there is no need to use more communications to transmit the same information. Returning to the question raised at the beginning of the article, why is using an analogy to explain TCP’s three-way handshake incorrect? This is mainly because the analogy does not clearly explain the core issue — avoiding the initialization of historical duplicate connections.\nReference RFC 793 — Transmission Control Protocol — IETF Tools Why do we need a 3-way handshake? Why not just 2-way? # TCP 3-Way Handshake Process ","date":"2024-01-30T03:04:00Z","permalink":"https://example.com/p/why-tcp-requires-three-handshakes-to-establish-a-connection/","title":"Why TCP requires three handshakes to establish a connection?"},{"content":"Photo by Alex Kulikov on Unsplash\nIPv6 has been around for a long time, and I have dealt with many IPv6 tasks in my work. However, I never thought about switching my EC2 instance to IPv4. Yesterday, while going through my email trash, I came across a message stating that AWS will start charging for IPv4 addresses from February 1, 2024. This caught my attention, so I decided to switch my EC2 instance to IPv6 today. The process was a bit of a hassle. This article is not only applicable to EC2 instances but should also work for other Linux hosts.\nAdding an IPv6 Address to EC2 Since my EC2 instance’s DNS resolution is handled by Cloudflare, I mainly referred to this blog post: Amazon’s $2bn IPv4 Tax — and How to Avoid Paying It\nAnd also, the official AWS documentation on Migrating Your VPC from IPv4 to IPv6.\nIt’s worth noting that the demo in the “Migrating Your VPC from IPv4 to IPv6” documentation assumes that the VPC has both a public and a private subnet. If you, like me, only have a public subnet, you can skip that part.\nI must say, AWS documentation is well-written, and there’s a lot to learn from it.\nThe result after completing the setup should look like this, with both IPv4 and IPv6 addresses. Make sure to add the same rules for IPv6 in the security group.\nApplication Support On my EC2 instance, I only have Nginx and Docker running, and I usually log in via SSH. So, I need to add IPv6 support for Nginx and SSH.\nNginx For your HTTP server block (the one listening on port 80), add the line listen [::]:80;. This allows Nginx to listen to both IPv4 and IPv6 HTTP traffic. Your modified server block should look like this:\n1 2 3 4 5 6 server { listen 80; listen [::]:80; server_name hexo.hxzhouh.com; return 301 https://$host$request_uri; } For each HTTPS server block (those listening on port 443), add listen [::]:443 ssl; inside each block. This enables Nginx to listen for HTTPS traffic on IPv6. For example, for the first HTTPS server block, you need to make the following modification:\n1 2 3 4 5 6 server { listen 443 ssl; listen [::]:443 ssl; server_name hexo.hxzhouh.com; # other configurations... } Make these modifications for each HTTPS server block. Then, test the Nginx configuration with Nginx -t. If there are no issues, reload the Nginx configuration with systemctl reload nginx.\nSSHD In the sshd_config file, uncomment the line AddressFamily any (i.e., remove the preceding #) to enable IPv6 listening for SSH and other applications.\n1 2 3 4 5 6 7 vim /etc/ssh/sshd_config ​ #Port 22 AddressFamily any AddressFamily inet #ListenAddress 0.0.0.0 #ListenAddress :: Then, restart SSHD with sudo systemctl reload sshd. Use the netstat -tupln command to check if SSH is successfully listening on IPv6. If you see the following output, it means SSH is listening on IPv6:\nNow the application layer modifications are complete.\nDNS Configuration Finally, in Cloudflare, modify the DNS settings by changing the previous IPv4 A records to AAAA records for IPv6.\nTest Test everything to make sure it’s working fine, and then you can delete the IPv4 address to avoid being charged.\nUppublished: Running instances cannot have their IPv4 addresses removed, but you can rebuild them using an AMI. It’s a bit of a hassle, but at least AWS won’t send me any more emails. ✌️\nReferences How to remove IPv4 public IP address from EC2 instances before February 2024? (IPv6)\n","date":"2024-01-19T20:27:55Z","permalink":"https://example.com/p/aws-ec2-switch-to-ipv6save-43-per-year/","title":"Aws ec2 switch to ipv6,Save $43 per year"},{"content":"In 2023, there have been some changes to Go’s concurrency library, and this article provides an overview of these changes. Minor details such as typos and documentation changes will not be covered.\nsync.Map In Go 1.21.0, three functions related to Once were added to the sync package to facilitate the usage of Once:\n1 2 3 func OnceFunc(f func()) func() func OnceValue[T any](f func() T) func() T func OnceValues[T1, T2 any](f func() (T1, T2)) func() (T1, T2) The functionality of these three functions is as follows:\nOnceFunc: Returns a function g that, when called multiple times, will only execute f once. If f panics during execution, subsequent calls to g will not execute f, but each call will still panic. OnceValue: Returns a function g that, when called multiple times, will only execute f once. The return type of g is T, which is an additional return value compared to the previous function. The panic behavior is the same as OnceFunc. OnceValues: Returns a function g that, when called multiple times, will only execute f once. The return type of g is (T1, T2), which is an additional return value compared to the previous function. The panic behavior is the same as OnceFunc. In theory, you can add more functions and return more values. However, since Go does not have a tuple type, the return values of function g cannot be simplified to a tuple type. In any case, Go 1.21.0 only added these three functions.\nWhat are the benefits of these functions? Previously, when using sync.Once, such as initializing a thread pool, we needed to define a variable for the thread pool and call sync.Once.Do every time we accessed the thread pool variable:\n1 2 3 4 5 6 7 8 9 10 11 12 func TestOnce(t *testing.T) { var pool any var once sync.Once var initFn = func() { // initialize pool pool = 1 } for i := 0; i \u0026lt; 10; i++ { once.Do(initFn) t.Log(pool) } } With OnceValue, the code can be simplified:\n1 2 3 4 5 6 7 8 9 func TestOnceValue(t *testing.T) { var initPool = func() any { return 1 } var poolGenerator = sync.OnceValue(initPool) for i := 0; i \u0026lt; 10; i++ { t.Log(poolGenerator()) } } The code is slightly simplified, and you only need to call the returned function g to obtain the singleton.\nIn summary, these three functions are just encapsulations of sync.Once to make it more convenient to use.\nUnderstanding copyChecker We know that sync.Cond has two fields, noCopy and checker. noCopy can be statically checked using the go vet tool, but checker is checked at runtime:\n1 2 3 4 5 6 7 type Cond struct { noCopy noCopy // L is held while observing or changing the condition L Locker notify notifyList checker copyChecker } Previously, the conditions for copyChecker were as follows, although it is just three simple lines, it is not easy to understand:\n1 2 3 4 5 6 7 func (c *copyChecker) check() { if uintptr(*c) != uintptr(unsafe.Pointer(c)) \u0026amp;\u0026amp; !atomic.CompareAndSwapUintptr((*uintptr)(c), 0, uintptr(unsafe.Pointer(c))) \u0026amp;\u0026amp; uintptr(*c) != uintptr(unsafe.Pointer(c)) { panic(\u0026#34;sync.Cond is copied\u0026#34;) } } Now, with added comments, the meaning of these three lines is explained:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func (c *copyChecker) check() { ​ // Check if c has been copied in three steps: ​ // 1. The first comparison is the fast-path. If c has been initialized and not copied, this will return immediately. Otherwise, c is either not initialized, or has been copied. ​ // 2. Ensure c is initialized. If the CAS succeeds, we\u0026#39;re done. If it fails, c was either initialized concurrently and we simply lost the race, or c has been copied. ​ // 3. Do step 1 again. Now that c is definitely initialized, if this fails, c was copied. if uintptr(*c) != uintptr(unsafe.Pointer(c)) \u0026amp;\u0026amp; !atomic.CompareAndSwapUintptr((*uintptr)(c), 0, uintptr(unsafe.Pointer(c))) \u0026amp;\u0026amp; uintptr(*c) != uintptr(unsafe.Pointer(c)) { panic(\u0026#34;sync.Cond is copied\u0026#34;) } } The main logic is as follows:\nThe first step is a fast check, directly comparing the pointer of c and the pointer of c itself. If they are not equal, it means that c has been copied. This is the fastest check path. The second step ensures that c is initialized. It initializes c using CAS (CompareAndSwap). If the CAS succeeds, we\u0026rsquo;re done. If it fails, it means that c was either initialized concurrently and we simply lost the race, or c has been copied. The third step repeats the first step’s check. Since we know that c is definitely initialized at this point, if the check fails, it means that c was copied. The entire logic uses CAS combined with two pointer checks to ensure the correctness of the judgment.\nIn summary, the first step is a performance optimization. The second step uses CAS to ensure initialization. The third step rechecks to ensure the judgment.\nOptimization in sync.Map Previously, the implementation of the Range function in sync.Map was as follows:\n1 2 3 4 5 6 7 8 9 10 func (m *Map) Range(f func(key, value any) bool) { ... if read.amended { read = readOnly{m: m.dirty} m.read.Store(\u0026amp;read) m.dirty = nil m.misses = 0 } ... } There was a line of code: m.read.Store(\u0026amp;read), which caused read to escape to the heap. To avoid the escape of read, a small trick was employed by introducing a new variable:\n1 2 3 4 5 6 7 8 9 10 11 func (m *Map) Range(f func(key, value any) bool) { ... if read.amended { read = readOnly{m: m.dirty} copyRead := read m.read.Store(\u0026amp;Read) m.dirty = nil m.misses = 0 } ... } Issue #62404 analyzed this problem.\nReplacing done in sync.Once implementation with atomic.Uint32 Previously, the implementation of sync.Once was as follows:\n1 2 3 4 type Once struct { done uint32 m Mutex } The done field was of type uint32 to indicate whether Once has been executed. The reason for using uint32 instead of bool is that uint32 can be used with atomic operations from the atomic package, while bool cannot.\nNow, the implementation of sync.Once is as follows:\n1 2 3 4 type Once struct { done atomic.Uint32 m Mutex } Since Go 1.19, the standard library has provided atomic wrappers for basic types, and a large amount of code in the Go standard library has been replaced with atomic.XXX types.\nIn my opinion, this modification may result in a performance decrease compared to the previous implementation in certain cases. I will write an article specifically to explore this.\nBesides sync.Once, there are other types that have been replaced with atomic.XXX types in their usage. Is it necessary to replace them?\nOptimization in Initial Implementation of sync.OnceFunc The initial implementation of sync.OnceFunc was as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func OnceFunc(f func()) func() { var ( once Once valid bool p any ) g := func() { defer func() { p = recover() if !valid { panic(p) } }() f() valid = true } return func() { once.Do(g) if !valid { panic(p) } } } If you look closely at this code, you will notice that the function f passed to OnceFunc/OnceValue/OnceValues remains alive even after it has been executed once, as long as the returned function g is not garbage collected. This is unnecessary because f only needs to be executed once and can be garbage collected afterwards. Therefore, an optimization can be made to set f to nil after it is executed, allowing it to be garbage collected.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func OnceFunc(f func()) func() { var ( once Once valid bool p any ) // Construct the inner closure just once to reduce costs on the fast path. g := func() { defer func() { p = recover() if !valid { // Re-panic immediately so on the first call the user gets a // complete stack trace into f. panic(p) } }() f() f = nil // Do not keep f alive after invoking it. valid = true // Set only if f does not panic. } return func() { once.Do(g) if !valid { panic(p) } } } Context As we know, in Go 1.20, a new method WithCancelCause was added ( func WithCancelCause(parent Context) (ctx Context, cancel CancelCauseFunc)), which allows us to pass the cancellation cause to the Context generated by WithCancelCause. This allows us to retrieve the cancellation cause using the context.Cause method.\n1 2 3 4 ctx, cancel := context.WithCancelCause(parent) cancel(myError) ctx.Err() // returns context.Canceled context.Cause(ctx) // returns myError Of course, this implementation is only halfway done, as timeout-related Context also needs this functionality. Therefore, in Go 1.21.0, two additional functions were added:\n1 2 func WithDeadlineCause(parent Context, d time.Time, cause error) (Context, CancelFunc) func WithTimeoutCause(parent Context, timeout time.Duration, cause error) (Context, CancelFunc) These two functions, unlike WithCancelCause, directly pass the cause as a parameter instead of using the returned cancel function.\nGo 1.21.0 also introduced a function AfterFunc, which is similar to time.AfterFunc, but it returns a Context that is automatically canceled after the timeout. The implementation of this function is as follows:\n1 func AfterFunc(ctx Context, f func()) (stop func() bool) The specified Context triggers the invocation of f immediately when done (either due to timeout or cancellation). The returned stop function is used to stop the invocation of f. If stop is called and returns true, f will not be invoked.\nThis is a helper function, but it may be difficult to understand, and it is unlikely to be widely used.\nOther minor performance optimizations, such as replacing type emptyCtx int with type emptyCtx struct{}, are not mentioned here.\nAn additional function func WithoutCancel(parent Context) Context was added, which creates a Context that is not affected when the parent is canceled.\ngolang.org/x/sync No Significant Changes in sync errgroup now supports setting the cause using withCancelCause. singleflight added an Unwrap method to panicError.\n","date":"2024-01-19T12:45:00Z","permalink":"https://example.com/p/overview-of-changes-in-gos-concurrency-library-in-2023/","title":"Overview of Changes in Go’s Concurrency Library in 2023"},{"content":"\nToday, I will introduce some commonly used commands and tools for viewing Go assembly code and debugging Go programs. These tools can be used in regular situations or when engaging with colleagues or online discussions, allowing you to have an upper hand in critical moments.\nFor example, if a colleague claims that the first piece of code is more efficient than the second one:\n1 2 3 4 5 6 7 8 9 package main type Student struct { Class int } ​ func main() { var a = \u0026amp;Student{1} println(a) } 1 2 3 4 5 6 7 8 9 10 package main ​ type Student struct { Class int } func main() { var a = Student{1} var b = \u0026amp;a println(b) } and they explained it in such a way that you couldn’t win the argument. What should you do? Just use a single command to generate the assembly code and expose their argument, giving them a reality check.\nGenerating Assembly Code with go tool In fact, it’s quite simple. There are two commands that can achieve this:\n1 go tool compile -S main.go or:\n1 go build main.go \u0026amp;\u0026amp; go tool objdump ./main The first one is for compilation, which means compiling the source code into an .o object file and outputting the corresponding assembly code.\nThe second one is for disassembling, which means decompiling the executable file into assembly code. Therefore, you need to compile the code into an executable file using the go build command first.\nAlthough these two commands are not identical, they both reveal that the assembly code generated for the two example code snippets is the same. Your colleague’s “ claims” falls apart, and you’ve got them cornered.\nLocating the Runtime Source Code Go is a language with a runtime. What is the runtime? It’s essentially a set of auxiliary programs that the user didn’t write. The runtime writes code for us, such as the Go scheduler.\nAll we need to know is that we can create goroutines using the go keyword, and then we can pile up our business logic. As for how goroutines are scheduled, we don\u0026rsquo;t need to worry about it at all because that\u0026rsquo;s the job of the runtime scheduler.\nBut how can we correlate our code with the code inside the runtime?\nThe methods mentioned earlier can achieve this by adding a grep command.\nFor example, if I want to know which function in the runtime corresponds to the go keyword, I can write a test code snippet:\n1 2 3 4 5 6 7 package main ​ func main() { go func() { println(1 + 2) }() } Since the line go func() { }() is on line 4, we can add a condition when using grep:\n1 2 3 go tool compile -S main.go | grep \u0026#34;main.go:4\u0026#34; ​ // or ​ go build main.go \u0026amp;\u0026amp; go tool objdump ./main | grep \u0026#34;main.go:4\u0026#34; By analyzing the code, we can immediately see that the line go func(){} corresponds to the newproc() function. By further studying the newproc() function, we can gain a better understanding of how goroutines are created.\nDebugging Code with dlv Some may ask, “Are there any other methods or tools available for debugging Go programs and interacting with them?” The answer is yes! That’s where the dlv debugging tool comes in. It currently provides the best support for debugging Go programs.\nPreviously, I hadn’t really explored it in-depth and only knew some very basic commands. However, this time I have learned a few advanced commands, which have significantly enhanced my understanding of Go.\nLet’s demonstrate how to use dlv with a specific task.\nWe know that appending elements to a nil slice will not cause any problems. However, if we try to insert new elements into a nil map, it will immediately panic. Why does this happen and where exactly does the panic occur?\nFirst, let’s write a sample program that triggers a panic in a map:\n1 2 3 4 5 6 package main ​ func main() { var m map[int]int m[1] = 1 } Next, compile the program and generate an executable file using the go build command:\n1 go build main.go Then, enter the debugging mode using dlv:\n1 dlv exec ./main To set a breakpoint, we can use the b command in three different ways:\n1 2 3 b + address b + line number b + function name Let’s set a breakpoint at the line where the map assignment occurs, which is line 5. We’ll add a breakpoint at that line:\n1 2 3 (dlv) b main.go:5 Breakpoint 1 set at 0x104203070 for main.main() ./main.go:5 (dlv) Use the c command to directly run until the breakpoint is reached. Then, execute the disass command to see the assembly instructions:\nNext, use the si command to execute a single instruction. Repeat the si command to execute until the mapassign_fast64 function is reached:\nThen, let’s set a breakpoint in the map_fast64.go file:\n1 b /opt/homebrew/Cellar/go/1.21.6/libexec/src/runtime/map_fast64.go:93 Now, by using the s command, we can step into the branch where h is checked for nil and then the panic function is executed:\nAt this point, we have found the code that triggers a panic when assigning to a nil map. From here, we can follow the graph and locate the corresponding position in the runtime source code for further exploration.\nIn addition, we can use the bt command to view the call stack:\nUsing the frame 1 command, we can jump to the corresponding position. In this case, 1 corresponds to the main.go:5 line where we set the breakpoint. Isn\u0026rsquo;t it fascinating?\nIn the graph above, we can also clearly see that the user goroutine is called all the way by the goexit function. When the user goroutine completes, it goes back to the goexit function to do some finalization work. However, that\u0026rsquo;s beyond the scope of our discussion.\nAdditionally, dlv can also help us with the second part, “Locating the Runtime Source Code.”\nSummary Today, I provided systematic methods for viewing runtime source code or assembly code corresponding to user code using commands and tools, which is very practical. To summarize:\n1 2 3 go tool compile go tool objdump dlv In the future, we will continue analyzing Go source code using these tools, such as for maps and slices. With these tools, our learning process will be more efficient.\n","date":"2024-01-16T20:23:47Z","permalink":"https://example.com/p/how-to-analyze-go-code-in-assembly/","title":"How to analyze Go code in assembly"},{"content":"\nThe inaugural v2 version of the standard library in Go, hails from the esteemed math/rand/v2 repository. It is set to make its grand debut with the official release of Go1.22, poised to serve as a reliable and production-ready resource.\nReasons The original math/rand library in the standard package had numerous deficiencies and areas for improvement. These included outdated generators, slow algorithms (performance), and unfortunate conflicts with crypto/rand.Read, among other issues. There is a plan in place to upgrade the v2 versions of standard libraries. Starting with math allows for the accumulation of experience and resolving tooling ecosystem challenges (such as support from tools like gopls and goimports for v2 packages). Subsequent iterations can then address higher-risk packages, like sync/v2 or encoding/json/v2. Go1 requires compatibility guarantees, making it impractical to directly modify the original library. The issues with math/rand are also more prominent and evident. change list Removed Rand.Read and the top-level Read function. Removed Source.Seed, Rand.Seed, and the top-level Seed function (meaning that top-level functions like Int will always use random seeding). Removed Source64, as Source now provides the Uint64 method, making the original methods unnecessary. Utilized a more direct implementation for Float32 and Float64. For example, in the case of Float64, the original implementation used float64(r.Int63()) / (1\u0026lt;\u0026lt;63). However, this had a problem of occasionally rounding to 1.0, while Float64 should never round. The improvement involves changing it to float64(r.Int63n(1\u0026lt;\u0026lt;53)) / (1\u0026lt;\u0026lt;53), which avoids the rounding issue. Implemented Rand.Perm using Rand.Shuffle. This improves efficiency and ensures only one implementation. Renamed Int31, Int31n, Int63, and Int64n to Int32, Int32n, Int64, and Int64n, respectively. These names were unnecessary and confusing. Added Uint32, Uint32n, Uint64, Uint64n, Uint, and Uintn as top-level functions and methods on Rand. Utilized Lemire’s algorithm in Intn, Uintn, Int32n, Uint32n, Int64n, and Uint64n, resulting in improved performance. Introduced a new implementation of Source called PCG-DXSM, including related APIs like NewPCG. Removed the Mitchell \u0026amp; Reeds LFSR generator and NewSource. example Read \u0026amp; Seed The functions Read and Seed have been removed. It is recommended to use crypto/rand\u0026rsquo;s Read function instead.\n1 2 3 4 5 6 7 8 9 10 11 12 13 import ( \u0026#34;crypto/rand\u0026#34; \u0026#34;fmt\u0026#34; ) ​ func main() { b := make([]byte, 3) _, err := rand.Read(b) if err != nil { panic(err) } fmt.Printf(\u0026#34;hxzhouh: %v\\n\u0026#34;, b) } output：\n1 hxzhouh: [48 71 122] For the Seed function, it is advised to call New(NewSource(seed)) in order to reinitialize the random number generator.\ninternal The functions N, IntN, and UintN now utilize a novel implementation algorithm. Interested individuals are encouraged to allocate extra time to examine it in detail: A fast alternative to the modulo reduction\nThe functions Intn, Int31, Int31n, Int63, and Int64n have been renamed as follows: IntN, Int32, Int32N, Int64, and Int64N, respectively.\nAdditionally, new functions Uint32, Uint32N, Uint64, Uint64N, Uint, and UintN have been introduced to generate random unsigned integers. They have also been added as corresponding functions within the Rand structure.\nThe newly added function N generates random numbers of arbitrary integer types. This function is implemented using generics, and the following integer types are its type parameters:\nint int8 int16 int32 int64 Summary: Today, we have shared and further described the new math/rand/v2 library, highlighting key changes including performance optimization (algorithm rewrite), standardization, and additions of new random generators.\nGiven the substantial amount of information covered, we have selected and presented only the aspects that are essential for understanding and using the library. However, for those who are interested in delving deeper, it is recommended to refer to the full documentation of https://pkg.go.dev/math/rand/v2@master\n","date":"2024-01-13T20:50:00Z","permalink":"https://example.com/p/go1.22-add-frist-v2-lib-math/rand/v2-more-fast-and-more-standard/","title":"Go1.22 add frist v2 lib, math/rand/v2 more fast and more Standard"},{"content":"\nThis article has expired, please do not use it\nGitHub Copilot is a AI assistant developed by GitHub to help developers write code. It is a Visual Studio Code plugin based on OpenAI Codex, providing features such as code suggestions, auto-completion, auto-fixing, and auto-refactoring. Currently, GitHub Copilot also supports chat functionality, powered by GPT-4. However, it is only available within the context of VS Code. In this article, I will explain how to set up a chat service using GitHub Copilot, allowing you to use it anywhere.\nOpen-Source Components ChatGPT-Next-Web: A well-designed, cross-platform ChatGPT web UI with support for GPT3, GPT4, and Gemini Pro. copilot-gpt4-service An AWS EC2 server is needed to run the copilot-gpt4-service, but you can choose any cloud service provider you prefer or run it locally. Vercel: A free static website hosting service to deploy the ChatGPT-Next-Web service. Cloudflare: A free CDN service used for domain name resolution. Obtaining the GitHub Copilot Token Please refer to this document, GitHub Copilot Token. On a Mac, you can use the following command to obtain the token:\n1 bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/aaamoon/copilot-gpt4-service/master/shells/get_copilot_token.sh)\u0026#34; As shown in the screenshot, you need to copy the token to the clipboard for later use.\nRunning the copilot-gpt4-service The copilot-gpt4-service supports Docker deployment, and the official project provides a docker-compose file. You only need to modify the environment variables. Add a server block in NGINX: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 server { listen 443 ssl; listen [::]:443 ssl; server_name chat.example.com; ​ ssl_certificate /etc/nginx/cert/public.pem; ssl_certificate_key /etc/nginx/cert/private.key; ​ location / { proxy_pass http://127.0.0.1:8086/; # Replace with the copilot-gpt4-service address rewrite ^/(.*)$ /$1 break; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Upgrade-Insecure-Requests 1; proxy_set_header X-Forwarded-Proto https; } } Don\u0026rsquo;t forget to reload the NGINX configuration file.\nAdd a domain name resolution for \u0026ldquo;chat.example.com\u0026rdquo; on Cloudflare, pointing to the IP address of your NGINX server. Deploying the ChatGPT-Next-Web Service on Vercel ChatGPT-Next-Web supports one-click deployment to Vercel. You don\u0026rsquo;t need to make any changes.\nOnce the deployment on Vercel is complete, you can add a custom domain name, such as \u0026ldquo;chatnext.example.com\u0026rdquo;.\nTesting Open your browser and enter \u0026ldquo;chatnext.example.com\u0026rdquo;. Click on the settings button in the bottom left corner and select \u0026ldquo;Custom Interface\u0026rdquo;. Enter \u0026quot; https://chat.example.com\u0026quot; as the API endpoint, and use the GitHub Copilot token copied in the first step as the API token. Click on \u0026ldquo;Save\u0026rdquo;, and you can start chatting.\n","date":"2024-01-12T20:16:03Z","permalink":"https://example.com/p/how-to-set-up-chatgpt-4-service-using-github-copilot/","title":"How to Set Up ChatGPT-4 Service Using GitHub Copilot"},{"content":"Hello,World hello,World ","date":"2020-09-09T00:00:00Z","image":"https://example.com/helena-hertz-wWZzXlDpMog-unsplash.jpg","permalink":"https://example.com/p/test-chinese/","title":"Hello,World"}]